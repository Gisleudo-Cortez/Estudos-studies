{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Develop a classification model\n",
    "\n",
    "In this exercise, you'll work with the weather dataset and develop a training code to predict rainfall for the next day. The preprocess_dataset.py contains helper functions to pre-process the dataset. Your task is to finish the scaffolded train.py to formulate a high-level model training flow.\n",
    "\n",
    "Feel free to explore the Python files to see the complete implementation of the workflow.\n",
    "\n",
    "NOTE: Use python3 instead of python to run Python scripts.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split the pre-processed dataset in train.py file using the train_test_split method from scikit-learn.\n",
    "\n",
    "    Train the model using the training set, by specifying the correct arguments to the train_model method.\n",
    "\n",
    "    Calculate test set metrics using the test set, by specifying the correct arguments to the evaluate_model method.\n",
    "\n",
    "    Save the metrics dictionary into a JSON file using the save_metrics method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from metrics_and_plots import plot_confusion_matrix, save_metrics\n",
    "from model import evaluate_model, train_model\n",
    "from utils_and_constants import PROCESSED_DATASET, TARGET_COLUMN\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(TARGET_COLUMN, axis=1)\n",
    "    y = data[TARGET_COLUMN]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data(PROCESSED_DATASET)\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1993)\n",
    "\n",
    "    # Train the model using the training set\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Calculate test set metrics\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    print(\"====================Test Set Metrics==================\")\n",
    "    print(json.dumps(metrics, indent=2))\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    # Save metrics into json file\n",
    "    save_metrics(metrics)\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! One of the reasons that we split preprocessing code from model training code is because it allows us to cache data, as we will see in later exercises.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Train a classification model\n",
    "\n",
    "In this exercise, you'll continue working with the weather dataset and train a Random Forest Classifier to predict rainfall. The preprocess_dataset.py contains helper functions to pre-process the dataset, and train.py contains the training code.\n",
    "\n",
    "Your task is to execute the pre-processing and training code one after another and generate metrics and output plots. Upon successful execution, metrics.json contents would automatically change to correct values.\n",
    "\n",
    "NOTE: Use python3 instead of python to run Python scripts.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Run the preprocessing code by running python3 preprocess_dataset.py in the shell.\n",
    "\n",
    "    Verify that processed_dataset/weather.csv file is now present.\n",
    "\n",
    "    Run the training code by running python3 train.py in the shell.\n",
    "\n",
    "    Verify that metrics.json has changed and confusion_matrix.png is now present.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! The model might be using a prediction threshold that is too high, causing it to be conservative in making positive predictions. Lowering the threshold can increase recall but may decrease precision.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Setup model training using CML\n",
    "\n",
    "In this exercise, you will use CML GitHub Action to train a Random Forest Classifier to predict rainfall. CML is a GitHub Action that abstracts generating reports for ML experiments.\n",
    "\n",
    "The training will trigger when you open a PR against the main branch. You'll continue working with the weather dataset; the preprocess_dataset.py file contains helper functions to pre-process the dataset as before.\n",
    "\n",
    "The output from running train.py is a metrics.json file containing model metrics, and confusion_matrix.png file containing a plot of the confusion matrix.\n",
    "\n",
    "Your task is to finish the scaffolded .github/workflows/train_cml.yaml to formulate a high-level model training flow.\n",
    "\n",
    "NOTE: Use python3 instead of python to run Python scripts.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Setup CML GitHub Action iterative/setup-cml@v1.\n",
    "\n",
    "    Add evaluation metrics data, metrics.json, to the markdown report in the Write CML report step.\n",
    "\n",
    "    Add confusion matrix plot, confusion_matrix.png , to the markdown report in the Write CML report step.\n",
    "\n",
    "    Write the correct cml comment subcommand to create a comment in the PR.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "name: model-training\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "    branches: main\n",
    "\n",
    "permissions: write-all\n",
    "\n",
    "jobs:\n",
    "  train_and_report_eval_performance:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout \n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: 3.9\n",
    "\n",
    "      # Setup CML GitHub Action\n",
    "      - name: Setup CML\n",
    "        uses: iterative/setup-cml@v1\n",
    "          \n",
    "      - name: Train model\n",
    "        run: |\n",
    "          python3 preprocess_dataset.py\n",
    "          python3 train.py\n",
    "\n",
    "      - name: Write CML report\n",
    "        env:\n",
    "          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "        run: |\n",
    "          # Add metrics data to markdown\n",
    "          cat metrics.json >> model_eval_report.md\n",
    "          \n",
    "          # Add confusion matrix plot to markdown\n",
    "          echo \"![confusion matrix plot](./confusion_matrix.png)\" >> model_eval_report.md\n",
    "\n",
    "          # Create comment from markdown report\n",
    "          cml comment create model_eval_report.md\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! The cml comment update command updates the previous comment instead of creating a new one each time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are .dvc files needed?\n",
    "\n",
    "What is the primary purpose of a .dvc file in Data Version Control (DVC)?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    To serve as a backup for data files.\n",
    "    \n",
    "    \n",
    "    To manage the execution of Python scripts.\n",
    "    \n",
    "    \n",
    "    To store the complete history of Git commits.\n",
    "    \n",
    "    \n",
    "    To contain metadata and checksums for a tracked data file.{Answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Data versioning in action\n",
    "\n",
    "Data Version Control (DVC) provides a systematic approach to versioning data, a critical aspect often overlooked. With DVC, you can precisely track changes in your datasets, ensuring reproducibility, collaboration, and troubleshooting ease. It's your safeguard against data-related challenges, fostering trust and efficiency in your data-driven projects.\n",
    "\n",
    "In this exercise, you will practice initializing a DVC project and versioning a dataset. Git has already been initialized for this project.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Initialize DVC in the workspace.\n",
    "\n",
    "    Verify that .dvcignore file and .dvc folder are present.\n",
    "\n",
    "    Add dataset.csv to DVC and examine the contents of dataset.csv.dvc by opening it in the file editor.\n",
    "\n",
    "    Verify that DVC cache is populated by running find .dvc/cache -type f command in terminal.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "dvc init\n",
    "\n",
    "dvc add dataset.csv\n",
    "\n",
    "find .dvc/cache -type f \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice! You can verify the integrity of the cached dataset by computing its MD5 hash md5sum $(find .dvc/cache -type f) and comparing it to acutal dataset MD5 hash.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Benefits of DVC Remotes\n",
    "\n",
    "What are the primary benefits of using DVC remotes for data and ML models in your projects?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    DVC remotes replace the need for Git repositories entirely.\n",
    "    \n",
    "    \n",
    "    DVC remotes help you track software code more efficiently.\n",
    "    \n",
    "    \n",
    "    DVC remotes assist in syncing large files, centralizing data, and conserving local storage. {Answer}\n",
    "    \n",
    "    \n",
    "    DVC remotes are limited to cloud-based providers like AWS, excluding on-prem storage options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "DVC remotes in action\n",
    "\n",
    "In this exercise, you'll learn how to set up and use DVC remotes to store and share your datasets securely. Whether it's a colleague across the globe or your future self working on the project, DVC remotes ensure that your data is readily accessible and up-to-date. This exercise already has DVC initialized and the dataset added to DVC cache. We will be limiting ourselves to DVC remotes set up on a local filesystem.\n",
    "\n",
    "The syntax for adding a default DVC remote is\n",
    "\n",
    "dvc remote add -d --local <remote_name> </path/to/folder>\n",
    "\n",
    "where -d indicates the default DVC remote, and --local indicates that the DVC remote is pointed locally.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Set up a local DVC remote named myremote pointed at /tmp/dvc/localremote and verify it is empty by examining output of ls /tmp/dvc/localremote.\n",
    "\n",
    "    Examine the contents of .dvc/config.local, is the default set correctly to myremote?\n",
    "\n",
    "    Run dvc push and verify that the local remote now contains the file.\n",
    "\n",
    "    Run dvc pull and verify that \"Everything up to date\" appears as shell output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "dvc remote add -d --local myremote /tmp/dvc/localremote\n",
    "\n",
    "dvc push\n",
    "\n",
    "dvc pull\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice! Similar to previous exercise, you can verify the integrity of the pushed dataset by computing its MD5 hash md5sum $(find /tmp/dvc/localremote/ -type f) and comparing it to acutal dataset MD5 hash.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Creating a DVC pipeline\n",
    "\n",
    "Imagine a simple example of a workflow where a document is printed and then scanned to create a signed PDF document, with DVC managing the dependencies and outputs of each stage.\n",
    "\n",
    "The print stage depends on printing instructions outlined in print.sh and produces the pages output. The scan stage depends on instructions in scan.sh and pages (output of printer) and produces a signed.pdf output.\n",
    "\n",
    "Your task is to design a DVC pipeline outlining the workflow using the dvc stage add command. Its syntax is\n",
    "\n",
    "dvc stage add -n <stage_name> -d <dependency> -o <output> <command>\n",
    "\n",
    "You can add multiple dependencies and outputs with repeated use of -d and -o flags, respectively.\n",
    "\n",
    "NOTE: DVC has already been initialized in the exercise setup. There is no need to run dvc init again.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Design the print stage with print.sh as a dependency, pages as output, and ./print.sh as command.\n",
    "\n",
    "    Design the scan stage with scan.sh and pages as dependencies, signed.pdf as output, and ./scan.sh as command.\n",
    "\n",
    "    Verify dvc.yaml is written correctly.\n",
    "\n",
    "    Visualize the pipeline with dvc dag.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "dvc stage add -n print -d prin.sh -o pages ./print.sh\n",
    "\n",
    "dvc stage add -n scan -d scan.sh -d pages -o signed.pdf ./scan.sh\n",
    "\n",
    "dvc dag\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Note that you can visualize relationship between outputs in a DVC pipeline by using dvc dag --outs command.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Train ML models with DVC\n",
    "\n",
    "Ensuring the reproducibility of your experiments and maintaining a clean project structure can be challenging.\n",
    "\n",
    "This exercise will guide you through a scenario where you have to train a machine-learning model using a structured approach. You'll leverage DVC for managing data versioning and reproducibility in machine learning projects. Your task is to complete the scaffolded dvc.yaml file provides instructions for preprocessing data and training a model, ensuring that you can confidently reproduce your experiments and collaborate effectively with your team.\n",
    "\n",
    "NOTE: Use python3 instead of python to run Python scripts.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Run the data preprocessing script in the preprocess stage.\n",
    "\n",
    "    Run the model training script in the train stage.\n",
    "\n",
    "    Specify the preprocessed dataset as a dependency in the train stage.\n",
    "\n",
    "    Run the dvc repro command on the terminal to run the DVC pipeline.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "stages:\n",
    "  preprocess:\n",
    "    # Run the data preprocessing script\n",
    "    cmd: python3 preprocess_dataset.py\n",
    "    deps:\n",
    "    - preprocess_dataset.py\n",
    "    - raw_dataset/weather.csv\n",
    "    - utils_and_constants.py\n",
    "    outs:\n",
    "    - processed_dataset/weather.csv\n",
    "  train:\n",
    "    # Run the model training script\n",
    "    cmd: python3 train.py\n",
    "    deps:\n",
    "    - metrics_and_plots.py\n",
    "    - model.py\n",
    "    # Specify the preprocessed dataset as a dependency\n",
    "    - processed_dataset/weather.csv\n",
    "    - train.py\n",
    "    - utils_and_constants.py\n",
    "    outs:\n",
    "    - metrics.json\n",
    "    - confusion_matrix.png\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Note that the successful execution of dvc repro command generates a dvc.lock file.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
