{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Describe statistical concepts that underpin hypothesis testing and experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 * Define different statistical distributions (e.g. binomial, normal, Poisson, t-distribution, chi-square, and F-distribution, etc. )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Distributions\n",
    "\n",
    "Here are some of the most common statistical distributions:\n",
    "\n",
    "* **Binomial distribution:**\n",
    "    The binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of trials. The number of trials is called the number of successes, and the probability of success on each trial is called the success probability. The binomial distribution is often used to model the probability of getting a certain number of heads in a series of coin flips, or the number of customers who will buy a product from a store.\n",
    "    - The probability of success on each trial is denoted by p, and the probability of failure is denoted by q. The binomial distribution is defined by the following formula:\n",
    "        - P(x successes in n trials) = nCx * p^x * q^(n - x)\n",
    "    * Discrete\n",
    "    * Bell-shaped\n",
    "    * Number of successes in a fixed number of trials\n",
    "* **Normal distribution:**\n",
    "    The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is bell-shaped. The normal distribution is one of the most important distributions in statistics, and is used in a wide variety of applications, including hypothesis testing, confidence intervals, and regression analysis. The normal distribution is often used to model the heights of people, the weights of animals, or the scores on standardized tests.\n",
    "    - The normal distribution is defined by the following formula:\n",
    "        - f(x) = 1 / (σ * √(2π)) * exp(-(x - μ)^2 / (2 * σ^2)) : where μ is the mean, σ is the standard deviation, and f(x) is the probability density function.\n",
    "    * Continuous\n",
    "    * Bell-shaped\n",
    "    * Hypothesis testing, confidence intervals, regression analysis\n",
    "* **Poisson distribution:**\n",
    "    The Poisson distribution is a discrete probability distribution that describes the number of events that occur in a fixed interval of time or space. The Poisson distribution is often used to model the number of phone calls that come into a call center in a given hour, or the number of defects that occur in a manufactured product. The Poisson distribution is based on the assumption that the events occur independently of each other and at a constant rate.\n",
    "    -   The Poisson distribution is defined by the following formula:\n",
    "        - P(x events in a time interval) = λ^x * exp(-λ) / x! : where λ is the average number of events per time interval.\n",
    "    * Discrete\n",
    "    * Exponential\n",
    "    * Number of events that occur in a fixed interval of time or space\n",
    "* **t-distribution:**\n",
    "    The t-distribution is a continuous probability distribution that is similar to the normal distribution, but is used when the sample size is small. The t-distribution is often used in hypothesis testing and confidence intervals when the population standard deviation is unknown. The t-distribution is bell-shaped, but it is more spread out than the normal distribution. This is because the t-distribution takes into account the uncertainty of the sample standard deviation.\n",
    "    -   The t-distribution is defined by the following formula:\n",
    "        - f(t) = (1 + (t / df)^2)^(-(iris + 1) / 2) : where iris is the degrees of freedom.\n",
    "    * Continuous\n",
    "    * Bell-shaped\n",
    "    * Hypothesis testing, confidence intervals\n",
    "* **Chi-square distribution:**\n",
    "    The chi-square distribution is a continuous probability distribution that is used to test the goodness of fit of a model to data. It is also used to calculate the p-value for a hypothesis test. The chi-square distribution is bell-shaped, but it is more spread out than the normal distribution. This is because the chi-square distribution takes into account the uncertainty of the sample variance.\n",
    "    -   The chi-square distribution is defined by the following formula:\n",
    "        - f(x) = (x / 2)^k / Γ(k / 2) : where k is the degrees of freedom.\n",
    "    * Continuous\n",
    "    * Bell-shaped\n",
    "    * Goodness of fit, p-value calculation\n",
    "* **F-distribution:**\n",
    "    The F-distribution is a continuous probability distribution that is used to compare the variances of two or more populations. It is often used in ANOVA tests. The F-distribution is bell-shaped, but it is more spread out than the normal distribution. This is because the F-distribution takes into account the uncertainty of the sample variances.\n",
    "    -   The F-distribution is defined by the following formula:\n",
    "        - f(x) = (x / df1) * (df2 / (df1 + df2))^((df2 - 2) / 2) : where df1 and df2 are the degrees of freedom for the two populations being compared.\n",
    "    * Continuous\n",
    "    * Uniform\n",
    "    * Comparing variances of two or more populations\n",
    "\n",
    "| Distribution | Probability | Shape | Applications |\n",
    "|---|---|---|---|\n",
    "| Binomial | Discrete | Bell-shaped | Number of successes in a fixed number of trials |\n",
    "| Normal | Continuous | Bell-shaped | Hypothesis testing, confidence intervals, regression analysis |\n",
    "| Poisson | Discrete | Exponential | Number of events that occur in a fixed interval of time or space |\n",
    "| t-distribution | Continuous | Bell-shaped | Hypothesis testing, confidence intervals |\n",
    "| Chi-square | Continuous | Uniform | Goodness of fit, p-value calculation |\n",
    "| F-distribution | Continuous | Uniform | Comparing variances of two or more populations |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 * Explain the statistical concepts in hypothesis testing (e.g. null hypothesis, alternative hypothesis, one-tailed and two-tailed hypothesis tests, etc. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null hypothesis\n",
    "* The null hypothesis is a statement about the population parameter that is being tested. It is usually the statement of no difference or no effect. For example, the null hypothesis for a study on the effectiveness of a new drug might be that the drug has **no effect** on the patient's condition.\n",
    "\n",
    "## Alternative hypothesis\n",
    "* The alternative hypothesis is the opposite of the null hypothesis. It is the statement that the researcher is trying to prove. For example, the alternative hypothesis for the study on the new drug might be that the drug **does have an effect** on the patient's condition.\n",
    "\n",
    "## One-tailed and two-tailed hypothesis tests\n",
    "* A one-tailed hypothesis test is a test in which the alternative hypothesis specifies the direction of the difference between the null hypothesis and the population parameter. For example, the alternative hypothesis for the study on the new drug might be that the drug **has a positive effect** on the patient's condition. A two-tailed hypothesis test is a test in which the alternative hypothesis does not specify the direction of the difference between the null hypothesis and the population parameter. For example, the alternative hypothesis for the study on the new drug might be that the drug **has any effect** on the patient's condition.\n",
    "\n",
    "## P-value\n",
    "* The p-value is a probability that is used to determine whether to reject the null hypothesis. The p-value is calculated from the sample data and is the probability of obtaining a result as extreme as the one observed in the sample, given that the null hypothesis is true. A low p-value means that the probability of obtaining a result as extreme as the one observed in the sample is very low if the null hypothesis is true. This suggests that the null hypothesis is probably false.\n",
    "\n",
    "\n",
    "| Hypothesis | Example | Description |\n",
    "|---|---|---|\n",
    "| Null hypothesis | The drug has no effect on the patient's condition. | The null hypothesis is a statement of no difference or no effect. It is the statement that the researcher is trying to disprove. |\n",
    "| Alternative hypothesis | The drug has a positive effect on the patient's condition. | The alternative hypothesis is the opposite of the null hypothesis. It is the statement that the researcher is trying to prove. |\n",
    "| One-tailed hypothesis test | The drug has a positive effect on the patient's condition. | A one-tailed hypothesis test is a test in which the alternative hypothesis specifies the direction of the difference between the null hypothesis and the population parameter. In this case, the researcher is only interested in whether the drug has a positive effect on the patient's condition. |\n",
    "| Two-tailed hypothesis test | The drug has any effect on the patient's condition. | A two-tailed hypothesis test is a test in which the alternative hypothesis does not specify the direction of the difference between the null hypothesis and the population parameter. In this case, the researcher is interested in whether the drug has any effect on the patient's condition, whether it is positive or negative. |\n",
    "\n",
    "\n",
    "Hypothesis testing is a statistical procedure that is used to determine whether there is enough evidence to reject the null hypothesis. If the p-value is less than a pre-specified level of significance, then the null hypothesis is rejected. This means that there is enough evidence to conclude that the alternative hypothesis is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 * Explain the statistical concepts in the experimental design (e.g. control group, randomization, confounding variables, etc. )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Concepts in Experimental Design\n",
    "\n",
    "* **Control group:**\n",
    "    * A control group is a group of subjects in an experiment that is not exposed to the experimental treatment.\n",
    "    * The control group is used to compare the results of the experimental treatment group to see if there is a significant difference.\n",
    "    * The control group is essential for experimental design because it provides a baseline against which to compare the results of the experimental treatment group.\n",
    "    * If there is no significant difference between the control group and the experimental treatment group, then it is unlikely that the experimental treatment had any effect.\n",
    "* **Randomization:**\n",
    "    * Randomization is the process of assigning subjects to groups in an experiment in a random manner.\n",
    "    * This helps to ensure that the groups are as similar as possible, which minimizes the chances of bias.\n",
    "    * Randomization is important for experimental design because it helps to ensure that the groups are as similar as possible.\n",
    "    * This is important because if the groups are not similar, then it is possible that the results of the experiment are due to differences between the groups rather than the experimental treatment.\n",
    "* **Confounding variables:**\n",
    "    * Confounding variables are variables that can affect the outcome of an experiment but are not the focus of the experiment.\n",
    "    * Can be controlled for by randomization or by matching the groups on the confounding variable.\n",
    "    * Confounding variables are a major threat to the validity of experimental results.\n",
    "    * If a confounding variable is not controlled for, then it is possible that the results of the experiment are due to the confounding variable rather than the experimental treatment.\n",
    "* **Blinding:**\n",
    "    * Blinding is the process of keeping the subjects and/or the researchers in an experiment unaware of the treatment group assignment.\n",
    "    * This helps to prevent bias in the results of the experiment.\n",
    "    * Blinding is important for experimental design because it helps to prevent bias in the results of the experiment.\n",
    "    * If the subjects or the researchers know which group they are in, then they may be more likely to interpret the results of the experiment in a way that supports their hypothesis.\n",
    "* **Replication:**\n",
    "    * Replication is the process of repeating an experiment multiple times.\n",
    "    * This helps to increase the reliability of the results of the experiment.\n",
    "    * Replication is important for experimental design because it helps to increase the reliability of the results of the experiment.\n",
    "    * If the results of the experiment are replicated multiple times, then it is more likely that the results are valid.\n",
    "\n",
    "* **Internal validity:**\n",
    "    * Internal validity is the extent to which the results of an experiment can be attributed to the experimental treatment.\n",
    "    * A high level of internal validity means that the results of the experiment are unlikely to be due to other factors, such as confounding variables.\n",
    "* **External validity:**\n",
    "    * External validity is the extent to which the results of an experiment can be generalized to other populations or settings.\n",
    "    * A high level of external validity means that the results of the experiment are likely to be applicable to other people or situations.\n",
    "* **Power:**\n",
    "    * Power is the probability of rejecting the null hypothesis when it is false.\n",
    "    * A high power means that the experiment is likely to detect a real effect, even if the effect is small.\n",
    "* **Type I error:**\n",
    "    * A Type I error is the error of rejecting the null hypothesis when it is true.\n",
    "    * This is also known as a false positive.\n",
    "* **Type II error:**\n",
    "    * A Type II error is the error of failing to reject the null hypothesis when it is false.\n",
    "    * This is also known as a false negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 * Explain parameter estimation and confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation and Confidence Intervals\n",
    "\n",
    "- Parameter estimation is the process of estimating the value of a population parameter from a sample of data. \n",
    "- A confidence interval is a range of values that is likely to contain the true value of the population parameter.\n",
    "\n",
    "For example, suppose we want to estimate the average height of all adults in the United States. We could take a sample of 100 adults and measure their heights. The average height of the sample would be an estimate of the average height of the population.\n",
    "\n",
    "We could also calculate a confidence interval for the average height of the population. This would be a range of values that is likely to contain the true average height of the population. For example, the confidence interval might be from 5 feet 10 inches to 6 feet 1 inch.\n",
    "\n",
    "The width of the confidence interval depends on the sample size and the confidence level. A higher confidence level means that the confidence interval will be wider. For example, a 95% confidence interval will be wider than a 90% confidence interval.\n",
    "\n",
    "* **Parameter estimation:** Parameter estimation is the process of estimating the value of a population parameter from a sample of data. The most common method of parameter estimation is **maximum likelihood estimation**.\n",
    "* **Confidence interval:** A confidence interval is a range of values that is likely to contain the true value of the population parameter. The confidence interval is calculated using a **confidence level**, which is the probability that the confidence interval will contain the true value of the population parameter.\n",
    "* **Width of the confidence interval:** The width of the confidence interval depends on the sample size and the confidence level. A higher confidence level means that the confidence interval will be wider.\n",
    "* **Statistical inference:** Statistical inference is the process of making inferences about the population based on a sample of data. Parameter estimation and confidence intervals are two important tools for statistical inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Apply sampling methods to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0  setosa  \n",
       "1  setosa  \n",
       "2  setosa  \n",
       "3  setosa  \n",
       "4  setosa  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data for study\n",
    "from sklearn.datasets import  load_iris\n",
    "import pandas as pd\n",
    "\n",
    "data = load_iris(as_frame=True)\n",
    "iris = pd.concat([data.data, data.target], axis=1)\n",
    "\n",
    "map_names = {\n",
    "    0 : data.target_names[0],\n",
    "    1 : data.target_names[1],\n",
    "    2 : data.target_names[2]\n",
    "}\n",
    "\n",
    "iris['target'] = iris['target'].replace(map_names)\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 * Distinguish between different types of random sampling techniques and apply themethods using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple random sampling: This is the simplest type of random sampling. In simple random sampling, each member of the population has an equal chance of being selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92    versicolor\n",
       "94    versicolor\n",
       "90    versicolor\n",
       "48        setosa\n",
       "44        setosa\n",
       "Name: target, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virginica', 'virginica', 'versicolor', 'setosa', 'versicolor']\n"
     ]
    }
   ],
   "source": [
    "# using pandas\n",
    "display(iris['target'].sample(n=5))\n",
    "\n",
    "# using random\n",
    "import random\n",
    "\n",
    "population = list(iris.target.values)\n",
    "\n",
    "sample = random.sample(population, 5)\n",
    "\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Systematic random sampling: In systematic random sampling, you select every _k_th member of the population.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          setosa\n",
      "30         setosa\n",
      "60     versicolor\n",
      "90     versicolor\n",
      "120     virginica\n",
      "Name: target, dtype: object\n",
      "['setosa', 'setosa', 'versicolor', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# using pandas\n",
    "\n",
    "# Calculate the step size\n",
    "step_size = len(iris) // 5\n",
    "\n",
    "# Generate the sample indices\n",
    "sample_indices = list(range(0, len(iris), step_size))\n",
    "\n",
    "# Print the sample\n",
    "print(iris['target'].iloc[sample_indices])\n",
    "\n",
    "# Using random\n",
    "\n",
    "k = step_size\n",
    "\n",
    "sample = []\n",
    "\n",
    "for i in range(0, len(iris), k):\n",
    "  sample.append(iris['target'][i])\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stratified random sampling: In stratified random sampling, you divide the population into strata and then randomly sample from each stratum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4.8 3.0 1.4 0.3 'setosa']\n",
      "  [5.7 4.4 1.5 0.4 'setosa']\n",
      "  [5.0 3.4 1.5 0.2 'setosa']\n",
      "  [5.1 3.8 1.9 0.4 'setosa']\n",
      "  [5.1 3.5 1.4 0.2 'setosa']]\n",
      "\n",
      " [[6.0 2.2 4.0 1.0 'versicolor']\n",
      "  [6.7 3.0 5.0 1.7 'versicolor']\n",
      "  [7.0 3.2 4.7 1.4 'versicolor']\n",
      "  [6.1 2.9 4.7 1.4 'versicolor']\n",
      "  [6.0 2.9 4.5 1.5 'versicolor']]\n",
      "\n",
      " [[6.2 3.4 5.4 2.3 'virginica']\n",
      "  [7.2 3.6 6.1 2.5 'virginica']\n",
      "  [7.7 2.8 6.7 2.0 'virginica']\n",
      "  [6.5 3.2 5.1 2.0 'virginica']\n",
      "  [7.2 3.0 5.8 1.6 'virginica']]]\n",
      "Fold 0:\n",
      "  Train: index=[ 59  47  36 147  76  98  12 144  21  96  46 103  13  51  57   5 129  20\n",
      " 137  80 111  89 135  40 142  60  27   8  69  30   0 118 149  88  35  83\n",
      "  39 113 126 107 139  74  78 143 132  33 138 119  50 121  68 109  56 116\n",
      "  85 117   1  31 127   6  14  17  32  49  44   3   2 134  53 133  99 115\n",
      " 141 106  64  63 145  52  61 146 108  73  26  38  71  87  54 114 105 140\n",
      " 128  67 122  91   9  11 100  45  41  42  18  19 131  72  92 104 148 130\n",
      "  86 110  97  29  81  77  84  65  15  43  58  62   4  22 123 120  37  55\n",
      "  93  95  82  23 101  48  25  66   7]\n",
      "  Test:  index=[ 75  94 102 136  70  34  28  10 125  16 112  24  79 124  90]\n",
      "Fold 1:\n",
      "  Train: index=[ 57  55  37 144 131 100  63 112  95  25  33  76 101 148   6  13  87  59\n",
      " 105  36  48  47  26  94   1  79  12  51 140  42  86 139 113  35   2 123\n",
      "  83  97  75 111  88 133 141 109 130 103 127  80  67  91  78  99 125  90\n",
      "  68  96 116  38  65  17 147 117 137  21  98  39  44 106 149 110 119   5\n",
      "  23   4  22  43  74  45  84  81  72 124 115  27  31   8  77   7  70 108\n",
      "  40  89  16 102  54  32  46  85  15  60  49  71  53 138 145  66 134  93\n",
      "  28 104  41  62  20 118 122  19   9 135  24  18 121  11 128  64   3  82\n",
      " 107 142 126  58  30 120  73  52 114]\n",
      "  Test:  index=[ 29  34  61   0 136 146  14  92  69  10 132  56 143  50 129]\n",
      "Fold 2:\n",
      "  Train: index=[ 82  26  62  64  21  31  59  16  49  78 138  14  35 115 118 137  25  37\n",
      " 142 116  22 111  17  71  96   9 135 104  69 107 114  30  61  66   2  12\n",
      "   6  76  38  77  50  52  56 125   0  93  73 117 140  87  74 100  53  46\n",
      "  67   7  40 146  99 126 103   8 147  86   3  41 112   5  90 136 123  39\n",
      "  42  65 128  27  95  88  36  89  47  72 133  63 105  45  98  92  15  83\n",
      "   4  11 113 102 120  68  60 134  13 129  58  79 121   1  23  84  51 108\n",
      "  43 109  54 149 101  91 130 139 106 131 144 124  29  80  85  33  18  48\n",
      "  70  28 145  24  10 148 132 119  97]\n",
      "  Test:  index=[ 94 143  55  32  57  75  44  20 127 122  19 110  34  81 141]\n",
      "Fold 3:\n",
      "  Train: index=[144   6  21  91 100  71  67  22  92  55  75  87  94 108 140 126 109  16\n",
      "  25 103 114 139 143   7 116  12  61  29 148  81  80  44  51 106   4  52\n",
      "  38 104 123 107 132  77  64  45   3  50 136  70  19 149  93 112  85  27\n",
      "  43  74 127  28  17  68  79  63 130  31 102  34  60   0  72 146  40 118\n",
      "  46  57   5 134 121  69  10  30  49  54   9 111  14  97  82  18 105  88\n",
      "  11 128 115  41  37  24  23  98  90 117 120 133  73  56  47  84  99 142\n",
      "  39  66  96 122 125  59  58  86 110 101  48 138  33  32  20 129  65 131\n",
      "  76   1 141  13  15  53 119  26 145]\n",
      "  Test:  index=[ 35 137 124 147  36 113  89  83  42 135  78   2  62   8  95]\n",
      "Fold 4:\n",
      "  Train: index=[143  74  71 101  24 121 102  98 141  87 136 125  62 134  99  33  46 122\n",
      "  40  38  47 124 103  17 146  88  35  28  13  42  21  14  56 109  73  34\n",
      "  12  57  95 131  16  63   5  76  86  11  39  91 140  55  52 115 133 149\n",
      "  97  67  50  18  64  79 110 113 116  15  60 108 104   1  58  36  51 119\n",
      " 142 128  30  65  83  81 112   9  69  84   8 127 139  96  29 117  66  70\n",
      "  45 120 145  53  93  68  19 148  54 137  43  44 105  89  32  31 123  94\n",
      "  61 135   7  75 114  26  10  48   6   3 111 144   4  41 106  22 126  59\n",
      "  49 130  82 138 118  23  92  80  37]\n",
      "  Test:  index=[  2  25 100  72 147 132  27  90 107 129  78  85   0  77  20]\n"
     ]
    }
   ],
   "source": [
    "#Using random\n",
    "\n",
    "strata = list(iris.target.unique())\n",
    "\n",
    "sample = []\n",
    "\n",
    "for stratum in strata:\n",
    "  population = iris.loc[iris['target'] == stratum]\n",
    "  samples = random.sample(list(population[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)','petal width (cm)', 'target']].values), 5)\n",
    "  sample.append(samples)\n",
    "\n",
    "print(np.array(sample))\n",
    "\n",
    "# Using pandas \n",
    "iris.groupby('target').sample(n=5)\n",
    "\n",
    "# Using sklearn\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sss.split(iris.iloc[:, :-1], iris.iloc[:, -1])):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cluster random sampling: In cluster random sampling, you randomly select clusters from the population and then sample all members of each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_random_sample(df, k, n_samples):\n",
    "  \"\"\"\n",
    "  Cluster random sampling algorithm.\n",
    "\n",
    "  Args:\n",
    "    df: A DataFrame of data points.\n",
    "    k: The number of clusters to sample.\n",
    "\n",
    "  Returns:\n",
    "    A list of k clusters.\n",
    "  \"\"\"\n",
    "\n",
    "  clusters = []\n",
    "  for _ in range(k):\n",
    "    cluster = []\n",
    "    while len(cluster) < n_samples:\n",
    "      index = random.randint(0, len(df) - 1)\n",
    "      cluster.append(df.iloc[index].values)\n",
    "    clusters.append(cluster)\n",
    "  return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.5, 2.3, 4.0, 1.3, 'versicolor'],\n",
       "       [6.5, 2.8, 4.6, 1.5, 'versicolor'],\n",
       "       [5.7, 2.8, 4.5, 1.3, 'versicolor'],\n",
       "       [6.3, 3.3, 4.7, 1.6, 'versicolor'],\n",
       "       [4.9, 2.4, 3.3, 1.0, 'versicolor']], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[array([6.9, 3.1, 5.4, 2.1, 'virginica'], dtype=object)],\n",
       " [array([6.3, 3.3, 4.7, 1.6, 'versicolor'], dtype=object)],\n",
       " [array([4.8, 3.1, 1.6, 0.2, 'setosa'], dtype=object)],\n",
       " [array([6.6, 3.0, 4.4, 1.4, 'versicolor'], dtype=object)],\n",
       " [array([5.0, 3.5, 1.6, 0.6, 'setosa'], dtype=object)]]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using pandas\n",
    "\n",
    "cluster_labels = iris.target\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "cluster_indices = np.random.choice(len(cluster_labels) - n_samples, size=n_samples, replace=False)\n",
    "\n",
    "samples = []\n",
    "\n",
    "for cluster_index in cluster_indices:\n",
    "    sample = iris.iloc[cluster_index : cluster_index + n_samples]\n",
    "    samples.append(sample.values)\n",
    "\n",
    "display(samples[0])\n",
    "\n",
    "# using a custom function\n",
    "\n",
    "cluster_random_sample(iris, 5, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 * Sample data from a statistical distribution (e.g. normal, binomial, Poisson, exponential, etc.) using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04212624,  0.18185751,  0.14583106,  0.31338098, -1.34649079])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5, 4, 4, 5, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([8, 8, 6, 5, 7])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1.82355793, 1.16453715, 1.28156788, 1.14111947, 1.80062599])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate 5 samples from a normal distribution with mean 0 and standard deviation 1\n",
    "samples = stats.norm(0, 1).rvs(5)\n",
    "display(samples)\n",
    "\n",
    "# Generate 5 samples from a binomial distribution with n=10 and p=0.5\n",
    "samples = stats.binom(10, 0.5).rvs(5)\n",
    "display(samples)\n",
    "\n",
    "# Generate 5 samples from a Poisson distribution with lambda=5\n",
    "samples = stats.poisson(5).rvs(5)\n",
    "display(samples)\n",
    "\n",
    "# Generate 5 samples from an exponential distribution with mean 1\n",
    "samples = stats.expon(1).rvs(5)\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82560445, 0.24134217, 1.08769223, 0.92162577, 0.39867176])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 5, 4, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, 4, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.56762036, 0.97899688, 1.2752171 , 0.14785828, 1.02736449])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using only numpy\n",
    "\n",
    "# Generate 5 samples from a normal distribution with mean 0 and standard deviation 1\n",
    "normal = np.random.normal(0, 1, 5)\n",
    "display(normal)\n",
    "\n",
    "# Generate 5 samples from a binomial distribution with n=10 and p=0.5\n",
    "binomial = np.random.binomial(10, 0.5, 5)\n",
    "display(binomial)\n",
    "\n",
    "# Generate 5 samples from a Poisson distribution with lambda=5\n",
    "poisson = np.random.poisson(5, 5)\n",
    "display(poisson)\n",
    "\n",
    "# Generate 5 samples from an exponential distribution with mean 1\n",
    "exponential = np.random.exponential(1, 5)\n",
    "display(exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Calculate a probability from a statistical distribution (e.g. normal, binomial, Poisson, exponential, etc.) Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24609375000000003"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3520653267642995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.615960654833063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the probability of getting 5 heads in 10 coin flips\n",
    "binom = stats.binom(10, 0.5).pmf(5)\n",
    "display(binom)\n",
    "\n",
    "# Calculate the probability of getting a value between 0 and 1 from a standard normal distribution\n",
    "norm = stats.norm(0, 1).pdf(0.5)\n",
    "display(norm)\n",
    "\n",
    "# Calculate the probability of getting a value less than 5 from a Poisson distribution with lambda=5\n",
    "poisson = stats.poisson(5).cdf(5)\n",
    "display(poisson)\n",
    "\n",
    "# Calculate the probability of getting a value less than 1 from an exponential distribution with mean 1\n",
    "expon = stats.expon(1).cdf(1)\n",
    "display(expon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implement methods for performing statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 * Run statistical tests (e.g. t-test, ANOVA test, chi-square test) using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -7.244424595510364\n",
      "p-value: 9.454388464164657e-12\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Create two groups of data\n",
    "group1 = np.random.normal(0, 1, 100)\n",
    "group2 = np.random.normal(1, 1, 100)\n",
    "\n",
    "# Run the t-test\n",
    "t_statistic, p_value = stats.ttest_ind(group1, group2)\n",
    "\n",
    "# Print the results\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f-statistic: 84.36763762231348\n",
      "p-value: 9.67545646594095e-30\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Create three groups of data\n",
    "group1 = np.random.normal(0, 1, 100)\n",
    "group2 = np.random.normal(1, 1, 100)\n",
    "group3 = np.random.normal(2, 1, 100)\n",
    "\n",
    "# Run the ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "# Print the results\n",
    "print(\"f-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi-statistic: 5.4\n",
      "p-value: 0.02013675155034633\n",
      "degrees of freedom: 1\n",
      "expected values: [[15. 15.]\n",
      " [15. 15.]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = np.array([[20, 10], [10, 20]])\n",
    "\n",
    "# Run the chi-square test\n",
    "chi_statistic, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the results\n",
    "print(\"chi-statistic:\", chi_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"degrees of freedom:\", dof)\n",
    "print(\"expected values:\", expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 * Analyze the results of statistical tests from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution\n",
    "\n",
    "* The binomial distribution describes the probability of obtaining a certain number of successes in a fixed number of independent Bernoulli trials.\n",
    "* In hypothesis testing, if you have conducted a binomial test and obtained a p-value, you can interpret it as follows:\n",
    "    * If the p-value is less than the chosen significance level (e.g., 0.05), you have evidence to reject the null hypothesis, suggesting that the observed results are statistically significant.\n",
    "    * If the p-value is greater than the significance level, you do not have sufficient evidence to reject the null hypothesis, implying that the observed results are not statistically significant.\n",
    "\n",
    "### Normal Distribution\n",
    "\n",
    "* The normal distribution (also called Gaussian distribution) is a continuous probability distribution commonly used to model real-world data that exhibit a symmetric bell-shaped curve.\n",
    "* In hypothesis testing, when comparing means or conducting tests such as the t-test or z-test:\n",
    "    * The test statistic (e.g., t-statistic or z-score) measures the number of standard deviations a data point or sample mean is away from the population mean.\n",
    "    * A positive or negative test statistic indicates the direction of the difference between the sample mean and population mean.\n",
    "    * The p-value represents the probability of observing the data or more extreme results under the null hypothesis.\n",
    "    * If the p-value is less than the chosen significance level, you can reject the null hypothesis in favor of the alternative hypothesis.\n",
    "\n",
    "### Poisson Distribution\n",
    "\n",
    "* The Poisson distribution models the probability of a given number of events occurring within a fixed interval of time or space.\n",
    "* In hypothesis testing, when analyzing count data or event occurrences:\n",
    "    * The test statistic depends on the specific test being conducted (e.g., chi-square test for count data).\n",
    "    * The p-value indicates the probability of observing the data or more extreme results under the null hypothesis.\n",
    "    * Similar to other tests, if the p-value is less than the chosen significance level, you may reject the null hypothesis.\n",
    "\n",
    "### t-Distribution\n",
    "\n",
    "* The t-distribution is used when sample sizes are small or when the population standard deviation is unknown. It closely resembles the normal distribution but has fatter tails.\n",
    "* In hypothesis testing, particularly in situations with small sample sizes:\n",
    "    * The t-statistic measures how much the sample mean differs from the null hypothesis mean in terms of standard error.\n",
    "    * The p-value represents the probability of observing the data or more extreme results under the null hypothesis.\n",
    "    * If the p-value is less than the chosen significance level, you can reject the null hypothesis.\n",
    "\n",
    "### Chi-Square Distribution\n",
    "\n",
    "* The chi-square distribution is commonly used in tests of independence, goodness-of-fit, and homogeneity.\n",
    "* In hypothesis testing using chi-square tests:\n",
    "    * The chi-square test statistic measures the discrepancy between the observed and expected frequencies.\n",
    "    * The p-value indicates the probability of observing the data or more extreme results under the null hypothesis.\n",
    "    * If the p-value is less than the chosen significance level, you can reject the null hypothesis.\n",
    "\n",
    "### F-Distribution\n",
    "\n",
    "* The F-distribution is used in analysis of variance (ANOVA) and other tests involving variance ratios.\n",
    "* In hypothesis testing using ANOVA or other F-tests:\n",
    "    * The F-statistic measures the ratio of variability between groups to variability within groups.\n",
    "    * The p-value represents the probability of observing the data or more extreme results under the null hypothesis.\n",
    "    * If the p-value is less than the chosen significance level, you can reject the null hypothesis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estudos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
