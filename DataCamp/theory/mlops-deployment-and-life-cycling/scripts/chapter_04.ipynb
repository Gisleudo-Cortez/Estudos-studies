{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift vs drift\n",
    "\n",
    "You have learned how models deteriorate and your options for detecting performance drops.\n",
    "\n",
    "Can you tell which of the following statements are true and which are false?\n",
    "\n",
    "![Answer](images/ch_04-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency\n",
    "\n",
    "You learned about an important term called \"verification latency\".\n",
    "\n",
    "When the verification latency is in the order of months, your only option is to resort to input data monitoring as an indirect way to detect model deterioration in time.\n",
    "\n",
    "Imagine now that you are building an ML-based application that predicts stock prices based on data collected from various news portals and APIs on the Internet. Whenever you make a prediction, the ground truth is available in a matter of seconds.\n",
    "\n",
    "Does it still make sense to implement input data monitoring in this use case?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Yes, but only if the ground truth is extremely expensive to obtain quickly.\n",
    "    \n",
    "    \n",
    "    No. Nothing beats the ground truth, and if it's quickly available, it makes no sense to use less accurate workarounds.\n",
    "    \n",
    "    \n",
    "    It always makes sense to implement input data monitoring. {Answer}\n",
    "\n",
    "**Based on the ground truth only, we might get the impression that our model has deteriorated, while, in fact, our data pipeline is broken and feeding corrupted inputs to our model. So input monitoring should always be included in your overall approach to monitoring models in production.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Already?\n",
    "\n",
    "Imagine the following scenario: You have trained a model using a full year of labeled data and deployed it into production.\n",
    "\n",
    "One day after deployment, you decide to check your data monitoring dashboard, and you see the following picture, where the training inputs are represented with blue and the production inputs with orange dots.\n",
    "\n",
    "![train vs prod data](images/which%20type%20of%20shift%20-%20MLOps.png)\n",
    "\n",
    "What conclusion can you make with certainty?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    We have a clear case of covariate shift and, consequently, concept drift as well.\n",
    "    \n",
    "    \n",
    "    Covariate shift has obviously occurred, with potential concept drift to be verified after obtaining the ground truth\n",
    "    \n",
    "    \n",
    "    Concept drift has certainly occurred, but we need the ground truth to check if we have a covariate shift as well.\n",
    "    \n",
    "    \n",
    "    We can make no certain conclusion based on comparing one day of production data with a full year of training data. Moreover, the differences in the distributions are not drastic in any way. {Answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The monitoring system\n",
    "\n",
    "By now, it is clear that an ML application is much more than just the model and that each component can be a point of failure.\n",
    "\n",
    "For that reason, a comprehensive monitoring system is of utmost importance for your ability to capture and resolve issues in production quickly.\n",
    "\n",
    "What are some fundamental features that one such system should have?\n",
    "\n",
    "![Answer](images/ch_04-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alerting\n",
    "\n",
    "As you have heard in this lesson, you can monitor data in production by using the following:\n",
    "\n",
    "    deterministic methods: by checking if all required attributes are present and have values within strictly defined sets; and\n",
    "    statistical methods: by checking if the distribution of data observed in production is significantly different than the distribution observed at training time.\n",
    "\n",
    "You have also heard that the latter methods can be very sensitive to even the smallest changes and generate a large number of uninformative (non-actionable) alerts.\n",
    "\n",
    "But why is that actually an issue?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Because too many false alarms and alerts for the smallest changes can induce so-called alert fatigue, making actual issues pass unnoticed. {Answer}\n",
    "    \n",
    "    \n",
    "    Because it is very hard to find good Data Scientists that are capable of interpreting the outputs of statistical tests.\n",
    "    \n",
    "    \n",
    "    Because an extremely large number of alerts can inflate our logs so much, the server file system becomes overloaded, crashing our service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-centric vs Model-centric\n",
    "\n",
    "As you have just heard, there are two general approaches to improving the performance of an ML model:\n",
    "\n",
    "    The model-centric approach, and\n",
    "    The data-centric approach\n",
    "\n",
    "Can you tell which of the following actions is typical for which of these two philosophies?\n",
    "\n",
    "![Answer](images/ch_04-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-in-the-Loop\n",
    "\n",
    "You heard about the ML application design pattern called the Human-in-the-Loop. That is when a human expert takes over, making them more difficult estimations when the model is not confident enough. This means continuous labeling of new data, which enables continuous model maintenance. Sounds perfect, right?\n",
    "\n",
    "Now imagine you need to build two ML applications:\n",
    "\n",
    "    one for detecting lung disease from x-ray images, and\n",
    "    one for predicting stock prices and trading stocks at a frequency of several thousand transactions per second (so-called high-frequency trading, or HFT).\n",
    "\n",
    "Which of these apps would NOT be a good candidate for a Human-in-the-Loop design and why?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    The lung disease app because humans can not beat ML algorithms in image recognition.\n",
    "    \n",
    "    \n",
    "    The HFT app. Because humans cannot make decisions in such a short time interval. {Answer}\n",
    "    \n",
    "    \n",
    "    The lung disease app because it is more important to produce a diagnosis as fast as possible, even if the model has low confidence.\n",
    "    \n",
    "    \n",
    "    The HFT app because humans cannot beat ML algorithms in picking the best stocks and predicting their prices.\n",
    "\n",
    "**A Human-in-the-Loop system makes sense only when there is enough time to include the expert in the estimation process. That is the case with most medical diagnostics applications, where patients often don't mind waiting for days, even weeks, for the correct diagnosis. But when decisions and actions need to be made at a pace that exceeds human capacities, we have to rely solely on algorithms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of governance\n",
    "\n",
    "Model governance sets the rules and controls for machine learning models running in production. For some models, like those that banks use for credit risk scoring, an elaborate governance framework is mandated by laws. But even with no regulatory pressure, having some level of control over what goes into production and how is more than valuable.\n",
    "\n",
    "You have learned the core elements and features of a model governance framework.\n",
    "\n",
    "In this exercise, your task is to differentiate between those fundamental building blocks and other ML concepts which are not within the scope of governance by placing them into the True or False bucket, respectively.\n",
    "\n",
    "![Answer](images/ch_04-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stages of governance\n",
    "\n",
    "You have heard that governance spans all stages of our ML project life cycle, from design to development, to operations.\n",
    "\n",
    "In each of these stages, you must provide answers to different questions in order to comply with your ML governance framework.\n",
    "\n",
    "Are you able to connect each question with the project phase it refers to?\n",
    "\n",
    "![Answer](images/ch_04-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk classification\n",
    "\n",
    "As you have learned, model governance never follows a one-size-fits-all approach.\n",
    "\n",
    "As each additional control can slow down the project and increase the final cost of the ML service, you should apply them only where they are really necessary.\n",
    "\n",
    "One of the first steps in this process is to classify the model in question into the appropriate risk category.\n",
    "\n",
    "Whether you use three risk categories (low, medium, high), five, or ten -- we usually assign the models to them based on two questions, which are:\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    \"How many Data Scientists have worked on this model?\" and \"How many MLOps engineers have worked on it?\"\n",
    "    \n",
    "    \n",
    "    \"What was the budget of this ML project?\" and \"How much are the customers willing to pay for this service?\"\n",
    "    \n",
    "    \n",
    "    \"What is the direct and indirect cost of each possible prediction error?\" and \"How often will this model make predictions?\" {Answer\n",
    "    \n",
    "**Correct! Model risk is, ultimately, a cumulative measure of \"how much would make the mistakes of this model cost us?\" and to assess it, we need to know its usage frequency and costs of errors.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
