{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline or online?\n",
    "\n",
    "In this lesson, you learned that one of the most basic differences between serving modes is whether their execution is scheduled or triggered by events and user requests.\n",
    "\n",
    "Your final approach should be dictated by the needs of your users, without unnecessary complexities that add no value.\n",
    "\n",
    "For example, if you have built a model for generating weather predictions that your users would like to receive only once a day, every evening, the most reasonable choice would be:\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Stream processing\n",
    "    \n",
    "    \n",
    "    \"Batch\" a.k.a. \"offline\" a.k.a. \"static\" prediction {Answer}\n",
    "    \n",
    "    \n",
    "    \"Online\" a.k.a. \"dynamic\" prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When time matters - a bit\n",
    "\n",
    "You have learned how the acceptable latency of your Machine Learning service will impact the choice of the serving mode you will implement.\n",
    "\n",
    "Sometimes users can wait for days, even weeks. Sometimes, a second is too much.\n",
    "\n",
    "The lower the expected latency, the bigger the engineering challenges and the cost of your service becomes. Therefore, avoid over-engineering and match the design of your ML service to what the users require and are willing to pay for.\n",
    "\n",
    "For example, say you are building an ML service for analyzing and summarizing large .pdf documents. If your users tell you that they would like to receive the outputs of your service within 5 minutes of making a request to it, the most reasonable serving mode for your use case would be:\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Real-time prediction\n",
    "    \n",
    "    \n",
    "    Near-real-time prediction a.k.a. stream processing {Answer}\n",
    "    \n",
    "    \n",
    "    Offline batch prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client-server\n",
    "\n",
    "In the video, you learned that an API is a point of contact between a client and a server.\n",
    "\n",
    "In this context, what is exactly a client?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Any application that uses your ML model via the API {Answer}\n",
    "    \n",
    "    \n",
    "    The user that pays for the API usage, irrespective of who the actual consumers are.\n",
    "    \n",
    "    \n",
    "    The application that exposes your model to the outside world via the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API functionalities\n",
    "\n",
    "You learned that the API, being the contact point between your ML service and the outside world, has many important safeguarding functionalities.\n",
    "\n",
    "If it happens that one of the client applications sends you a request containing only two out of four required features, you must be able to detect that immediately and inform the client that they have made a mistake.\n",
    "\n",
    "Otherwise your service may return a generic error and it may seem like there is an issue on your side.\n",
    "\n",
    "This check is a basic functionality of every serious API and we call it:\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Output validation\n",
    "    \n",
    "    \n",
    "    Authentication\n",
    "    \n",
    "    \n",
    "    Input validation {Answer}\n",
    "    \n",
    "    \n",
    "    Throttling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which test is it?\n",
    "\n",
    "If you want to extensively test how a new component of your ML application works together with other internal and external components, which type of tests would you write for that purpose, and in which environment should you run them?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Smoke tests in the production environment\n",
    "    \n",
    "    \n",
    "    Integration tests in the staging environment {Answer}\n",
    "    \n",
    "    \n",
    "    Unit tests in the production environment\n",
    "    \n",
    "    \n",
    "    Smoke tests in the staging environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progression through environments\n",
    "\n",
    "What is the sequence of environments through which we deploy and test our models until we finally make our service available to the end user?\n",
    "\n",
    "![Answer](images/ch_03-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests per environment\n",
    "\n",
    "In the last video, you learned about the different types of tests your ML application should be subjected to. You also heard about different environments that help you in this process of testing and bringing your app from development to production.\n",
    "\n",
    "Do you still remember which environment served which purpose?\n",
    "\n",
    "![Answer](images/ch_03-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fitting deployment strategy\n",
    "\n",
    "You have realized that your current model is severely degraded, so you have collected a new batch of training data and trained the new model.\n",
    "\n",
    "Your model pipeline hasn't changed in any way. You have just re-trained it on new data. Therefore, you are confident it is safe to immediately start serving this new, better-performing model to all your users.\n",
    "\n",
    "Which deployment strategy makes the most sense in this case?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Canary deployment\n",
    "    \n",
    "    \n",
    "    Blue/green deployment {Answer}\n",
    "    \n",
    "    \n",
    "    Shadow deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order of risk\n",
    "\n",
    "Order the deployment strategies, from the one with the highest to lowest risk, in terms of user exposure to potential unexpected behavior.\n",
    "\n",
    "![Answer](images/ch_03-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shadow of the shadow\n",
    "\n",
    "So, although shadow deployment carries the lowest risk regarding user exposure to unexpected behavior, it still has its downsides. There is no free lunch.\n",
    "\n",
    "Do you remember what it is?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    It makes rollback in case of errors very difficult.\n",
    "    \n",
    "    \n",
    "    It can't work in batch mode.\n",
    "    \n",
    "    \n",
    "    Every request is processed by both the old and new model, which can increase the service latency. {Answer}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
