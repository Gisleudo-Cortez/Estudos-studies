{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Introduction_to_Deep_Learning_with_PyTorch/datasets/'\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAccessing the different attributes of a tensor is critical for debugging, as many errors are related to operations on incompatible devices or data types.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Creating tensors and accessing attributes\n",
    "\n",
    "Tensors are the primary data structure in PyTorch and will be the building blocks for our deep learning models. They share many similarities with NumPy arrays but have some unique attributes too.\n",
    "\n",
    "In this exercise, you'll practice creating a tensor from a Python list and displaying some of its attributes.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Begin by importing PyTorch.\n",
    "    Create a tensor from the Python list list_a.\n",
    "---\n",
    "\n",
    "    Display the tensor device.\n",
    "    Display the tensor data type.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "list_a = [1, 2, 3, 4]\n",
    "\n",
    "# Create a tensor from list_a\n",
    "tensor_a = torch.tensor(list_a)\n",
    "\n",
    "# Display the tensor device\n",
    "print(tensor_a.device)\n",
    "\n",
    "# Display the tensor data type\n",
    "print(tensor_a.dtype)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Accessing the different attributes of a tensor is critical for debugging, as many errors are related to operations on incompatible devices or data types.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "array_a = np.array([[1,1,1],[2,3,4],[4,5,6]])\n",
    "array_b = np.array([[7,5,4],[2,2,8],[6,3,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1],\n",
      "        [ 4,  7, 28],\n",
      "        [22, 17, 46]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! Such tensor operations are used in modern deep learning architectures and mastering them is the first step on your journey to PyTorch mastery!\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Creating tensors from NumPy arrays\n",
    "\n",
    "Tensors are the fundamental data structure of PyTorch. You can create complex deep learning algorithms by learning how to manipulate them.\n",
    "\n",
    "The torch package has been imported, and two NumPy arrays have been created, named array_a and array_b. Both arrays have the same dimensions.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create two tensors, tensor_a and tensor_b, from the NumPy arrays array_a and array_b, respectively.\n",
    "    Subtract tensor_b from tensor_a.\n",
    "    Perform an element-wise multiplication of tensor_a and tensor_b.\n",
    "    Add the resulting tensors from the two previous steps together.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create two tensors from the arrays\n",
    "tensor_a = torch.tensor(array_a)\n",
    "tensor_b = torch.tensor(array_b)\n",
    "\n",
    "# Subtract tensor_b from tensor_a \n",
    "tensor_c = tensor_a - tensor_b\n",
    "\n",
    "# Multiply each element of tensor_a with each element of tensor_b\n",
    "tensor_d = tensor_a * tensor_b\n",
    "\n",
    "# Add tensor_c with tensor_d\n",
    "tensor_e = tensor_c + tensor_d\n",
    "print(tensor_e) \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! Such tensor operations are used in modern deep learning architectures and mastering them is the first step on your journey to PyTorch mastery!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7354]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood job on creating your first neural network! In practice, you'll find that modern neural networks can contain hundreds of layers and millions of parameters. Recall that the model output is not meaningful until the model is trained, i.e. until the weights and biases of each layer can meaningfully be used to produce output.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Your first neural network\n",
    "\n",
    "In this exercise, you will implement a small neural network containing two linear layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor.\n",
    "\n",
    "The torch package and the torch.nn package have already been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Create a neural network of linear layers that takes a tensor of dimensions 1 * 8  as input and outputs a tensor of dimensions 1 * 1\n",
    ".\n",
    "Use any output dimension for the first layer you want.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(8,1),\n",
    "nn.Linear(1,1)\n",
    "                     )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job on creating your first neural network! In practice, you'll find that modern neural networks can contain hundreds of layers and millions of parameters. Recall that the model output is not meaningful until the model is trained, i.e. until the weights and biases of each layer can meaningfully be used to produce output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6900]])\n",
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNice work! In subsequent lessons, we'll work on using nn.Sequential() to directly add linear layers, as well as the nn.Softmax() or nn.Sigmoid() activation functions to neural networks.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Using the sigmoid and softmax functions\n",
    "\n",
    "The sigmoid and softmax functions are two of the most popular activation functions in deep learning. They are both usually used as the last step of a neural network. Sigmoid functions are used for binary classification problems, whereas softmax functions are often used for multi-class classification problems. This exercise will familiarize you with creating and using both functions.\n",
    "\n",
    "Let's say that you have a neural network that returned the values contained in the score tensor as a pre-activation output. You will apply activation functions to this output.\n",
    "\n",
    "torch.nn is already imported as nn.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Create a sigmoid function and apply it on the score tensor to generate a probability.\n",
    "\n",
    "Create a softmax function and apply it on the score tensor to generate a probability.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "score = torch.tensor([[0.8]])\n",
    "\n",
    "# Create a sigmoid function and apply it on the score tensor\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(score)\n",
    "print(probability)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "scores = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on the score tensor\n",
    "softmax = nn.Softmax(dim = -1)\n",
    "probabilities = softmax(scores)\n",
    "print(probabilities)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! In subsequent lessons, we'll work on using nn.Sequential() to directly add linear layers, as well as the nn.Softmax() or nn.Sigmoid() activation functions to neural networks.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
