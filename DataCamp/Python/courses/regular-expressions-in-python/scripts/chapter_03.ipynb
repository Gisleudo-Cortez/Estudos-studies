{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = '@robot9! @robot4& I have a good feeling that the show isgoing to be amazing! @robot9$ @robot7%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@robot9!', '@robot4&', '@robot9$', '@robot7%']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! The advantage of regular expressions is that you can use both normal characters and metacharacters. You can easily extract complex patterns, which would be more complicated otherwise.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Are they bots?\n",
    "\n",
    "The company that you are working for asked you to perform a sentiment analysis using a dataset with tweets. First of all, you need to do some cleaning and extract some information.\n",
    "While printing out some text, you realize that some tweets contain user mentions. Some of these mentions follow a very strange pattern. A few examples that you notice: @robot3!, @robot5& and @robot7#\n",
    "\n",
    "To analyze if those users are bots, you will do a proof of concept with one tweet and extract them using the .findall() method.\n",
    "\n",
    "You write down some helpful metacharacters to help you later:\n",
    "\n",
    "\\d: digit\n",
    "\\w: word character\n",
    "\\W: non-word character\n",
    "\\s: whitespace\n",
    "\n",
    "The text of one tweet was saved in the variable sentiment_analysis. You can use print(sentiment_analysis) to view it in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the re module.\n",
    "    Write a regex that matches the user mentions that starts with @ and follows the pattern, e.g. @robot3!.\n",
    "    Find all the matches of the pattern in the sentiment_analysis variable.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the re module\n",
    "import re\n",
    "\n",
    "# Write the regex\n",
    "regex = r\"@robot\\d\\W\"\n",
    "\n",
    "# Find all matches of regex\n",
    "print(re.findall(regex, sentiment_analysis))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! The advantage of regular expressions is that you can use both normal characters and metacharacters. You can easily extract complex patterns, which would be more complicated otherwise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = \"Unfortunately one of those moments wasn't a giant squid monster. User_mentions:2, likes: 9, number of retweets: 7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User_mentions:2']\n",
      "['likes: 9']\n",
      "['number of retweets: 7']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice job! Using metacharacters in regular expressions will allow you to match types of characters such as digits. You can encounter many forms of whitespace such as tabs, space or new line. To make sure you match all of them always specify whitespaces as \\\\s.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Find the numbers\n",
    "\n",
    "While examining the tweet text in your dataset, you detect that some tweets carry more information. The text contains the number of retweets, user mentions, and likes. You decide to extract this important information that is given as in this example:\n",
    "\n",
    "Agh,snow! User_mentions:9, likes: 5, number of retweets: 4\n",
    "\n",
    "You pull a list of metacharacters:\\d digit,\\w word character,\\s whitespace.\n",
    "\n",
    "Always indicate whitespace with metacharacters.\n",
    "\n",
    "The variable sentiment_analysis containing the text of one tweet and the re module were loaded in your session. You can use print() to view it in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Write a regex that matches the number of user mentions given as, for example, User_mentions:9 in sentiment_analysis.\n",
    "---\n",
    "Write a regex that matches the number of likes given as, for example, likes: 5 in sentiment_analysis.\n",
    "---\n",
    "Write a regex that matches the number of retweets given as, for example, number of retweets: 4 in sentiment_analysis.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a regex to obtain user mentions\n",
    "print(re.findall(r\"User_mentions:\\d\", sentiment_analysis))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Write a regex to obtain number of likes\n",
    "print(re.findall(r\"likes:\\s\\d\", sentiment_analysis))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Write a regex to obtain number of retweets\n",
    "print(re.findall(r\"number\\sof\\sretweets:\\s\\d\", sentiment_analysis))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job! Using metacharacters in regular expressions will allow you to match types of characters such as digits. You can encounter many forms of whitespace such as tabs, space or new line. To make sure you match all of them always specify whitespaces as \\s.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = 'He#newHis%newTin love with$newPscrappy. #8break%He is&newYmissing him@newLalready'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: He#newHis%newTin love with$newPscrappy. #8break%He is&newYmissing him@newLalready\n",
      "He is in love with scrappy.  He is missing him already\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! Regular expressions are very useful for finding and replacing complex patterns in text. Using them will make your analysis cleaner and faster.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Match and split\n",
    "\n",
    "Some of the tweets in your dataset were downloaded incorrectly. Instead of having spaces to separate words, they have strange characters. You decide to use regular expressions to handle this situation. You print some of these tweets to understand which pattern you need to match.\n",
    "\n",
    "You notice that the sentences are always separated by a special character, followed by a number, the word break, and after that, another special character, e.g &4break!. The words are always separated by a special character, the word new, and a normal random character, e.g #newH.\n",
    "\n",
    "The variable sentiment_analysis containing the text of one tweet, as well as the re module were already loaded in your session. You can use print(sentiment_analysis) to view it in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write a regex that matches the pattern separating the sentences in sentiment_analysis, e.g. &4break!.\n",
    "---\n",
    "\n",
    "    Replace regex_sentence with a space \" \" in the variable sentiment_analysis. Assign it to sentiment_sub.\n",
    "---\n",
    "\n",
    "    Write a regex that matches the pattern separating the words in sentiment_analysis, e.g. #newH.\n",
    "---\n",
    "\n",
    "    Replace regex_words with a space in the variable sentiment_sub. Assign it to sentiment_final and print out the result.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "print(f\"Original: {sentiment_analysis}\")\n",
    "\n",
    "# Write a regex to match pattern separating sentences\n",
    "regex_sentence = r\"\\W\\dbreak\\W\"\n",
    "\n",
    "# Replace the regex_sentence with a space\n",
    "sentiment_sub = re.sub(regex_sentence, \" \", sentiment_analysis)\n",
    "\n",
    "# Write a regex to match pattern separating words\n",
    "regex_words = r\"\\Wnew\\w\"\n",
    "\n",
    "# Replace the regex_words and print the result\n",
    "sentiment_final = re.sub(regex_words, \" \", sentiment_sub)\n",
    "print(sentiment_final)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Regular expressions are very useful for finding and replacing complex patterns in text. Using them will make your analysis cleaner and faster.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545    Boredd. Colddd @blueKnight39 Internet keeps st...\n",
       "546    I had a horrible nightmare last night @anitaLo...\n",
       "547    im lonely  keep me company @YourBestCompany! @...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(path_data+'short_tweets.csv')\n",
    "data.head()\n",
    "\n",
    "sentiment_analysis = data.iloc[545:548, -1]\n",
    "display(sentiment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.tellyourstory.com']\n",
      "['@blueKnight39']\n",
      "[]\n",
      "['@anitaLopez98', '@MyredHat31']\n",
      "['https://radio.foxnews.com']\n",
      "['@YourBestCompany', '@foxRadio']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! Regular expressions provide very useful metacharacters that you should not forget to use. \\\\S is an example. It is very useful to use when you know a pattern doesn't contain spaces and you have reached the end when you do find one.\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Everything clean\n",
    "\n",
    "Back to your Twitter sentiment analysis project! There are several types of strings that increase your sentiment analysis complexity. But these strings do not provide any useful sentiment. Among them, we can have links and user mentions.\n",
    "\n",
    "In order to clean the tweets, you want to extract some examples first. You know that most of the times links start with http and do not contain any whitespace, e.g. https://www.datacamp.com. User mentions start with @ and can have letters and numbers only, e.g. @johnsmith3.\n",
    "\n",
    "You write down some helpful quantifiers to help you: * zero or more times, + once or more, ? zero or once.\n",
    "\n",
    "The list sentiment_analysis containing the text of three tweets are already loaded in your session. You can use print() to view the data in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the re module.\n",
    "    Write a regex to find all the matches of http links appearing in each tweet in sentiment_analysis. Print out the result.\n",
    "    Write a regex to find all the matches of user mentions appearing in each tweet in sentiment_analysis. Print out the result.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import re module\n",
    "import re\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "\t# Write regex to match http links and print out result\n",
    "\tprint(re.findall(r\"https?\\S+\", tweet))\n",
    "\n",
    "\t# Write regex to match user mentions and print out result\n",
    "\tprint(re.findall(r\"@\\w+\\d*\", tweet))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Regular expressions provide very useful metacharacters that you should not forget to use. \\S is an example. It is very useful to use when you know a pattern doesn't contain spaces and you have reached the end when you do find one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232    I would like to apologize for the repeated Vid...\n",
       "233    @zaydia but i cant figure out how to get there...\n",
       "234    FML: So much for seniority, bc of technologica...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis = data.iloc[232:235, -1]\n",
    "sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['32 minutes ago']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1st May 2019']\n",
      "['23rd June 2018']\n",
      "[]\n",
      "[]\n",
      "['23rd June 2018 17:54']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAmazing job! Handling dates can become a really difficult task. Fortunately, regular expressions can simplify this task!\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Some time ago\n",
    "\n",
    "You are interested in knowing when the tweets were posted. After reading a little bit more, you learn that dates are provided in different ways. You decide to extract the dates using .findall() so you can normalize them afterwards to make them all look the same.\n",
    "\n",
    "You realize that the dates are always presented in one of the following ways:\n",
    "\n",
    "27 minutes ago\n",
    "\n",
    "4 hours ago\n",
    "\n",
    "23rd june 2018\n",
    "\n",
    "1st september 2019 17:25\n",
    "\n",
    "The list sentiment_analysis containing the text of three tweets, as well as the re module are already loaded in your session. You can use print() to view the data in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Complete the for-loop with a regex that finds all dates in a format similar to 27 minutes ago or 4 hours ago.\n",
    "---\n",
    "Complete the for-loop with a regex that finds all dates in a format similar to 23rd june 2018.\n",
    "---\n",
    "Complete the for-loop with a regex that finds all dates in a format similar to 1st september 2019 17:25.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Complete the for loop with a regex to find dates\n",
    "for date in sentiment_analysis:\n",
    "\tprint(re.findall(r\"\\d{1,2}\\s\\w+\\sago\", date))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Complete the for loop with a regex to find dates\n",
    "for date in sentiment_analysis:\n",
    "\tprint(re.findall(r\"\\d{1,2}\\w+\\s\\w+\\s\\d{4}\", date))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Complete the for loop with a regex to find dates\n",
    "for date in sentiment_analysis:\n",
    "\tprint(re.findall(r\"\\d{1,2}\\w+\\s\\w*\\s\\d{4}\\s\\d{1,2}:\\d{2}\", date))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Amazing job! Handling dates can become a really difficult task. Fortunately, regular expressions can simplify this task!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = 'ITS NOT ENOUGH TO SAY THAT IMISS U #MissYou #SoMuch #Friendship #Forever'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITS', 'NOT', 'ENOUGH', 'TO', 'SAY', 'THAT', 'IMISS', 'U', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! Regular expressions can be very useful when replacing and splitting string using complex patterns.\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Getting tokens\n",
    "\n",
    "Your next step is to tokenize the text of your tweets. Tokenization is the process of breaking a string into lexical units or, in simpler terms, words. But first, you need to remove hashtags so they do not cloud your process. You realize that hashtags start with a # symbol and contain letters and numbers but never whitespace. After that, you plan to split the text at whitespace matches to get the tokens.\n",
    "\n",
    "You bring your list of quantifiers to help you: * zero or more times, + once or more, ? zero or once, {n, m} minimum n, maximum m.\n",
    "\n",
    "The variable sentiment_analysis containing the text of one tweet as well as the re module are already loaded in your session. You can use print(sentiment_analysis) to view it in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write a regex that matches the described hashtag pattern. Assign it to the regex variable.\n",
    "---\n",
    "\n",
    "    Replace all the matches of the regex with an empty string \"\". Assign it to no_hashtag variable.\n",
    "---\n",
    "\n",
    "    Split the text in the no_hashtag variable at every match of one or more consecutive whitespace.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a regex matching the hashtag pattern\n",
    "regex = r\"#\\w+\"\n",
    "\n",
    "# Replace the regex by an empty string\n",
    "no_hashtag = re.sub(regex, \"\", sentiment_analysis)\n",
    "\n",
    "# Get tokens by splitting text\n",
    "print(re.split(r\"\\s+\", no_hashtag))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Regular expressions can be very useful when replacing and splitting string using complex patterns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780    AIshadowhunters.txt aaaaand back to my literat...\n",
       "781    ouMYTAXES.txt I am worried that I won't get my...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis = data.iloc[780:782, -1]\n",
    "sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "AIshadowhunters.txt aaaaand back to my literature review. At least i have a friendly cup of coffee to keep me company\n",
      "[]\n",
      "ouMYTAXES.txt I am worried that I won't get my $900 even though I paid tax last year\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! The dot . metacharacter is very useful when we want to match all repetitions of any character. However, we need to be very careful how we use it.\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Finding files\n",
    "\n",
    "You are not satisfied with your tweets dataset cleaning. There are still extra strings that do not provide any sentiment. Among them are strings that refer to text file names.\n",
    "\n",
    "You also find a way to detect them:\n",
    "\n",
    "    They appear at the start of the string.\n",
    "    They always start with a sequence of 2 or 3 upper or lowercase vowels (a e i o u).\n",
    "    They always finish with the txt ending.\n",
    "\n",
    "You are not sure if you should remove them directly. So you write a script to find and store them in a separate dataset.\n",
    "\n",
    "You write down some metacharacters to help you: ^ anchor to beginning, . any character.\n",
    "\n",
    "The variable sentiment_analysis containing the text of two tweets as well as the re module are already loaded in your session. You can use print() to view it in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write a regex that matches the pattern of the text file names, e.g. aemyfile.txt.\n",
    "    Find all matches of the regex in the elements of sentiment_analysis. Print out the result.\n",
    "    Replace all matches of the regex with an empty string \"\". Print out the result.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a regex to match text file name\n",
    "regex = r\"^[aeiouAEIOU]{2,3}.txt\"\n",
    "\n",
    "for text in sentiment_analysis:\n",
    "\t# Find all matches of the regex\n",
    "\tprint(re.findall(regex, text))\n",
    "    \n",
    "\t# Replace all matches with empty string\n",
    "\tprint(re.sub(regex, \"\", text))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! The dot . metacharacter is very useful when we want to match all repetitions of any character. However, we need to be very careful how we use it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = ['n.john.smith@gmail.com', '87victory@hotmail.com', '!#mary-=@msca.net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email n.john.smith@gmail.com is a valid email\n",
      "The email 87victory@hotmail.com is a valid email\n",
      "The email !#mary-=@msca.net is invalid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! Validating strings is a task that becomes simpler when we use regular expressions. Square brackets are very useful for optional characters. Notice that we used the .match() method. The reason is that we want to match the pattern from the beginning of the string.\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Give me your email\n",
    "\n",
    "A colleague has asked for your help! When a user signs up on the company website, they must provide a valid email address.\n",
    "The company puts some rules in place to verify that the given email address is valid:\n",
    "\n",
    "    The first part can contain:\n",
    "        Upper A-Z or lowercase letters a-z\n",
    "        Numbers\n",
    "        Characters: !, #, %, &, *, $, .\n",
    "    Must have @\n",
    "    Domain:\n",
    "        Can contain any word characters\n",
    "        But only .com ending is allowed\n",
    "\n",
    "The project consists of writing a script that checks if the email address follow the correct pattern. Your colleague gave you a list of email addresses as examples to test.\n",
    "\n",
    "The list emails as well as the re module are loaded in your session. You can use print(emails) to view the emails in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write a regular expression to match valid email addresses as described.\n",
    "    Match the regex to the elements contained in emails.\n",
    "    To print out the message indicating if it is a valid email or not, complete .format() statement.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a regex to match a valid email address\n",
    "regex = r\"[A-Za-z0-9!#%&*$.].@\\w+\\.com\"\n",
    "\n",
    "for example in emails:\n",
    "  \t# Match the regex to the string\n",
    "    if re.findall(regex, example):\n",
    "        # Complete the format method to print out the result\n",
    "      \tprint(\"The email {email_example} is a valid email\".format(email_example=example))\n",
    "    else:\n",
    "      \tprint(\"The email {email_example} is invalid\".format(email_example=example))   \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Validating strings is a task that becomes simpler when we use regular expressions. Square brackets are very useful for optional characters. Notice that we used the .match() method. The reason is that we want to match the pattern from the beginning of the string.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "passwords = ['Apple34!rose', 'My87hou#4$', 'abc123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The password Apple34!rose is a valid password\n",
      "The password My87hou#4$ is a valid password\n",
      "The password abc123 is invalid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! Notice how metacharacters help you perform complex pattern matching in a few lines of code. For that reason, Regex has many applications in Data Science, particularly in Text Mining and Natural Language Processing.\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Invalid password\n",
    "\n",
    "The second part of the website project is to write a script that validates the password entered by the user. The company also puts some rules in order to verify valid passwords:\n",
    "\n",
    "    It can contain lowercase a-z and uppercase letters A-Z\n",
    "    It can contain numbers\n",
    "    It can contain the symbols: *, #, $, %, !, &, .\n",
    "    It must be at least 8 characters long but not more than 20\n",
    "\n",
    "Your colleague also gave you a list of passwords as examples to test.\n",
    "\n",
    "The list passwords and the module re are loaded in your session. You can use print(passwords) to view them in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write a regular expression to check if the passwords are valid according to the description.\n",
    "    Search the elements in the passwords list to find out if they are valid passwords.\n",
    "    To print out the message indicating if it is a valid password or not, complete .format() statement.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a regex to check if the password is valid\n",
    "regex = r\"[a-zA-z0-9*#$%!&.]{8,20}\"\n",
    "\n",
    "for example in passwords:\n",
    "  \t# Scan the strings to find a match\n",
    "    if re.findall(regex, example):\n",
    "        # Complete the format method to print out the result\n",
    "      \tprint(\"The password {pass_example} is a valid password\".format(pass_example=example))\n",
    "    else:\n",
    "      \tprint(\"The password {pass_example} is invalid\".format(pass_example=example))   \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Notice how metacharacters help you perform complex pattern matching in a few lines of code. For that reason, Regex has many applications in Data Science, particularly in Text Mining and Natural Language Processing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'I want to see that <strong>amazing show</strong> again!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to see that amazing show again!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! Remember that a greedy quantifier will try to match as much as possible while a non-greedy quantifier will do it as few times as needed, expanding one character at a time and giving us the match we are looking for. Good!\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Understanding the difference\n",
    "\n",
    "You need to keep working and cleaning your tweets dataset. You realize that there are some HTML tags present. You need to remove them but keep the inside content as they are useful for analysis.\n",
    "\n",
    "Let's take a look at this sentence containing an HTML tag:\n",
    "\n",
    "I want to see that <strong>amazing show</strong> again!.\n",
    "\n",
    "You know that to get the HTML tag you need to match anything that sits inside angle brackets < >. But the biggest problem is that the closing tag has the same structure. If you match too much, you will end up removing key information. So you need to decide whether to use a greedy or a lazy quantifier.\n",
    "\n",
    "The string is already loaded as string to your session.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the re module.\n",
    "    Write a regex expression to replace HTML tags with an empty string.\n",
    "    Print out the result.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import re\n",
    "import re\n",
    "\n",
    "# Write a regex to eliminate tags\n",
    "string_notags = re.sub(r\"<.+?>\", \"\", string)\n",
    "\n",
    "# Print out the result\n",
    "print(string_notags)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Remember that a greedy quantifier will try to match as much as possible while a non-greedy quantifier will do it as few times as needed, expanding one character at a time and giving us the match we are looking for. Good!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = 'Was intending to finish editing my 536-page novel manuscript tonight, but that will probably not happen. And only 12 pages are left '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '3', '6', '1', '2']\n",
      "['536', '12']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! Even though greedy quantifiers lead to longer matches, they are sometimes the best option. Because lazy quantifiers match as few as possible, they return a shorter match than we expected. It is always good to know when to use greedy and lazy quantifiers!\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Greedy matching\n",
    "\n",
    "Next, you see that numbers still appear in the text of the tweets. So, you decide to find all of them.\n",
    "\n",
    "Let's imagine that you want to extract the number contained in the sentence I was born on April 24th. A lazy quantifier will make the regex return 2 and 4, because they will match as few characters as needed. However, a greedy quantifier will return the entire 24 due to its need to match as much as possible.\n",
    "\n",
    "The re module as well as the variable sentiment_analysis are already loaded in your session. You can use print(sentiment_analysis) to view it in the IPython Shell.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Use a lazy quantifier to match all numbers that appear in the variable sentiment_analysis.\n",
    "---\n",
    "Now, use a greedy quantifier to match all numbers that appear in the variable sentiment_analysis.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a lazy regex expression \n",
    "numbers_found_lazy = re.findall(r\"[0-9]+?\", sentiment_analysis)\n",
    "\n",
    "# Print out the result\n",
    "print(numbers_found_lazy)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Write a greedy regex expression \n",
    "numbers_found_greedy = re.findall(r\"[0-9]+\", sentiment_analysis)\n",
    "\n",
    "# Print out the result\n",
    "print(numbers_found_greedy)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Even though greedy quantifiers lead to longer matches, they are sometimes the best option. Because lazy quantifiers match as few as possible, they return a shorter match than we expected. It is always good to know when to use greedy and lazy quantifiers!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = \"Put vacation photos online (They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying). \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"(They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying)\"]\n",
      "['(They were so cute)', \"(I'm crying)\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! Notice that using greedy quantifiers always leads to longer matches that sometimes are not desired. Making quantifiers lazy by adding ? to match a shorter pattern is a very important consideration to keep in mind when handling data for text mining. Cool!\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 12\n",
    "\n",
    "\"\"\"\n",
    "Lazy approach\n",
    "\n",
    "You have done some cleaning in your dataset but you are worried that there are sentences encased in parentheses that may cloud your analysis.\n",
    "\n",
    "Again, a greedy or a lazy quantifier may lead to different results.\n",
    "\n",
    "For example, if you want to extract a word starting with a and ending with e in the string I like apple pie, you may think that applying the greedy regex a.+e will return apple. However, your match will be apple pie. A way to overcome this is to make it lazy by using ? which will return apple.\n",
    "\n",
    "The re module and the variable sentiment_analysis are already loaded in your session.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Use a greedy quantifier to match text that appears within parentheses in the variable sentiment_analysis.\n",
    "---\n",
    "Now, use a lazy quantifier to match text that appears within parentheses in the variable sentiment_analysis.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a greedy regex expression to match \n",
    "sentences_found_greedy = re.findall(r\"\\(.*\\)\", sentiment_analysis)\n",
    "\n",
    "# Print out the result\n",
    "print(sentences_found_greedy)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Write a lazy regex expression\n",
    "sentences_found_lazy = re.findall(r\"\\(.*?\\)\", sentiment_analysis)\n",
    "\n",
    "# Print out the results\n",
    "print(sentences_found_lazy)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Notice that using greedy quantifiers always leads to longer matches that sometimes are not desired. Making quantifiers lazy by adding ? to match a shorter pattern is a very important consideration to keep in mind when handling data for text mining. Cool!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
