{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "metadata": {},
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nero/Documents/Estudos/DataCamp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Ingesting JSON data with pandas\n",
    "\n",
    "When developing a data pipeline, you may have to work with non-tabular data and data sources, such as APIs or JSON files. In this exercise, we'll practice extracting data from a JSON file using pandas.\n",
    "\n",
    "pandas has been imported as pd, and the JSON file you'll ingest is stored at the path \"testing_scores.json\".\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Update the extract() function read a JSON file into a pandas DataFrame, orienting by records.\n",
    "\n",
    "    Pass the path testing_scores.json to the extract() function, and store the output to a variable called raw_testing_scores.\n",
    "\n",
    "    Print the head of the raw_testing_scores DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def extract(file_path):\n",
    "  # Read the JSON file into a DataFrame\n",
    "  return pd.read_json(file_path, orient=\"records\")\n",
    "\n",
    "# Call the extract function with the appropriate path, assign to raw_testing_scores\n",
    "raw_testing_scores = extract('testing_scores.json')\n",
    "\n",
    "# Output the head of the DataFrame\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent data extracting! Ingestiong JSON files into pandas DataFrames is the first step in preparing non-tabular data for further transformation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Reading JSON data into memory\n",
    "\n",
    "When data is stored in JSON format, it's not always easy to load into a DataFrame. This is the case for the \"nested_testing_scores.json\" file. Here, the data will have to be manually manipulated before it can be stored in a DataFrame.\n",
    "\n",
    "To help get you started, pandas has been loaded into the workspace as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Use pandas to read the JSON file into a DataFrame.\n",
    "    Pass the \"nested_scores.json\" file to the extract() function.\n",
    "---\n",
    "    Import the json library.\n",
    "    Use the json library to load the \"nested_scores.json\" file into memory.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def extract(file_path):\n",
    "  \t# Read the JSON file into a DataFrame, orient by index\n",
    "\treturn pd.read_json(file_path, orient=\"index\")\n",
    "\n",
    "# Call the extract function, pass in the desired file_path\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Import the json library\n",
    "import json\n",
    "\n",
    "def extract(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        # Load the data from the JSON file\n",
    "        raw_data = json.load(json_file)\n",
    "    return raw_data\n",
    "\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "\n",
    "# Print the raw_testing_scores\n",
    "print(raw_testing_scores)\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You're off to a great start! The data from the JSON file has been loaded into a dictionary in-memory.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Iterating over dictionaries\n",
    "\n",
    "Once JSON data is loaded into a dictionary, you can leverage Python's built-in tools to iterate over its keys and values.\n",
    "\n",
    "The \"nested_school_scores.json\" file has been read into a dictionary stored in the raw_testing_scores variable, which takes the following form:\n",
    "\n",
    "{\n",
    "    \"01M539\": {\n",
    "        \"street_address\": \"111 Columbia Street\",\n",
    "        \"city\": \"Manhattan\",\n",
    "        \"scores\": {\n",
    "              \"math\": 657,\n",
    "              \"reading\": 601,\n",
    "              \"writing\": 601\n",
    "        }\n",
    "  }, ...\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Loop through the keys of the raw_testing_scores dictionary.\n",
    "    Add each key to the raw_testing_scores_keys list.\n",
    "---\n",
    "    Now, loop through a list of values from the raw_testing_scores dictionary.\n",
    "---\n",
    "    Finally, loop through both the keys and values of the raw_testing_scores dictionary, simultaneously.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "raw_testing_scores_keys = []\n",
    "\n",
    "# Iterate through the keys of the raw_testing_scores dictionary\n",
    "for school_id in raw_testing_scores.keys():\n",
    "  \t# Append each key to the raw_testing_scores_keys list\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "    \n",
    "print(raw_testing_scores_keys[0:3])\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "raw_testing_scores_values = []\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_info in raw_testing_scores.values():\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "    \n",
    "print(raw_testing_scores_values[0:3])\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "raw_testing_scores_keys = []\n",
    "raw_testing_scores_values = []\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "\n",
    "print(raw_testing_scores_keys[0:3])\n",
    "print(raw_testing_scores_values[0:3])\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great iteration! Iterating through the both the keys and values of dictionaries lays a foundation for working with non-tabluar JSON data. Keep up the good work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Parsing data from dictionaries\n",
    "\n",
    "When JSON data is loaded into memory, the resulting dictionary can be complicated. Key-value pairs may contain another dictionary, such are called nested dictionaries. These nested dictionaries are frequently encountered when dealing with APIs or other JSON data. In this exercise, you will practice extracting data from nested dictionaries and handling missing values.\n",
    "\n",
    "The dictionary below is stored in the school variable. Good luck!\n",
    "\n",
    "{\n",
    "    \"street_address\": \"111 Columbia Street\",\n",
    "    \"city\": \"Manhattan\",\n",
    "    \"scores\": {\n",
    "        \"math\": 657,\n",
    "        \"reading\": 601\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Parse the value stored at the \"street_address\" key from the school dictionary.\n",
    "\n",
    "    Parse the value stored at the \"scores\" key from the school dictionary.\n",
    "\n",
    "    Parse the values stored at the \"math\", \"reading\", and \"writing\" keys from the scores dictionary, and set the default value to 0.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Parse the street_address from the dictionary\n",
    "street_address = school.get(\"street_address\")\n",
    "\n",
    "# Parse the scores dictionary\n",
    "scores = school.get(\"scores\")\n",
    "\n",
    "# Try to parse the math, reading and writing values from scores\n",
    "math_score = scores.get(\"math\", 0)\n",
    "reading_score = scores.get('reading',0)\n",
    "writing_score = scores.get('writing',0)\n",
    "\n",
    "print(f\"Street Address: {street_address}\")\n",
    "print(f\"Math: {math_score}, Reading: {reading_score}, Writing: {writing_score}\")\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Understanding how to pull data from nested dictionaries is a valuable skill when working with non-tabluar data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Transforming JSON data\n",
    "\n",
    "Chances are, when reading data from JSON format into a dictionary, you'll probably have to apply some level of manual transformation to the data before it can be stored in a DataFrame. This is common when working with nested dictionaries, which you'll have the opportunity to explore in this exercise.\n",
    "\n",
    "The \"nested_school_scores.json\" file has been read into a dictionary available in the raw_testing_scores variable, which takes the following form:\n",
    "\n",
    "{\n",
    "    \"01M539\": {\n",
    "        \"street_address\": \"111 Columbia Street\",\n",
    "        \"city\": \"Manhattan\",\n",
    "        \"scores\": {\n",
    "              \"math\": 657,\n",
    "              \"reading\": 601,\n",
    "              \"writing\": 601\n",
    "        }\n",
    "  }, ...\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Loop through both the keys and values of the raw_testing_scores dictionary.\n",
    "\n",
    "    Extract the \"street_address\" from each dictionary nested in the raw_testing_scores object.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "normalized_testing_scores = []\n",
    "\n",
    "# Loop through each of the dictionary key-value pairs\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\tnormalized_testing_scores.append([\n",
    "    \tschool_id,\n",
    "    \tschool_info.get(\"street_address\"),  # Pull the \"street_address\"\n",
    "    \tschool_info.get(\"city\"),\n",
    "    \tschool_info.get(\"scores\").get(\"math\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"reading\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"writing\", 0),\n",
    "    ])\n",
    "\n",
    "print(normalized_testing_scores)\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Outstanding! Using the json library and native-Python, you've extracted the JSON file into a list of lists.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Transforming and cleaning DataFrames\n",
    "\n",
    "Once data has been curated into a cleaned Python data structure, such as a list of lists, it's easy to convert this into a pandas DataFrame. You'll practice doing just this with the data that was curated in the last exercise.\n",
    "\n",
    "Per usual, pandas has been imported as pd, and the normalized_testing_scores variable stores the list of each schools testing data, as shown below.\n",
    "\n",
    "[\n",
    "    ['01M539', '111 Columbia Street', 'Manhattan', 657.0, 601.0, 601.0],\n",
    "    ...\n",
    "]   \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Create a pandas DataFrame from the list of lists stored in the normalized_testing_scores variable.\n",
    "\n",
    "    Set the columns names for the normalized_data DataFrame.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a DataFrame from the normalized_testing_scores list\n",
    "normalized_data = pd.DataFrame(normalized_testing_scores)\n",
    "\n",
    "# Set the column names\n",
    "normalized_data.columns = [\"school_id\", \"street_address\", \"city\", \"avg_score_math\", \"avg_score_reading\", \"avg_score_writing\"]\n",
    "\n",
    "normalized_data = normalized_data.set_index(\"school_id\")\n",
    "print(normalized_data.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congrats! You've extracted a JSON file, and manipulated it such that it can be stored in a pandas DataFrame for downstream transformation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Filling missing values with pandas\n",
    "\n",
    "When building data pipelines, it's inevitable that you'll stumble upon missing data. In some cases, you may want to remove these records from the dataset. But in others, you'll need to impute values for the missing information. In this exercise, you'll practice using pandas to impute missing test scores.\n",
    "\n",
    "Data from the file \"testing_scores.json\" has been read into a DataFrame, and is stored in the variable raw_testing_scores. In addition to this, pandas has been loaded as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Print the head of the raw_testing_scores DataFrame, and observe the NaN values.\n",
    "---\n",
    "\n",
    "\n",
    "    Use the average of the \"math_score\" column to fill the NaN values in the \"math_score\" column.\n",
    "    Print the head of the updated DataFrame.\n",
    "---\n",
    "\n",
    "\n",
    "    For the \"math_score\", \"reading_score\" and \"writing_score\" columns, update the transform() function to fill NaN values with the mean of the respective columns, in place.\n",
    "    Print the head of the cleaned DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Print the head of the `raw_testing_scores` DataFrame\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Fill NaN values with the average from that column\n",
    "raw_testing_scores[\"math_score\"] = raw_testing_scores[\"math_score\"].fillna(raw_testing_scores[\"math_score\"].mean())\n",
    "\n",
    "# Print the head of the raw_testing_scores DataFrame\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "def transform(raw_data):\n",
    "\traw_data.fillna(\n",
    "    \tvalue={\n",
    "\t\t\t# Fill NaN values with column mean\n",
    "\t\t\t\"math_score\": raw_data[\"math_score\"].mean(),\n",
    "\t\t\t\"reading_score\": raw_data['reading_score'].mean(),\n",
    "\t\t\t\"writing_score\": raw_data['writing_score'].mean()\n",
    "\t\t}, inplace=True\n",
    "\t)\n",
    "\treturn raw_data\n",
    "\n",
    "clean_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the clean_testing_scores DataFrame\n",
    "print(clean_testing_scores.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nicely done! Working with missing values is something that takes practice, and an understanding of the problem at hand. Thanks to pandas, it's easy to implement a wide variety of logic using the .fillna() method. Keep up the great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Grouping data with pandas\n",
    "\n",
    "The output of a data pipeline is typically a \"modeled\" dataset. This dataset provides data consumers easy access to information, without having to perform much manipulation. Grouping data with pandas helps to build modeled datasets,\n",
    "\n",
    "pandas has been imported as pd, and the raw_testing_scores DataFrame contains data in the following form:\n",
    "\n",
    "              street_address       city  math_score  reading_score  writing_score\n",
    "01M539   111 Columbia Street  Manhattan       657.0          601.0          601.0\n",
    "02M294      350 Grand Street  Manhattan       395.0          411.0          387.0\n",
    "02M308      350 Grand Street  Manhattan       418.0          428.0          415.0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Use .loc[] to only keep the \"city\", \"math_score\", \"reading_score\", and \"writing_score\" columns.\n",
    "    Group the DataFrame by the \"city\" column, and find the mean of each city's math, reading, and writing scores.\n",
    "    Use the transform() function to create a grouped DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def transform(raw_data):\n",
    "\t# Use .loc[] to only return the needed columns\n",
    "\traw_data = raw_data.loc[:, ['city','math_score','reading_score','writing_score']]\n",
    "\t\n",
    "    # Group the data by city, return the grouped DataFrame\n",
    "\tgrouped_data = raw_data.groupby(by=[\"city\"], axis=0).mean()\n",
    "\treturn grouped_data\n",
    "\n",
    "# Transform the data, print the head of the DataFrame\n",
    "grouped_testing_scores = transform(raw_testing_scores)\n",
    "print(grouped_testing_scores.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great grouping! Leveraging pandas' aggregation capabilities help to create report-ready datasets for downstream data consumers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_street_name(row):\n",
    "\t# Split the street_address by spaces\n",
    "    split_street_address = row[\"street_address\"].split(\" \")\n",
    "    \n",
    "    # Remove the number\n",
    "    street_number = split_street_address[0]\n",
    "    try:\n",
    "    \tint(street_number)\n",
    "    except ValueError:\n",
    "    \treturn row[\"street_address\"]\n",
    "      \n",
    "    return \" \".join(split_street_address[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Applying advanced transformations to DataFrames\n",
    "\n",
    "pandas has a plethora of built-in transformation tools, but sometimes, more advanced logic needs to be used in a transformation. The apply function lets you apply a user-defined function to a row or column of a DataFrame, opening the door for advanced transformation and feature generation.\n",
    "\n",
    "The find_street_name() function parses the street name from the \"street_address\", dropping the street number from the string. This function has been loaded into memory, and is ready to be applied to the raw_testing_scores DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    In the definition of the transform() function, use the find_street_name() function to create a new column with the name \"street_name\".\n",
    "\n",
    "    Use the transform() function to clean the raw_testing_scores DataFrame.\n",
    "\n",
    "    Print the head of the cleaned_testing_scores DataFrame, observing the new \"street_name\" column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def transform(raw_data):\n",
    "\t# Use the apply function to extract the street_name from the street_address\n",
    "    raw_data[\"street_name\"] = raw_data.apply(\n",
    "   \t\t# Pass the correct function to the apply method\n",
    "        find_street_name,\n",
    "        axis=1\n",
    "    )\n",
    "    return raw_data\n",
    "\n",
    "# Transform the raw_testing_scores DataFrame\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the cleaned_testing_scores DataFrame\n",
    "print(cleaned_testing_scores.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Amazing stuff! Being able to 'apply' functions to a DataFrame can really help streamline data transformation, especially when the logic in a transformation is more complex than pandas can handle with its built-in functionality.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Loading data to a Postgres database\n",
    "\n",
    "After data has been extracted from a source system and transformed to align with analytics or reporting use cases, it's time to load the data to a final storage medium. Storing cleaned data in a SQL database makes it simple for data consumers to access and run queries against. In this example, you'll practice loading cleaned data to a Postgres database.\n",
    "\n",
    "sqlalchemy has been imported, and pandas is available as pd. The first few rows of the cleaned_testing_scores DataFrame are shown below:\n",
    "\n",
    "             street_address       city  math_score  ... best_score\n",
    "01M539  111 Columbia Street  Manhattan       657.0      Math\n",
    "02M545     350 Grand Street  Manhattan       613.0      Math\n",
    "01M292     220 Henry Street  Manhattan       410.0      Math\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Update the connection string to write to the schools database and create a connection object using sqlalchemy.\n",
    " \n",
    "    Use pandas to write the cleaned_testing_scores DataFrame to the scores table in the schools database.\n",
    " \n",
    "    If the table is already populated with data, make sure to replace the values with the current DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Update the connection string, create the connection object to the schools database\n",
    "db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/schools\")\n",
    "\n",
    "# Write the DataFrame to the scores_by_city table\n",
    "cleaned_testing_scores.to_sql(\n",
    "\tname=\"scores\",\n",
    "\tcon=db_engine,\n",
    "\tindex=False,\n",
    "\tif_exists=\"replace\"\n",
    ")\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Lovely loading! The .to_sql() method is a powerful tool that helps to simplify the 'load' component of ETL pipelines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Validating data loaded to a Postgres Database\n",
    "\n",
    "In this exercise, you'll finally get to build a data pipeline from end-to-end. This pipeline will extract school testing scores from a JSON file and transform the data to drop rows with missing scores. In addition to this, each will be ranked by the city they are located in, based on their total scores. Finally, the transformed dataset will be stored in a Postgres database.\n",
    "\n",
    "To give you a head start, the extract() and transform() functions have been built and used as shown below. In addition to this, pandas has been imported as pd. Best of luck!\n",
    "\n",
    "# Extract and clean the testing scores.\n",
    "raw_testing_scores = extract(\"testing_scores.json\")\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Update the load() function to write the clean_data DataFrame to the scores_by_city table in the schools database.\n",
    "    If data exists in the scores_by_city table, makes sure to replace it with the updated data.\n",
    "---\n",
    "\n",
    "    Load the data from the cleaned_testing_scores, using the db_engine that has already been defined.\n",
    "    Use pandas to read data from the scores_by_city table, and print the first few rows of the DataFrame to validate that data was persisted.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def load(clean_data, con_engine):\n",
    "\t# Store the data in the schools database\n",
    "    clean_data.to_sql(\n",
    "    \tname=\"scores_by_city\",\n",
    "\t\tcon=con_engine,\n",
    "\t\tif_exists=\"replace\",  # Make sure to replace existing data\n",
    "\t\tindex=True,\n",
    "\t\tindex_label=\"school_id\"\n",
    "    )\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "def load(clean_data, con_engine):\n",
    "    clean_data.to_sql(name=\"scores_by_city\", con=con_engine, if_exists=\"replace\", index=True, index_label=\"school_id\")\n",
    "    \n",
    "# Call the load function, passing in the cleaned DataFrame\n",
    "load(cleaned_testing_scores, db_engine)\n",
    "\n",
    "# Call query the data in the scores_by_city table, check the head of the DataFrame\n",
    "to_validate = pd.read_sql(\"SELECT * FROM scores_by_city\", con=db_engine)\n",
    "print(to_validate.head())\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Take a second to enjoy this! You just built a data pipeline that extracts data from a JSON file, transforms it, and stores it in a Postgres database for easy downstream access. Congrats!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
