{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Designing_Machine_Learning_Workflows_in_Python/datasets/'\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat, this worked as expected. The algorithm correctly identified the 10.0 as an outlier since it labeled it -1 which stands for anomalous. Let's move on.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "A simple outlier\n",
    "\n",
    "When you first encounter a new type of algorithm, it is always a great idea to test it with a very simple example. So you decide to create a list containing thirty examples with the value 1.0 and just one example with value 10.0, which you expect should be flagged as an outlier. To make sure you use the algorithm correctly, you convert the list to a pandas dataframe, and feed it into the local outlier factor algorithm. pandas is available to you as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the LocalOutlierFactor module as lof for convenience.\n",
    "    Create a list with thirty 1s followed by a 10, [1.0, 1.0, ..., 1.0, 10.0].\n",
    "    Cast the list to a data frame.\n",
    "    Print the outlier scores produced by the local outlier factor algorithm.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the LocalOutlierFactor module\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Create the list [1.0, 1.0, ..., 1.0, 10.0] as explained\n",
    "x = [1.0]*30\n",
    "x.append(10)\n",
    "\n",
    "# Cast to a data frame\n",
    "X = pd.DataFrame(x)\n",
    "\n",
    "# Fit the local outlier factor and print the outlier scores\n",
    "print(lof().fit_predict(X))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great, this worked as expected. The algorithm correctly identified the 10.0 as an outlier since it labeled it -1 which stands for anomalous. Let's move on.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrh = pd.read_csv(path_data+'arrh.csv')\n",
    "\n",
    "'''for column in arrh.columns:\n",
    "    # Replace negative values with 0\n",
    "    arrh[column] = arrh[column].apply(lambda x: max(x, 0))'''\n",
    "\n",
    "X = arrh.drop('class', axis=1)\n",
    "ground_truth = arrh['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    245\n",
       "0    207\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0]\n",
      " [ 31   0 176]\n",
      " [  2   0 243]]\n",
      "[[  0   0   0]\n",
      " [ 72   0 135]\n",
      " [ 19   0 226]]\n",
      "[[  0   0   0]\n",
      " [149   0  58]\n",
      " [ 77   0 168]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou now have much more control over the local outlier factor algorithm.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "LoF contamination\n",
    "\n",
    "Your medical advisor at the arrhythmia startup informs you that your training data might not contain all possible types of arrhythmia. How on earth will you detect these other types without any labeled examples? Could an anomaly detector tell the difference between healthy and unhealthy without access to labels? But first, you experiment with the contamination parameter to see its effect on the confusion matrix. You have LocalOutlierFactor as lof, numpy as np, the labels as ground_truth encoded in -1and 1 just like local outlier factor output, and the unlabeled training data as X.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Fit a local outlier factor and output the predictions on X and print the confusion matrix for these predictions.\n",
    "---\n",
    "Repeat but now set the proportion of datapoints to be flagged as outliers to 0.2. Print the confusion matrix.\n",
    "---\n",
    "Now set the contamination to be equal to the actual proportion of outliers in the data.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Fit the local outlier factor and output predictions\n",
    "preds = lof().fit_predict(X)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(ground_truth, preds))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Set the contamination parameter to 0.2\n",
    "preds = lof(contamination=0.2).fit_predict(X)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(ground_truth, preds))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Contamination to match outlier frequency in ground_truth\n",
    "preds = lof(\n",
    "  contamination=round(np.mean(ground_truth == True),1)).fit_predict(X)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(ground_truth, preds))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You now have much more control over the local outlier factor algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat, you are now reassured that you understand the method correctly: 10.0 is labelled as -1, which means it is considered novel.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "A simple novelty\n",
    "\n",
    "You find novelty detection more useful than outlier detection, but want to make sure it works on the simple example you came up with before. This time you will use a sequence of thirty examples all with value 1.0 as a training set, and try to see if the example 10.0 is labeled as a novelty. You have access to pandas as pd, and the LocalOutlierFactor module as lof.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a pandas DataFrame containing thirty examples all equal to 1.0.\n",
    "    Initialize a local outlier factor novelty detector.\n",
    "    Fit the detector to the training data.\n",
    "    Output the novelty label of the datapoint 10.0, casted to a DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a list of thirty 1s and cast to a dataframe\n",
    "X = pd.DataFrame([1.0]*30)\n",
    "\n",
    "# Create an instance of a lof novelty detector\n",
    "detector = lof(novelty=True)\n",
    "\n",
    "# Fit the detector to the data\n",
    "detector.fit(X)\n",
    "\n",
    "# Use it to predict the label of an example with value 10.0\n",
    "print(detector.predict(pd.DataFrame([10])))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great, you are now reassured that you understand the method correctly: 10.0 is labelled as -1, which means it is considered novel.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ground_truth, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LocalOutlierFactor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWonderful, you now have a broad selection of detectors to choose from.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Three novelty detectors\n",
    "\n",
    "Finally, you know enough to run some tests on the use of a few anomaly detectors on the arrhythmia dataset. To test their performance, you will train them on an unlabeled training dataset, but then compare their predictions to the ground truth on the test data using their method .score_samples(). This time, you will be asked to import the detectors as part of the exercise, but you do have the data X_train, X_test, y_train, y_test preloaded as usual.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the one-class SVM detector from the svm module as onesvm, fit it to the training data, and score the test data.\n",
    "---\n",
    "    Adapt your code to import the isolation forest from the ensemble module as isof, fit it and score the test data.\n",
    "---\n",
    "    Adapt your code to import the LocalOutlierFactor module as lof, fit it to the training data, and score the test data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the novelty detector\n",
    "from sklearn.svm import OneClassSVM as onesvm\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "svm_detector = onesvm().fit(X_train)\n",
    "scores = svm_detector.score_samples(X_test)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Import the novelty detector\n",
    "from sklearn.ensemble import IsolationForest as isof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "isof_detector = isof().fit(X_train)\n",
    "isof_scores = isof_detector.score_samples(X_test)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Import the novelty detector\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "lof_detector = lof(novelty=True).fit(X_train)\n",
    "lof_scores = lof_detector.score_samples(X_test)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Wonderful, you now have a broad selection of detectors to choose from.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37 22]\n",
      " [17 37]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat, you now have the same amount of control over one-class SVM as you do over other detectors.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Contamination revisited\n",
    "\n",
    "You notice that one-class SVM does not have a contamination parameter. But you know well by now that you really need a way to control the proportion of examples that are labeled as novelties in order to control your false positive rate. So you decide to experiment with thresholding the scores. The detector has been imported as onesvm, you also have available the data as X_train, X_test, y_train, y_test, numpy as np, and confusion_matrix().\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fit the 1-class SVM and score the test data.\n",
    "    Compute the observed proportion of outliers in the test data.\n",
    "    Use np.quantile() to find where to threshold the scores to achieve that proportion.\n",
    "    Use that threshold to label the test data. Print the confusion matrix.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Fit a one-class SVM detector and score the test data\n",
    "nov_det = onesvm().fit(X_train)\n",
    "scores = nov_det.score_samples(X_test)\n",
    "\n",
    "# Find the observed proportion of outliers in the test data\n",
    "prop = np.mean(y_test==1)\n",
    "\n",
    "# Compute the appropriate threshold\n",
    "threshold = np.quantile(scores, prop)\n",
    "\n",
    "# Print the confusion matrix for the thresholded scores\n",
    "print(confusion_matrix(y_test, scores > threshold))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great, you now have the same amount of control over one-class SVM as you do over other detectors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_data+'hep.csv')\n",
    "data.head()\n",
    "\n",
    "features = data.drop('HISTOLOGY', axis=1)\n",
    "labels = data['HISTOLOGY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  20.55042579, 224.91353894, ..., 147.90919512,\n",
       "         37.059007  ,  35.8036311 ],\n",
       "       [ 20.55042579,   0.        , 222.11839185, ..., 146.5846172 ,\n",
       "         41.26899563,  49.80662607],\n",
       "       [224.91353894, 222.11839185,   0.        , ...,  77.7       ,\n",
       "        232.09353718, 234.20375744],\n",
       "       ...,\n",
       "       [147.90919512, 146.5846172 ,  77.7       , ...,   0.        ,\n",
       "        155.80789454, 156.95607666],\n",
       "       [ 37.059007  ,  41.26899563, 232.09353718, ..., 155.80789454,\n",
       "          0.        ,  22.42966785],\n",
       "       [ 35.8036311 ,  49.80662607, 234.20375744, ..., 156.95607666,\n",
       "         22.42966785,   0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.52631579, 0.52631579, ..., 0.42105263, 0.63157895,\n",
       "        0.57894737],\n",
       "       [0.52631579, 0.        , 0.52631579, ..., 0.52631579, 0.73684211,\n",
       "        0.78947368],\n",
       "       [0.52631579, 0.52631579, 0.        , ..., 0.47368421, 0.63157895,\n",
       "        0.57894737],\n",
       "       ...,\n",
       "       [0.42105263, 0.52631579, 0.47368421, ..., 0.        , 0.52631579,\n",
       "        0.52631579],\n",
       "       [0.63157895, 0.73684211, 0.63157895, ..., 0.52631579, 0.        ,\n",
       "        0.52631579],\n",
       "       [0.57894737, 0.78947368, 0.57894737, ..., 0.52631579, 0.52631579,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,  17., 221., ..., 145.,  27.,  33.],\n",
       "       [ 17.,   0., 219., ..., 143.,  37.,  43.],\n",
       "       [221., 219.,   0., ...,  76., 230., 230.],\n",
       "       ...,\n",
       "       [145., 143.,  76., ...,   0., 154., 154.],\n",
       "       [ 27.,  37., 230., ..., 154.,   0.,  19.],\n",
       "       [ 33.,  43., 230., ..., 154.,  19.,   0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nThat is right! The Hamming metric correctly sets the first two examples, that both are of class 2.0, to be nearest neigbors.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Find the neighbor\n",
    "\n",
    "It is clear that the local outlier factor algorithm depends a lot on the idea of a nearest neighbor, which in turn depends on the choice of distance metric. So you decide to experiment some more with the hepatitis dataset introduced in the previous lesson. You are given three examples stored in features, whose classes are stored in labels. You will identify the nearest neighbor to the first example (row with index 0) using three different distance metrics, Euclidean, Hamming and Chebyshev, and on the basis of that choose which distance metric to use. You will import the necessary module as part of the exercise, but pandas and numpy already available, as are features and their labels labels.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the DistanceMetric module as dm.\n",
    "---\n",
    "\n",
    "    Compute the pairwise Euclidean, Hamming and Chebyshev distances for all points.\n",
    "---\n",
    "Question\n",
    "\n",
    "Inspect the matrices dist_eucl, dist_hamm and dist_cheb in your terminal and identify the nearest neighbor to the first example for each metric. Noting that the labels of the three examples are, respectively, 2.0, 2.0 and 1.0, pick one of the following answers:\n",
    "(The Hamming distance works best for this dataset)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import DistanceMetric as dm\n",
    "from sklearn.metrics import DistanceMetric as dm\n",
    "\n",
    "# Find the Euclidean distance between all pairs\n",
    "dist_eucl = dm.get_metric('euclidean').pairwise(features)\n",
    "\n",
    "# Find the Hamming distance between all pairs\n",
    "dist_hamm = dm.get_metric('hamming').pairwise(features)\n",
    "\n",
    "# Find the Chebyshev distance between all pairs\n",
    "dist_cheb = dm.get_metric('chebyshev').pairwise(features)\n",
    "\n",
    "\n",
    "display(dist_eucl, dist_hamm, dist_cheb)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That is right! The Hamming metric correctly sets the first two examples, that both are of class 2.0, to be nearest neigbors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2025: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThere is no datapoint that all three metrics flag as an outlier. So choosing a distance metric should be done with great caution! You now have a concrete understanding of the effect of distance metrics on outlier detection.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Not all metrics agree\n",
    "\n",
    "In the previous exercise you saw that not all metrics agree when it comes to identifying nearest neighbors. But does this mean they might disagree on outliers, too? You decide to put this to the test. You use the same data as before, but this time feed it into a local outlier factor outlier detector. The module LocalOutlierFactor has been made available to you as lof, and the data is available as features.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Detect outliers in features using the euclidean metric.\n",
    "    Detect outliers in features using the hamming metric.\n",
    "    Detect outliers in features using the jaccard metric.\n",
    "    Find if all three metrics agree on any one outlier.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute outliers according to the euclidean metric\n",
    "out_eucl = lof(metric='euclidean').fit_predict(features)\n",
    "\n",
    "# Compute outliers according to the hamming metric\n",
    "out_hamm = lof(metric='hamming').fit_predict(features)\n",
    "\n",
    "# Compute outliers according to the jaccard metric\n",
    "out_jacc  = lof(metric='jaccard').fit_predict(features)\n",
    "\n",
    "# Find if the metrics agree on any one datapoint\n",
    "print(any(out_eucl + out_hamm + out_jacc == -3))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "There is no datapoint that all three metrics flag as an outlier. So choosing a distance metric should be done with great caution! You now have a concrete understanding of the effect of distance metrics on outlier detection.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>LTKCQEEVSHIPAVHPGSFRPKCDENGNYLPLQCYGSIGYCWCVFP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>GPSVFLFPPKPKDTLMISRTPEVTCVVVDVSHEDPEVKFNWYVDGV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>FNMQCQRRFYEALHDPNLNEEQRNAKIKSIRDDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFTDYTMDWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCKASQDVSIGVAWYQQKPGKAPKL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                                seq\n",
       "0  IMMUNE SYSTEM  LTKCQEEVSHIPAVHPGSFRPKCDENGNYLPLQCYGSIGYCWCVFP...\n",
       "1  IMMUNE SYSTEM  GPSVFLFPPKPKDTLMISRTPEVTCVVVDVSHEDPEVKFNWYVDGV...\n",
       "2  IMMUNE SYSTEM                 FNMQCQRRFYEALHDPNLNEEQRNAKIKSIRDDC\n",
       "3  IMMUNE SYSTEM  EVQLVESGGGLVQPGGSLRLSCAASGFTFTDYTMDWVRQAPGKGLE...\n",
       "4  IMMUNE SYSTEM  DIQMTQSPSSLSASVGDRVTITCKASQDVSIGVAWYQQKPGKAPKL..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins = pd.read_csv(path_data+'proteins_exercises.csv')\n",
    "\n",
    "proteins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting StringDist\n",
      "  Downloading StringDist-1.0.9.tar.gz (7.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: StringDist\n",
      "  Building wheel for StringDist (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for StringDist: filename=StringDist-1.0.9-cp311-cp311-linux_x86_64.whl size=17631 sha256=4b5c398bf79b1378dc9548b88e6c32b9bdde33bf4e872822700aa7442b565304\n",
      "  Stored in directory: /home/nero/.cache/pip/wheels/fd/d0/9b/188e72fe6cc1ad1ae96a71835adb85cd58b0622a3440b4acdf\n",
      "Successfully built StringDist\n",
      "Installing collected packages: StringDist\n",
      "Successfully installed StringDist-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install StringDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_532341/2410236371.py:28: DeprecationWarning: getargs: The 'u' format is deprecated. Use 'U' instead.\n",
      "  return stringdist.rdlevenshtein(U[0], v[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOvercoming compatibility problems when working with different Python modules is the sign of a true expert. What's more, you should now feel confident that success is possible even without labels.\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Restricted Levenshtein\n",
    "\n",
    "You notice that the stringdist package also implements a variation of Levenshtein distance called the Restricted Damerau-Levenshtein distance, and want to try it out. You will follow the logic from the lesson, wrapping it inside a custom function and precomputing the distance matrix before fitting a local outlier factor anomaly detector. You will measure performance with accuracy_score() which is available to you as accuracy(). You also have access to packages stringdist, numpy as np, pdist() and squareform() from scipy.spatial.distance, and LocalOutlierFactor as lof. The data has been preloaded as a pandas dataframe with two columns, label and sequence, and has two classes: IMMUNE SYSTEM and VIRUS.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write a function with input u and v, each of which is an array containing a string, and applies the rdlevenshtein() function on the two strings.\n",
    "    Reshape the sequence column from proteins by first casting it into an numpy array, and then using .reshape().\n",
    "    Compute a square distance matrix for sequences using my_rdlevenshtein(), and fit lof on it.\n",
    "    Compute accuracy by converting preds and proteins['label'] into booleans indicating whether a protein is a virus.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import stringdist\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "\n",
    "# Wrap the RD-Levenshtein metric in a custom function\n",
    "def my_rdlevenshtein(U, v):\n",
    "    return stringdist.rdlevenshtein(U[0], v[0])\n",
    "\n",
    "# Reshape the array into a numpy matrix\n",
    "sequences = np.array(proteins['seq']).reshape(-1, 1)\n",
    "\n",
    "# Compute the pairwise distance matrix in square form\n",
    "M = squareform(pdist(sequences, metric=my_rdlevenshtein))\n",
    "\n",
    "# Run a LoF algorithm on the precomputed distance matrix\n",
    "preds = lof(metric='precomputed').fit_predict(M)\n",
    "\n",
    "# Compute the accuracy of the outlier predictions\n",
    "print(accuracy(proteins['label'] == 'VIRUS', preds==-1))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Overcoming compatibility problems when working with different Python modules is the sign of a true expert. What's more, you should now feel confident that success is possible even without labels.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739375\n",
      "0.6959375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You have reached the end of this course. You are now ready to take off the kid gloves and face real-world machine learning problems. Good luck!\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Bringing it all together\n",
    "\n",
    "In addition to the distance-based learning anomaly detection pipeline you created in the last exercise, you want to also support a feature-based learning one with one-class SVM. You decide to extract two features: first, the length of the string, and, second, a numerical encoding of the first letter of the string, obtained using the function LabelEncoder() described in Chapter 1. To ensure a fair comparison, you will input the outlier scores into an AUC calculation. The following have been imported: LabelEncoder(), roc_auc_score() as auc() and OneClassSVM. The data is available as a pandas data frame called proteins with two columns, label and seq, and two classes, IMMUNE SYSTEM and VIRUS. A fitted LoF detector is available as lof_detector.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    For a string s, len(s) returns its length. Apply that to the seq column to obtain a new column len.\n",
    "    For a string s, list(s) returns a list of its characters. Use this to extract the first letter of each sequence, and encode it using LabelEncoder().\n",
    "    LoF scores are in the negative_outlier_factor_ attribute. Compute their AUC.\n",
    "    Fit a 1-class SVM to a data frame with only len and first as columns. Extract the scores and assess both the LoF scores and the SVM scores using AUC.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "lof_detector = lof(metric='precomputed')\n",
    "lof_detector.fit(M)\n",
    "\n",
    "\n",
    "# Create a feature that contains the length of the string\n",
    "proteins['len'] = proteins['seq'].apply(len)\n",
    "\n",
    "# Create a feature encoding the first letter of the string\n",
    "proteins['first'] =  LabelEncoder().fit_transform(proteins['seq'].apply(list).apply(lambda x: x[0]))\n",
    "\n",
    "# Extract scores from the fitted LoF object, compute its AUC\n",
    "scores_lof = lof_detector.negative_outlier_factor_\n",
    "print(auc(proteins['label']=='IMMUNE SYSTEM', scores_lof))\n",
    "\n",
    "# Fit a 1-class SVM, extract its scores, and compute its AUC\n",
    "svm = OneClassSVM().fit(proteins[['len', 'first']])\n",
    "scores_svm = svm.score_samples(proteins[['len', 'first']])\n",
    "print(auc(proteins['label']=='IMMUNE SYSTEM', scores_svm))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You have reached the end of this course. You are now ready to take off the kid gloves and face real-world machine learning problems. Good luck!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
