{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Designing_Machine_Learning_Workflows_in_Python/datasets/'\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrh = pd.read_csv(path_data+'arrh.csv')\n",
    "\n",
    "for column in arrh.columns:\n",
    "    # Replace negative values with 0\n",
    "    arrh[column] = arrh[column].apply(lambda x: max(x, 0))\n",
    "\n",
    "X = arrh.drop('class', axis=1)\n",
    "y = arrh['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131    1.0\n",
       "389    0.5\n",
       "286    0.4\n",
       "16     0.6\n",
       "418    0.7\n",
       "      ... \n",
       "106    0.0\n",
       "270    0.5\n",
       "348    1.5\n",
       "435    0.8\n",
       "102    1.1\n",
       "Name: chV6_PwaveAmp, Length: 339, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['chV6_PwaveAmp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWrapping up your workflow inside a pipeline is a sign of a true professional!\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Your first pipeline - again!\n",
    "\n",
    "Back in the arrhythmia startup, your monthly review is coming up, and as part of that an expert Python programmer will be reviewing your code. You decide to tidy up by following best practices and replace your script for feature selection and random forest classification, with a pipeline. You are using a training dataset available as X_train and y_train, and a number of modules: RandomForestClassifier, SelectKBest() and f_classif() for feature selection, as well as GridSearchCV and Pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a pipeline with the feature selector given by the sample code, and a random forest classifier. Name the first step feature_selection.\n",
    "    Add two key-value pairs in params, one for the number of features k in the selector with values 10 and 20, and one for n_estimators in the forest with possible values 2 and 5.\n",
    "    Initialize a GridSearchCV object with the given pipeline and parameter grid.\n",
    "    Fit the object to the data and print the best performing parameter combination.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "    'feature_selection__k': [10, 20],\n",
    "    'clf__n_estimators': [2, 5]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Ignore warnings during the execution of this block\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        # Initialize the grid search object\n",
    "        grid_search = GridSearchCV(pipe, param_grid=params, cv=3)\n",
    "\n",
    "        # Fit it to the data and print the best value combination\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Best parameters:\", grid_search.best_params_)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Wrapping up your workflow inside a pipeline is a sign of a true professional!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(y_test, y_est, cost_fp=10.0, cost_fn=1.0):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()\n",
    "    return cost_fp * fp + cost_fn * fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 20}\n",
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n",
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations, you can now incorporate the knowledge you acquired in Chapter 2 in your pipelines.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Custom scorers in pipelines\n",
    "\n",
    "You are proud of the improvement in your code quality, but just remembered that previously you had to use a custom scoring metric in order to account for the fact that false positives are costlier to your startup than false negatives. You hence want to equip your pipeline with scorers other than accuracy, including roc_auc_score(), f1_score(), and you own custom scoring function. The pipeline from the previous lesson is available as pipe, as is the parameter grid as params and the training data as X_train, y_train. You also have confusion_matrix() for the purpose of writing your own metric.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Convert the metric roc_auc_score() into a scorer, and feed it into GridSearchCV(). Then fit that to the data.\n",
    "---\n",
    "Now repeat for the F1 score, instead, given by f1_score().\n",
    "---\n",
    "Now repeat with a custom metric which is available to you as as simple Python function called my_metric().\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a custom scorer\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Initialize the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Create a custom scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Create a custom scorer\n",
    "scorer = make_scorer(my_metric)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)\n",
    "\n",
    "# Reset warnings filter\n",
    "warnings.resetwarnings()\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations, you can now incorporate the knowledge you acquired in Chapter 2 in your pipelines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations. You are now a full-stack data scientist!\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Pickles\n",
    "\n",
    "Finally, it is time for you to push your first model to production. It is a random forest classifier which you will use as a baseline, while you are still working to develop a better alternative. You have access to the data split in training test with their usual names, X_train, X_test, y_train and y_test, as well as to the modules RandomForestClassifier() and pickle, whose methods .load() and .dump() you will need for this exercise.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fit a random forest classifier to the data. Fix the random seed to 42 ensure that your results are reproducible.\n",
    "    Write the model to file using pickle. Open the destination file using the with open(____) as ____ syntax.\n",
    "    Now load the model from file into a different variable name, clf_from_file.\n",
    "    Store the predictions from the model you loaded into a variable preds.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import pickle\n",
    "\n",
    "# Fit a random forest to the training set\n",
    "clf = RandomForestClassifier(random_state=42).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "# Save it to a file, to be pushed to production\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)\n",
    "\n",
    "# Now load the model from file in the production environment\n",
    "with open('model.pkl','rb') as file:\n",
    "    clf_from_file = pickle.load(file)\n",
    "\n",
    "# Predict the labels of the test dataset\n",
    "preds = clf_from_file.predict(X_test)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations. You are now a full-stack data scientist!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are now using pipelines like a professional.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Custom function transformers in pipelines\n",
    "\n",
    "At some point, you were told that the sensors might be performing poorly for obese individuals. Previously you had dealt with that using weights, but now you are thinking that this information might also be useful for feature engineering, so you decide to replace the recorded weight of an individual with an indicator of whether they are obese. You want to do this using pipelines. You have numpy available as np, RandomForestClassifier(), FunctionTransformer(), and GridSearchCV().\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a custom feature extractor. This is a function that will output a modified copy of its input.\n",
    "    Replace each value of the first column with the indicator of whether that value is above a threshold given by a multiple of the column mean.\n",
    "    Convert the feature extractor above to a transformer and place it in a pipeline together with a random forest classifier.\n",
    "    Use grid search CV to try values 1, 2 and 3 for the multiplication constant multiplier in your feature extractor.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define a feature extractor to flag very large values\n",
    "def more_than_average(X, multiplier=1.0):\n",
    "  Z = X.copy()\n",
    "  Z[:,1] = Z[:,0] > multiplier*np.mean(Z[:,1])\n",
    "  return Z\n",
    "\n",
    "# Convert your function so that it can be used in a pipeline\n",
    "pipe = Pipeline([\n",
    "  ('ft', FunctionTransformer(more_than_average)),\n",
    "  ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Optimize the parameter multiplier using GridSearchCV\n",
    "params = {'ft__multiplier':[1,2,3]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You are now using pipelines like a professional.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7796610169491525\n",
      "0.7101449275362318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_503054/2286639822.py:24: ResourceWarning: unclosed file <_io.BufferedReader name='model.pkl'>\n",
      "  champion = pickle.load(open('model.pkl', 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFantastic progress! This way of working is very similar to agile software development, and can greatly accelerate your workflows.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Challenge the champion\n",
    "\n",
    "Having pushed your random forest to production, you suddenly worry that a naive Bayes classifier might be better. You want to run a champion-challenger test, by comparing a naive Bayes, acting as the challenger, to exactly the model which is currently in production, which you will load from file to make sure there is no confusion. You will use the F1 score for assessment. You have the data X_train, X_test, y_train and y_test available as before and GaussianNB(), f1_score() and pickle().\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Load the existing model from memory using pickle.\n",
    "    Fit a Gaussian Naive Bayes classifier to the training data.\n",
    "    Print the F1 score of the champion and then the challenger on the test data.\n",
    "    Overwrite the current model to disk with the one that performed best.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load the current model from disk\n",
    "champion = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "# Fit a Gaussian Naive Bayes to the training data\n",
    "challenger = GaussianNB().fit(X_train,y_train)\n",
    "\n",
    "# Print the F1 test scores of both champion and challenger\n",
    "print(f1_score(y_test, champion.predict(X_test)))\n",
    "print(f1_score(y_test, challenger.predict(X_test)))\n",
    "\n",
    "# Write back to disk the best-performing model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(champion, file=file)\n",
    "    file.close()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fantastic progress! This way of working is very similar to agile software development, and can greatly accelerate your workflows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('feature_selection', SelectKBest()), ('clf',RandomForestClassifier(random_state=2))])\n",
    "params = {'feature_selection__k': [10, 20], 'clf__n_estimators': [2, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.261062\n",
      "1   -0.256637\n",
      "2   -0.278761\n",
      "3   -0.274336\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work. The difference between training and test performance seems quite big here, and that is always a telltale sign of overfitting!\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Cross-validation statistics\n",
    "\n",
    "You used grid search CV to tune your random forest classifier, and now want to inspect the cross-validation results to ensure you did not overfit. In particular you would like to take the difference of the mean test score for each fold from the mean training score. The dataset is available as X_train and y_train, the pipeline as pipe, and a number of modules are pre-loaded including pandas as pd and GridSearchCV().\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a grid search object with three cross-validation folds and ensure it returns training as well as test statistics.\n",
    "    Fit the grid search object to the training data.\n",
    "    Store the results of the cross-validation, available in the cv_results_ attribute of the fitted CV object, into a dataframe.\n",
    "    Print the difference between the column containing the average test score and that containing the average training score.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import warnings\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Fit your pipeline using GridSearchCV with three folds\n",
    "grid_search = GridSearchCV(\n",
    "  pipe, params, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit the grid search\n",
    "gs = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Store the results of CV into a pandas dataframe\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "# Print the difference between mean test and training scores\n",
    "print(\n",
    "  results['mean_test_score']-results['mean_train_score'])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work. The difference between training and test performance seems quite big here, and that is always a telltale sign of overfitting!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! You now realise that the possibility of dataset shift introduces yet another parameter to optimize: the window size. This cannot be done with Cross-Validation on historical data, but instead requires the technique shown here.\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Tuning the window size\n",
    "\n",
    "You want to check for yourself that the optimal window size for the arrhythmia dataset is 50. You have been given the dataset as a pandas data frame called arrh, and want to use a subset of the data up to time t_now. Your test data is available as X_test, y_test. You will try out a number of window sizes, ranging from 10 to 100, fit a naive Bayes classifier to each window, assess its F1 score on the test data, and then pick the best performing window size. You also have numpy available as np, and the function f1_score() has been imported already. Finally, an empty list called accuracies has been initialized for you to store the accuracies of the windows.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the index of a sliding window of size w_size stopping at t_now using the .loc() method.\n",
    "    Construct X from the sliding window by removing the class column. Store that latter column as y.\n",
    "    Fit a naive Bayes classifier to X and y, and use it to predict the labels of the test data X_test.\n",
    "    Compute the F1 score of these predictions for each window size, and find the best-performing window size.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "wrange = range(10,100,10)\n",
    "t_now = 400\n",
    "accuracies = []\n",
    "# Loop over window sizes\n",
    "for w_size in wrange:\n",
    "\n",
    "    # Define sliding window\n",
    "    sliding = arrh.loc[(t_now - w_size + 1):t_now]\n",
    "\n",
    "    # Extract X and y from the sliding window\n",
    "    X, y = sliding.drop('class', axis=1), sliding['class']\n",
    "    \n",
    "    # Fit the classifier and store the F1 score\n",
    "    preds = GaussianNB().fit(X, y).predict(X_test)\n",
    "    accuracies.append(f1_score(y_test, preds))\n",
    "\n",
    "# Estimate the best performing window size\n",
    "optimal_window = wrange[np.argmax(accuracies)]\n",
    "print(optimal_window)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You now realise that the possibility of dataset shift introduces yet another parameter to optimize: the window size. This cannot be done with Cross-Validation on historical data, but instead requires the technique shown here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are now an sklearn ninja and nothing can stop you. Except for ... a lack of labelled data! Let's see what we can do about that in the next chapter.\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Bringing it all together\n",
    "\n",
    "You have two concerns about your pipeline at the arrhythmia detection startup:\n",
    "\n",
    "    The app was trained on patients of all ages, but is primarily being used by fitness users who tend to be young. You suspect this might be a case of domain shift, and hence want to disregard all examples above 50 years old.\n",
    "    You are still concerned about overfitting, so you want to see if making the random forest classifier less complex and selecting some features might help with that.\n",
    "\n",
    "You will create a pipeline with a feature selection SelectKBest() step and a RandomForestClassifier, both of which have been imported. You also have access to GridSearchCV(), Pipeline, numpy as np and pickle. The data is available as arrh\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a pipeline with SelectKBest() as step ft and RandomForestClassifier() as step clf.\n",
    "    Create a parameter grid to tune k in SelectKBest() and max_depth in RandomForestClassifier().\n",
    "    Use GridSearchCV() to optimize your pipeline against that grid and data containing only those aged under 50.\n",
    "    Save the optimized pipeline to a pickle for production.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "  ('ft', SelectKBest()), ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = {'ft__k': [5, 10], 'clf__max_depth': [10, 20]}\n",
    "\n",
    "# Execute grid search CV on a dataset containing under 50s\n",
    "grid_search = GridSearchCV(pipe, param_grid=grid)\n",
    "arrh = arrh.loc[arrh['age'] < 50]  # Filter rows where age < 50\n",
    "grid_search.fit(arrh.drop('class', axis=1), arrh['class'])\n",
    "\n",
    "# Push the fitted pipeline to production\n",
    "with open('pipe.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You are now an sklearn ninja and nothing can stop you. Except for ... a lack of labelled data! Let's see what we can do about that in the next chapter.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
