{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Designing_Machine_Learning_Workflows_in_Python/datasets/'\n",
    "credit = pd.read_csv(path_data + 'credit.csv')\n",
    "non_numeric_columns = ['checking_status',\n",
    " 'credit_history',\n",
    " 'purpose',\n",
    " 'savings_status',\n",
    " 'employment',\n",
    " 'personal_status',\n",
    " 'other_parties',\n",
    " 'property_magnitude',\n",
    " 'other_payment_plans',\n",
    " 'housing',\n",
    " 'job',\n",
    " 'own_telephone',\n",
    " 'foreign_worker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>property_magnitude</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'&lt;0'</td>\n",
       "      <td>6</td>\n",
       "      <td>'critical/other existing credit'</td>\n",
       "      <td>buy_radio_tv</td>\n",
       "      <td>1169</td>\n",
       "      <td>'no known savings'</td>\n",
       "      <td>'&gt;=7'</td>\n",
       "      <td>4</td>\n",
       "      <td>'male single'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>67</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>2</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'0&lt;=X&lt;200'</td>\n",
       "      <td>48</td>\n",
       "      <td>'existing paid'</td>\n",
       "      <td>buy_radio_tv</td>\n",
       "      <td>5951</td>\n",
       "      <td>'&lt;100'</td>\n",
       "      <td>'1&lt;=X&lt;4'</td>\n",
       "      <td>2</td>\n",
       "      <td>'female div/dep/mar'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>22</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'no checking'</td>\n",
       "      <td>12</td>\n",
       "      <td>'critical/other existing credit'</td>\n",
       "      <td>education</td>\n",
       "      <td>2096</td>\n",
       "      <td>'&lt;100'</td>\n",
       "      <td>'4&lt;=X&lt;7'</td>\n",
       "      <td>2</td>\n",
       "      <td>'male single'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>49</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>'unskilled resident'</td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  checking_status  duration                    credit_history       purpose  \\\n",
       "0            '<0'         6  'critical/other existing credit'  buy_radio_tv   \n",
       "1      '0<=X<200'        48                   'existing paid'  buy_radio_tv   \n",
       "2   'no checking'        12  'critical/other existing credit'     education   \n",
       "\n",
       "   credit_amount      savings_status employment  installment_commitment  \\\n",
       "0           1169  'no known savings'      '>=7'                       4   \n",
       "1           5951              '<100'   '1<=X<4'                       2   \n",
       "2           2096              '<100'   '4<=X<7'                       2   \n",
       "\n",
       "        personal_status other_parties  ...  property_magnitude age  \\\n",
       "0         'male single'          none  ...       'real estate'  67   \n",
       "1  'female div/dep/mar'          none  ...       'real estate'  22   \n",
       "2         'male single'          none  ...       'real estate'  49   \n",
       "\n",
       "   other_payment_plans housing existing_credits                   job  \\\n",
       "0                 none     own                2               skilled   \n",
       "1                 none     own                1               skilled   \n",
       "2                 none     own                1  'unskilled resident'   \n",
       "\n",
       "  num_dependents  own_telephone foreign_worker class  \n",
       "0              1            yes            yes  good  \n",
       "1              1           none            yes   bad  \n",
       "2              2           none            yes  good  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking_status            int64\n",
      "duration                   int64\n",
      "credit_history             int64\n",
      "purpose                    int64\n",
      "credit_amount              int64\n",
      "savings_status             int64\n",
      "employment                 int64\n",
      "installment_commitment     int64\n",
      "personal_status            int64\n",
      "other_parties              int64\n",
      "residence_since            int64\n",
      "property_magnitude         int64\n",
      "age                        int64\n",
      "other_payment_plans        int64\n",
      "housing                    int64\n",
      "existing_credits           int64\n",
      "job                        int64\n",
      "num_dependents             int64\n",
      "own_telephone              int64\n",
      "foreign_worker             int64\n",
      "class                     object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood job! With a fully numeric matrix you can now pretty much use any classifier you like! Let's try a couple out.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Feature engineering\n",
    "\n",
    "You are tasked with predicting whether or not a new cohort of loan applicants are likely to default on their loans. You have a historical dataset and wish to train a classifier on it. You notice that many features are in string format, which is a problem for your classifiers. You hence decide to encode the string columns numerically using LabelEncoder(). The function has been preloaded for you from the preprocessing submodule of sklearn. The dataset credit is also preloaded, as is a list of all column names whose data types are string, stored in non_numeric_columns.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Inspect the first three lines of your data using .head().\n",
    "---\n",
    "\n",
    "    For each column in non_numeric_columns, replace the string values with numeric values using LabelEncoder().\n",
    "---\n",
    "\n",
    "    Confirm your code worked by printing the data types in the credit data frame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Inspect the first few lines of your data using head()\n",
    "display(credit.head(3))\n",
    "\n",
    "# Create a label encoder for each column. Encode the values\n",
    "for column in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    credit[column] = le.fit_transform(credit[column])\n",
    "\n",
    "# Inspect the data types of the columns of the data frame\n",
    "print(credit.dtypes)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! With a fully numeric matrix you can now pretty much use any classifier you like! Let's try a couple out.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit['class']\n",
    "X = credit.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf': 0.775}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! You have just built your first pipeline. Did you wonder whether there are any additional parameters that you could tune to make AdaBoost even better? The answer is yes! Let's explore that in our next lesson on tuning parameters.\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Your first pipeline\n",
    "\n",
    "Your colleague has used AdaBoostClassifier for the credit scoring dataset. You want to also try out a random forest classifier. In this exercise, you will fit this classifier to the data and compare it to AdaBoostClassifier. Make sure to use train/test data splitting to avoid overfitting. The data is preloaded and transformed so that all features are numeric. The features are available as X and the labels as y. The module RandomForestClassifier has also been preloaded.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split the data into train (X_train and y_train) and test (X_test and y_test). Use 20% of the examples for the test set.\n",
    "---\n",
    "\n",
    "    Fit a RandomForest classifier to the training set and predict the labels of the test data.\n",
    "---\n",
    "\n",
    "    Use accuracy_score to assess the performance of your classifier. An empty dictionary called accuracies is pre-loaded in your environment.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create a random forest classifier, fixing the seed to 2\n",
    "rf_model = RandomForestClassifier(random_state=2).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "# Use it to predict the labels of the test data\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Assess the accuracy of both classifiers\n",
    "accuracies = {}\n",
    "accuracies['rf'] = accuracy_score(y_test, rf_predictions)\n",
    "display(accuracies)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You have just built your first pipeline. Did you wonder whether there are any additional parameters that you could tune to make AdaBoost even better? The answer is yes! Let's explore that in our next lesson on tuning parameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 40}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 50}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nExcellent. You now know how to deal with the extremely important issue of model complexity.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Grid search CV for model complexity\n",
    "\n",
    "In the last slide, you saw how most classifiers have one or more hyperparameters that control its complexity. You also learned to tune them using GridSearchCV(). In this exercise, you will perfect this skill. You will experiment with:\n",
    "\n",
    "    The number of trees, n_estimators, in a RandomForestClassifier.\n",
    "    The maximum depth, max_depth, of the decision trees used in an AdaBoostClassifier.\n",
    "    The number of nearest neighbors, n_neighbors, in KNeighborsClassifier.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Define the parameter grid as described in the code comment and create a grid object with a RandomForestClassifier().\n",
    "---\n",
    "Adapt your code to optimise n_estimators for an AdaBoostClassifier().\n",
    "---\n",
    "Adapt your code to optimise n_neighbors for an KNeighborsClassifier().\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Set a range for n_estimators from 10 to 40 in steps of 10\n",
    "param_grid = {'n_estimators': range(10, 50, 10)}\n",
    "\n",
    "# Optimize for a RandomForestClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "display(grid.best_params_)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Define a grid for n_estimators ranging from 1 to 10\n",
    "param_grid = {'n_estimators': range(1, 11)}\n",
    "\n",
    "# Optimize for a AdaBoostClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "display(grid.best_params_)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Define a grid for n_neighbors with values 10, 50 and 100\n",
    "param_grid = {'n_neighbors': [10,50,100]}\n",
    "\n",
    "# Optimize for KNeighborsClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "display(grid.best_params_)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent. You now know how to deal with the extremely important issue of model complexity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations, you now have the choice between label and one-hot encoding at your fingertips!\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Categorical encodings\n",
    "\n",
    "Your colleague has converted the columns in the credit dataset to numeric values using LabelEncoder(). He left one out: credit_history, which records the credit history of the applicant. You want to create two versions of the dataset. One will use LabelEncoder() and another one-hot encoding, for comparison purposes. The feature matrix is available to you as credit. You have LabelEncoder() preloaded and pandas as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Encode credit_history using LabelEncoder().\n",
    "    Concatenate the result to the original frame.\n",
    "    Create a new data frame by concatenating the 1-hot encoding dummies to the original frame.\n",
    "    Confirm that 1-hot encoding produces more columns than label encoding.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create numeric encoding for credit_history\n",
    "credit_history_num = LabelEncoder().fit_transform(\n",
    "  credit['credit_history'])\n",
    "\n",
    "# Create a new feature matrix including the numeric encoding\n",
    "X_num = pd.concat([X, pd.Series(credit_history_num)], axis=1)\n",
    "\n",
    "# Create new feature matrix with dummies for credit_history\n",
    "X_hot = pd.concat(\n",
    "  [X, pd.get_dummies(credit['credit_history'])], axis=1)\n",
    "\n",
    "# Compare the number of features of the resulting DataFrames\n",
    "print(X_hot.shape[1] > X_num.shape[1])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations, you now have the choice between label and one-hot encoding at your fingertips!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! You now have one more tool at your disposal to decide which features are worth introducing to your dataset.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Feature transformations\n",
    "\n",
    "You are discussing the credit dataset with the bank manager. She suggests that the safest loan applications tend to request mid-range credit amounts. Values that are either too low or too high suggest high risk. This means that a non-linear relationship might exist between this variable and the class. You want to test this hypothesis. You will construct a non-linear transformation of the feature. Then, you will assess which of the two features is better at predicting the class using SelectKBest() and the chi2() metric, both of which have been preloaded.\n",
    "\n",
    "The data is available as a pandas DataFrame called credit, with the class contained in the column class. You also have preloaded pandas as pd and numpy as np.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a function that transforms a numeric vector by considering the absolute difference of each value from the average value of the vector.\n",
    "    Apply this transformation to the credit_amount column of the dataset and store in new column called diff\n",
    "    Create a SelectKBest() feature selector to pick one of the two columns, credit_amount and diff using the chi2() metric.\n",
    "    Inspect the results.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "# Function computing absolute difference from column mean\n",
    "def abs_diff(x):\n",
    "    return np.abs(x-np.mean(x))\n",
    "\n",
    "# Apply it to the credit amount and store to new column\n",
    "credit['diff'] = abs_diff(credit['credit_amount'])\n",
    "\n",
    "# Create a feature selector with chi2 that picks one feature\n",
    "sk = SelectKBest(chi2, k=1)\n",
    "\n",
    "# Use the selector to pick between credit_amount and diff\n",
    "sk.fit(credit[['credit_amount','diff']], credit['class'])\n",
    "\n",
    "# Inspect the results\n",
    "display(sk.get_support())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You now have one more tool at your disposal to decide which features are worth introducing to your dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>QRSduration</th>\n",
       "      <th>PRinterval</th>\n",
       "      <th>Q-Tinterval</th>\n",
       "      <th>Tinterval</th>\n",
       "      <th>Pinterval</th>\n",
       "      <th>QRS</th>\n",
       "      <th>...</th>\n",
       "      <th>chV6_QwaveAmp</th>\n",
       "      <th>chV6_RwaveAmp</th>\n",
       "      <th>chV6_SwaveAmp</th>\n",
       "      <th>chV6_RPwaveAmp</th>\n",
       "      <th>chV6_SPwaveAmp</th>\n",
       "      <th>chV6_PwaveAmp</th>\n",
       "      <th>chV6_TwaveAmp</th>\n",
       "      <th>chV6_QRSA</th>\n",
       "      <th>chV6_QRSTA</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  height  weight  QRSduration  PRinterval  Q-Tinterval  Tinterval  \\\n",
       "0   75    0     190      80           91         193          371        174   \n",
       "1   56    1     165      64           81         174          401        149   \n",
       "2   54    0     172      95          138         163          386        185   \n",
       "3   55    0     175      94          100         202          380        179   \n",
       "4   75    0     190      80           88         181          360        177   \n",
       "\n",
       "   Pinterval  QRS  ...  chV6_QwaveAmp  chV6_RwaveAmp  chV6_SwaveAmp  \\\n",
       "0        121  -16  ...            0.0            9.0           -0.9   \n",
       "1         39   25  ...            0.0            8.5            0.0   \n",
       "2        102   96  ...            0.0            9.5           -2.4   \n",
       "3        143   28  ...            0.0           12.2           -2.2   \n",
       "4        103  -16  ...            0.0           13.1           -3.6   \n",
       "\n",
       "   chV6_RPwaveAmp  chV6_SPwaveAmp  chV6_PwaveAmp  chV6_TwaveAmp  chV6_QRSA  \\\n",
       "0             0.0             0.0            0.9            2.9       23.3   \n",
       "1             0.0             0.0            0.2            2.1       20.4   \n",
       "2             0.0             0.0            0.3            3.4       12.3   \n",
       "3             0.0             0.0            0.4            2.6       34.6   \n",
       "4             0.0             0.0           -0.1            3.9       25.4   \n",
       "\n",
       "   chV6_QRSTA  class  \n",
       "0        49.4      0  \n",
       "1        38.8      0  \n",
       "2        49.0      0  \n",
       "3        61.6      1  \n",
       "4        62.8      0  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrh = pd.read_csv(path_data + 'arrh.csv')\n",
    "arrh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in arrh.columns:\n",
    "    # Replace negative values with 0\n",
    "    arrh[column] = arrh[column].apply(lambda x: max(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = arrh.iloc[:, :-1]\n",
    "y = arrh['class'].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are already able to handle hundreds of features in a few lines of code! But what if the optimal number of estimators is different if you first apply feature selection? In Chapter 3 you will learn how to put your pipelines on steroids so that such questions can be asked in just one line of code.\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Bringing it all together\n",
    "\n",
    "You just joined an arrhythmia detection startup and want to train a model on the arrhythmias dataset arrh. You noticed that random forests tend to win quite a few Kaggle competitions, so you want to try that out with a maximum depth of 2, 5, or 10, using grid search. You also observe that the dimension of the dataset is quite high so you wish to consider the effect of a feature selection method.\n",
    "\n",
    "To make sure you don't overfit by mistake, you have already split your data. You will use X_train and y_train for the grid search, and X_test and y_test to decide if feature selection helps. All four dataset folds are preloaded in your environment. You also have access to GridSearchCV(), train_test_split(), SelectKBest(), chi2() and RandomForestClassifier as rfc.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use grid search to experiment with a maximum depth of 2, 5, and 10 for RandomForestClassifier and store the best performing parameter setting.\n",
    "    Now refit the estimator using the best-performing number of estimators as deduced above.\n",
    "    Apply the SelectKBest feature selector with the chi2 scoring function and refit the classifier.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "rfc = RandomForestClassifier\n",
    "\n",
    "# Find the best value for max_depth among values 2, 5 and 10\n",
    "grid_search = GridSearchCV(\n",
    "  rfc(random_state=1), param_grid={'max_depth':[2,5,10]})\n",
    "best_value = grid_search.fit(\n",
    "  X_train, y_train).best_params_['max_depth']\n",
    "\n",
    "# Using the best value from above, fit a random forest\n",
    "clf = rfc(\n",
    "  random_state=1, max_depth=best_value).fit(X_train, y_train)\n",
    "\n",
    "# Apply SelectKBest with chi2 and pick top 100 features\n",
    "vt = SelectKBest(chi2, k=100).fit(X_train, y_train)\n",
    "\n",
    "# Create a new dataset only containing the selected features\n",
    "X_train_reduced = vt.transform(X_train)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You are already able to handle hundreds of features in a few lines of code! But what if the optimal number of estimators is different if you first apply feature selection? In Chapter 3 you will learn how to put your pipelines on steroids so that such questions can be asked in just one line of code.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
