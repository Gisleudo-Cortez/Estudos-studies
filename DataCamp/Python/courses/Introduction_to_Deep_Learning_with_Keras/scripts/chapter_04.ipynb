{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Introduction_to_Deep_Learning_with_Keras/datasets/'\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('/home/nero/Documents/Estudos/DataCamp/Python/Introduction_to_Deep_Learning_with_Keras/models/best_banknote_model.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variace</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variace  skewness  curtosis  entropy  class\n",
       "0  3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1  4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2  3.86600   -2.6383    1.9242  0.10645      0\n",
       "3  3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4  0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path_data + 'banknotes.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1372, 4), (1372,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data.iloc[: , :-1].values\n",
    "\n",
    "targets = data['class']\n",
    "\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "#targets = to_categorical(data['class'])\n",
    "\n",
    "features.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(features, targets, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((960, 4), (412, 4), (960,), (412,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[8.93139839e-01],\n",
      "       [5.50193479e-04],\n",
      "       [1.58576369e-02],\n",
      "       [3.09565780e-03],\n",
      "       [9.41419542e-01],\n",
      "       [3.33950826e-04],\n",
      "       [9.70590055e-01],\n",
      "       [8.53904116e-04],\n",
      "       [1.49591826e-03],\n",
      "       [1.75976346e-03],\n",
      "       [1.12509599e-03],\n",
      "       [1.50597224e-03],\n",
      "       [8.01421236e-04],\n",
      "       [5.50632715e-01],\n",
      "       [2.60471622e-03],\n",
      "       [1.72065031e-02],\n",
      "       [7.31266057e-03],\n",
      "       [1.85551122e-03],\n",
      "       [9.40912247e-01],\n",
      "       [1.73294544e-03],\n",
      "       [2.24172766e-03],\n",
      "       [9.34918761e-01],\n",
      "       [5.73907018e-01],\n",
      "       [2.83137150e-03],\n",
      "       [7.83297233e-03],\n",
      "       [4.68030810e-01],\n",
      "       [1.84633955e-03],\n",
      "       [2.52774567e-04],\n",
      "       [6.63136831e-03],\n",
      "       [8.93449113e-02],\n",
      "       [5.21918060e-04],\n",
      "       [9.98566151e-01],\n",
      "       [5.69549680e-01],\n",
      "       [2.83759972e-03],\n",
      "       [3.48957750e-04],\n",
      "       [7.96873122e-04],\n",
      "       [1.02513239e-01],\n",
      "       [2.47507152e-04],\n",
      "       [3.70687619e-02],\n",
      "       [1.05405774e-03],\n",
      "       [8.37081671e-01],\n",
      "       [9.70737636e-01],\n",
      "       [9.07537282e-01],\n",
      "       [1.05618872e-03],\n",
      "       [4.33166385e-01],\n",
      "       [9.86776829e-01],\n",
      "       [5.62874377e-01],\n",
      "       [9.22126234e-01],\n",
      "       [3.60690756e-03],\n",
      "       [9.32397604e-01],\n",
      "       [8.86833370e-01],\n",
      "       [9.56060708e-01],\n",
      "       [1.81253289e-03],\n",
      "       [6.46098614e-01],\n",
      "       [9.61326420e-01],\n",
      "       [9.50455070e-01],\n",
      "       [9.24756527e-01],\n",
      "       [2.84915906e-04],\n",
      "       [3.44794337e-03],\n",
      "       [9.64894611e-03],\n",
      "       [9.97792542e-01],\n",
      "       [5.49363434e-01],\n",
      "       [8.67541730e-02],\n",
      "       [9.79896843e-01],\n",
      "       [1.45429489e-03],\n",
      "       [9.86686528e-01],\n",
      "       [6.53167367e-01],\n",
      "       [4.56120912e-03],\n",
      "       [9.71245229e-01],\n",
      "       [2.18046546e-01],\n",
      "       [4.27937955e-01],\n",
      "       [1.67494779e-03],\n",
      "       [2.87170196e-03],\n",
      "       [4.70212381e-03],\n",
      "       [8.30011904e-01],\n",
      "       [9.82931733e-01],\n",
      "       [5.20588696e-01],\n",
      "       [1.03542702e-02],\n",
      "       [1.64638951e-01],\n",
      "       [9.33663785e-01],\n",
      "       [9.88277197e-01],\n",
      "       [9.98232365e-01],\n",
      "       [1.61666404e-02],\n",
      "       [5.25511503e-01],\n",
      "       [2.39025592e-03],\n",
      "       [1.78877324e-01],\n",
      "       [4.27491277e-01],\n",
      "       [4.85765748e-03],\n",
      "       [9.59226310e-01],\n",
      "       [8.32810700e-01],\n",
      "       [9.25544067e-04],\n",
      "       [9.64379847e-01],\n",
      "       [8.82091761e-01],\n",
      "       [2.91576551e-04],\n",
      "       [8.31371546e-01],\n",
      "       [8.34105313e-01],\n",
      "       [8.27279165e-02],\n",
      "       [4.26433701e-03],\n",
      "       [7.74184227e-01],\n",
      "       [1.08065188e-03],\n",
      "       [7.98153575e-04],\n",
      "       [3.05003603e-04],\n",
      "       [9.10275996e-01],\n",
      "       [9.08637464e-01],\n",
      "       [8.29948246e-01],\n",
      "       [8.51595938e-01],\n",
      "       [7.88895995e-04],\n",
      "       [7.71558523e-01],\n",
      "       [1.06474450e-04],\n",
      "       [1.22700334e-02],\n",
      "       [2.35236995e-03],\n",
      "       [1.07709575e-03],\n",
      "       [8.26526046e-01],\n",
      "       [9.87028778e-01],\n",
      "       [4.20667708e-01],\n",
      "       [2.64160991e-01],\n",
      "       [8.85431170e-01],\n",
      "       [1.74941197e-02],\n",
      "       [8.99799943e-01],\n",
      "       [3.68184876e-04],\n",
      "       [9.45477843e-01],\n",
      "       [9.55343187e-01],\n",
      "       [9.84072447e-01],\n",
      "       [9.53348696e-01],\n",
      "       [9.41223562e-01],\n",
      "       [9.52856429e-03],\n",
      "       [9.91299212e-01],\n",
      "       [3.12388781e-03],\n",
      "       [3.62647232e-03],\n",
      "       [1.06270728e-03],\n",
      "       [9.09347180e-03],\n",
      "       [8.95632982e-01],\n",
      "       [9.05875862e-01],\n",
      "       [2.03257659e-03],\n",
      "       [1.48747629e-03],\n",
      "       [3.35715897e-02],\n",
      "       [7.37910867e-01],\n",
      "       [9.57350552e-01],\n",
      "       [1.83421522e-02],\n",
      "       [1.40328547e-02],\n",
      "       [2.81453639e-01],\n",
      "       [4.45080101e-01],\n",
      "       [9.26665008e-01],\n",
      "       [9.23005879e-01],\n",
      "       [9.61777806e-01],\n",
      "       [7.85680473e-01],\n",
      "       [9.71071124e-01],\n",
      "       [8.74786973e-01],\n",
      "       [7.56396796e-04],\n",
      "       [5.65054007e-02],\n",
      "       [2.27842294e-02],\n",
      "       [1.69378761e-02],\n",
      "       [9.84166205e-01],\n",
      "       [9.40715075e-01],\n",
      "       [9.52265024e-01],\n",
      "       [1.61851360e-03],\n",
      "       [5.66105470e-02],\n",
      "       [8.44763637e-01],\n",
      "       [5.23847282e-01],\n",
      "       [9.80280519e-01],\n",
      "       [1.73192210e-02],\n",
      "       [9.27632868e-01],\n",
      "       [3.07450024e-03],\n",
      "       [7.19250500e-01],\n",
      "       [9.59696293e-01],\n",
      "       [9.04649138e-01],\n",
      "       [7.61898756e-01],\n",
      "       [9.93614614e-01],\n",
      "       [9.88797486e-01],\n",
      "       [5.09104013e-01],\n",
      "       [6.54463889e-04],\n",
      "       [2.00967959e-04],\n",
      "       [1.10614870e-03],\n",
      "       [2.88378354e-02],\n",
      "       [1.86048597e-02],\n",
      "       [8.03057671e-01],\n",
      "       [2.83686444e-03],\n",
      "       [3.88930477e-02],\n",
      "       [9.84862566e-01],\n",
      "       [8.61621797e-01],\n",
      "       [6.70268416e-01],\n",
      "       [8.93167675e-01],\n",
      "       [3.15304566e-03],\n",
      "       [4.25337076e-01],\n",
      "       [9.62635994e-01],\n",
      "       [6.67225523e-03],\n",
      "       [1.31465867e-02],\n",
      "       [7.87567556e-01],\n",
      "       [9.60090637e-01],\n",
      "       [9.77726460e-01],\n",
      "       [2.83006323e-03],\n",
      "       [9.21079695e-01],\n",
      "       [3.04724276e-03],\n",
      "       [1.87206897e-03],\n",
      "       [9.20107663e-01],\n",
      "       [7.60520226e-04],\n",
      "       [7.59520352e-01],\n",
      "       [8.19255412e-01],\n",
      "       [9.45886433e-01],\n",
      "       [4.90000530e-04],\n",
      "       [9.73040283e-01],\n",
      "       [1.07784038e-02],\n",
      "       [9.84417498e-01],\n",
      "       [9.70033586e-01],\n",
      "       [9.83309686e-01],\n",
      "       [2.78677995e-04],\n",
      "       [8.09374498e-04],\n",
      "       [4.44166712e-04],\n",
      "       [9.85423803e-01],\n",
      "       [3.10826837e-03],\n",
      "       [6.70019398e-03],\n",
      "       [9.60956216e-01],\n",
      "       [3.62823671e-03],\n",
      "       [5.69561005e-01],\n",
      "       [9.62687373e-01],\n",
      "       [7.22370267e-01],\n",
      "       [4.61758405e-01],\n",
      "       [4.07042168e-03],\n",
      "       [9.98615623e-01],\n",
      "       [1.11954298e-03],\n",
      "       [1.73442753e-03],\n",
      "       [3.97224873e-01],\n",
      "       [2.45822244e-03],\n",
      "       [9.22175229e-01],\n",
      "       [9.59329247e-01],\n",
      "       [3.24945850e-03],\n",
      "       [4.93519881e-04],\n",
      "       [8.23754907e-01],\n",
      "       [9.30618823e-01],\n",
      "       [9.83516584e-05],\n",
      "       [8.91272485e-01],\n",
      "       [2.00002408e-03],\n",
      "       [6.89948022e-01],\n",
      "       [5.49619436e-01],\n",
      "       [4.46393620e-03],\n",
      "       [1.80944870e-03],\n",
      "       [4.76916693e-03],\n",
      "       [1.90825609e-03],\n",
      "       [7.69917369e-01],\n",
      "       [9.51065600e-01],\n",
      "       [6.97432333e-05],\n",
      "       [9.46367085e-01],\n",
      "       [8.20602119e-01],\n",
      "       [3.80626954e-02],\n",
      "       [2.13598944e-02],\n",
      "       [9.44306433e-01],\n",
      "       [3.70125030e-03],\n",
      "       [5.78410574e-04],\n",
      "       [2.74912983e-01],\n",
      "       [9.70926642e-01],\n",
      "       [1.36248360e-03],\n",
      "       [3.76996206e-04],\n",
      "       [2.64884412e-01],\n",
      "       [9.76848826e-02],\n",
      "       [9.71783340e-01],\n",
      "       [7.09064007e-01],\n",
      "       [5.69549680e-01],\n",
      "       [9.28511202e-01],\n",
      "       [5.65399649e-03],\n",
      "       [9.44876592e-05],\n",
      "       [6.84880279e-03],\n",
      "       [6.83736920e-01],\n",
      "       [9.49233770e-01],\n",
      "       [9.57450151e-01],\n",
      "       [8.46187526e-04],\n",
      "       [1.31175597e-03],\n",
      "       [6.66516304e-01],\n",
      "       [9.57338572e-01],\n",
      "       [9.97811139e-01],\n",
      "       [9.55852449e-01],\n",
      "       [9.09531882e-05],\n",
      "       [8.57198060e-01],\n",
      "       [9.38167930e-01],\n",
      "       [9.70897853e-01],\n",
      "       [8.98995578e-01],\n",
      "       [9.10242796e-01],\n",
      "       [9.57207739e-01],\n",
      "       [9.01505947e-01],\n",
      "       [5.53153316e-03],\n",
      "       [3.46567016e-04],\n",
      "       [8.97050370e-03],\n",
      "       [3.31601277e-02],\n",
      "       [1.54241652e-03],\n",
      "       [8.58024538e-01],\n",
      "       [9.77742851e-01],\n",
      "       [3.07151885e-03],\n",
      "       [8.68185103e-01],\n",
      "       [8.79194140e-01],\n",
      "       [6.42108023e-01],\n",
      "       [4.33019474e-02],\n",
      "       [8.15067673e-04],\n",
      "       [3.14912130e-03],\n",
      "       [7.05362484e-02],\n",
      "       [5.32950682e-04],\n",
      "       [1.00766763e-03],\n",
      "       [9.15163875e-01],\n",
      "       [7.80796120e-03],\n",
      "       [5.45207295e-04],\n",
      "       [9.76987541e-01],\n",
      "       [9.50672388e-01],\n",
      "       [3.24598104e-01],\n",
      "       [1.77379742e-01],\n",
      "       [4.10821056e-03],\n",
      "       [2.81449765e-01],\n",
      "       [7.66986515e-04],\n",
      "       [9.87167299e-01],\n",
      "       [7.81624436e-01],\n",
      "       [8.91420618e-03],\n",
      "       [1.64003707e-02],\n",
      "       [7.40706980e-01],\n",
      "       [9.17908456e-03],\n",
      "       [1.70290901e-03],\n",
      "       [5.39696142e-02],\n",
      "       [1.49938036e-02],\n",
      "       [1.24162951e-04],\n",
      "       [7.36316264e-01],\n",
      "       [5.19536138e-01],\n",
      "       [6.74428165e-01],\n",
      "       [9.52064276e-01],\n",
      "       [9.39198971e-01],\n",
      "       [7.28074729e-01],\n",
      "       [2.46096955e-04],\n",
      "       [9.86386895e-01],\n",
      "       [5.94004057e-02],\n",
      "       [9.50925648e-01],\n",
      "       [9.26100791e-01],\n",
      "       [3.46662506e-04],\n",
      "       [8.47718060e-01],\n",
      "       [2.12130763e-04],\n",
      "       [5.64754784e-01],\n",
      "       [6.18302845e-04],\n",
      "       [2.43252423e-03],\n",
      "       [9.71446037e-01],\n",
      "       [9.47666287e-01],\n",
      "       [2.77280152e-01],\n",
      "       [9.42098677e-01],\n",
      "       [8.34090710e-01],\n",
      "       [9.60704684e-01],\n",
      "       [1.02945872e-01],\n",
      "       [3.23676481e-03],\n",
      "       [9.13092375e-01],\n",
      "       [5.49049699e-04],\n",
      "       [2.83678848e-04],\n",
      "       [1.43585482e-03],\n",
      "       [8.22447896e-01],\n",
      "       [9.59567130e-01],\n",
      "       [1.27602994e-01],\n",
      "       [1.64210517e-02],\n",
      "       [2.17938842e-03],\n",
      "       [1.91246823e-03],\n",
      "       [9.33301868e-04],\n",
      "       [2.05838401e-03],\n",
      "       [9.30604398e-01],\n",
      "       [2.55429826e-04],\n",
      "       [1.20587880e-04],\n",
      "       [7.05329148e-05],\n",
      "       [8.51273119e-01],\n",
      "       [9.59809661e-01],\n",
      "       [5.73919535e-01],\n",
      "       [9.97900784e-01],\n",
      "       [9.92050767e-01],\n",
      "       [9.53634083e-01],\n",
      "       [5.73800802e-01],\n",
      "       [6.54947129e-04],\n",
      "       [7.67646492e-01],\n",
      "       [4.40865086e-04],\n",
      "       [9.49718177e-01],\n",
      "       [4.22180584e-03],\n",
      "       [9.68479514e-01],\n",
      "       [5.89987218e-01],\n",
      "       [9.73416090e-01],\n",
      "       [8.87683272e-01],\n",
      "       [1.06492313e-03],\n",
      "       [8.98306370e-01],\n",
      "       [2.77280152e-01],\n",
      "       [1.55968564e-02],\n",
      "       [4.25571129e-02],\n",
      "       [5.54352283e-01],\n",
      "       [1.53835653e-03],\n",
      "       [7.99219787e-01],\n",
      "       [5.27964847e-04],\n",
      "       [9.55086470e-01],\n",
      "       [4.42879647e-03],\n",
      "       [9.03620601e-01],\n",
      "       [2.77280152e-01],\n",
      "       [8.26874971e-01],\n",
      "       [9.82036650e-01],\n",
      "       [1.06227361e-02],\n",
      "       [9.56352711e-01],\n",
      "       [9.90365207e-01],\n",
      "       [9.82239306e-01],\n",
      "       [3.41410562e-03],\n",
      "       [4.47310414e-03],\n",
      "       [3.74532363e-04],\n",
      "       [6.20125188e-03],\n",
      "       [1.17384625e-04],\n",
      "       [6.03516120e-03],\n",
      "       [9.78015065e-01],\n",
      "       [6.64853374e-04],\n",
      "       [2.37938642e-04],\n",
      "       [9.37730908e-01],\n",
      "       [9.34377253e-01],\n",
      "       [2.64884412e-01],\n",
      "       [9.77096379e-01],\n",
      "       [1.95883767e-04],\n",
      "       [9.85638320e-01],\n",
      "       [5.22964285e-04],\n",
      "       [7.31688440e-01],\n",
      "       [3.02557468e-01],\n",
      "       [4.33246046e-03],\n",
      "       [8.79941523e-01],\n",
      "       [3.96679282e-01]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNice job! Let's use this function for something more interesting.\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "It's a flow of tensors\n",
    "\n",
    "If you have already built a model, you can use the model.layers and the tensorflow.keras.backend to build functions that, provided with a valid input tensor, return the corresponding output tensor.\n",
    "\n",
    "This is a useful tool when we want to obtain the output of a network at an intermediate layer.\n",
    "\n",
    "For instance, if you get the input and output from the first layer of a network, you can build an inp_to_out function that returns the result of carrying out forward propagation through only the first layer for a given input tensor.\n",
    "\n",
    "So that's what you're going to do right now!\n",
    "\n",
    "X_test from the Banknote Authentication dataset and its model are preloaded. Type model.summary() in the console to check it.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import tensorflow.keras.backend as K.\n",
    "    Use the model.layers list to get a reference to the input and output of the first layer.\n",
    "    Use K.function() to define a function that maps inp to out.\n",
    "    Print the results of passing X_test through the 1st layer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import tensorflow.keras backend\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job! Let's use this function for something more interesting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot():\n",
    "  fig, ax = plt.subplots()\n",
    "  plt.scatter(layer_output[:, 0], layer_output[:, 1],c = y_test,edgecolors='none')\n",
    "  plt.title('Epoch: {}, Test Accuracy: {:3.1f} %'.format(i+1, test_accuracy * 100.0))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9563\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.9563\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9636\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9660\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9684\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThat took a while! If you take a look at the graphs you can see how the neurons are learning to spread out the inputs based on whether they are fake or legit dollar bills. (A single fake dollar bill is represented as a purple dot in the graph) At the start the outputs are closer to each other, the weights are learned as epochs go by so that fake and legit dollar bills get a different, further and further apart output. Click in between the graphs fast, it's like a movie!\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Neural separation\n",
    "\n",
    "Put on your gloves because you're going to perform brain surgery!\n",
    "\n",
    "Neurons learn by updating their weights to output values that help them better distinguish between the different output classes in your dataset. You will make use of the inp_to_out() function you just built to visualize the output of two neurons in the first layer of the Banknote Authentication model as it learns.\n",
    "\n",
    "The model you built in chapter 2 is ready for you to use, just like X_test and y_test. Paste show_code(plot) in the console if you want to check plot().\n",
    "\n",
    "You're performing heavy duty, once all is done, click through the graphs to watch the separation live!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use the previously defined inp_to_out() function to get the outputs of the first layer when fed with X_test.\n",
    "    Use the model.evaluate() method to obtain the validation accuracy for the test dataset at each epoch.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "for i in range(0, 21):\n",
    "  \t# Train model for 1 epoch\n",
    "    h = model.fit(X_train, y_train, batch_size = 16, epochs = 1, verbose = 0)\n",
    "    if i%4==0: \n",
    "      # Get the output of the first layer\n",
    "      layer_output = inp_to_out([X_test])[0]\n",
    "      \n",
    "      # Evaluate model accuracy for this epoch\n",
    "      test_accuracy = model.evaluate(X_test, y_test)[1] \n",
    "      \n",
    "      # Plot 1st vs 2nd neuron output\n",
    "      #plot()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That took a while! If you take a look at the graphs you can see how the neurons are learning to spread out the inputs based on whether they are fake or legit dollar bills. (A single fake dollar bill is represented as a purple dot in the graph) At the start the outputs are closer to each other, the weights are learned as epochs go by so that fake and legit dollar bills get a different, further and further apart output. Click in between the graphs fast, it's like a movie!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 784)               25872     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat start! Your autoencoder is now ready. Let's see what you can do with it!\\n\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise endocer ---- NOT WORKING\n",
    "\n",
    "'''# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Building an autoencoder\n",
    "\n",
    "Autoencoders have several interesting applications like anomaly detection or image denoising. They aim at producing an output identical to its inputs. The input will be compressed into a lower dimensional space, encoded. The model then learns to decode it back to its original form.\n",
    "\n",
    "You will encode and decode the MNIST dataset of handwritten digits, the hidden layer will encode a 32-dimensional representation of the image, which originally consists of 784 pixels (28 x 28). The autoencoder will essentially learn to turn the 784 pixels original image into a compressed 32 pixels image and learn how to use that encoded representation to bring back the original 784 pixels image.\n",
    "\n",
    "The Sequential model and Dense layers are ready for you to use.\n",
    "\n",
    "Let's build an autoencoder!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a Sequential model.\n",
    "    Add a dense layer with as many neurons as the encoded image dimensions and input_shape the number of pixels in the original image.\n",
    "    Add a final layer with as many neurons as pixels in the input image.\n",
    "    Compile your autoencoder using adadelta as an optimizer and binary_crossentropy loss, then summarise it.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great start! Your autoencoder is now ready. Let's see what you can do with it!\n",
    "\"\"\"'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example From Keras Documentation\n",
    "link : https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 784)               25872     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fixed AutoEncoder\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "#################################\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 8s 1us/step\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "\n",
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 5s 18ms/step - loss: 0.2745 - val_loss: 0.1903\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1712 - val_loss: 0.1539\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.1442 - val_loss: 0.1336\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.1286 - val_loss: 0.1213\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1182 - val_loss: 0.1126\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.1109 - val_loss: 0.1066\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.1057 - val_loss: 0.1023\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.1019 - val_loss: 0.0991\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.0992 - val_loss: 0.0968\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0973 - val_loss: 0.0954\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.0961 - val_loss: 0.0943\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0953 - val_loss: 0.0938\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 3s 14ms/step - loss: 0.0947 - val_loss: 0.0933\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0943 - val_loss: 0.0930\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0941 - val_loss: 0.0927\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0939 - val_loss: 0.0926\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0937 - val_loss: 0.0924\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.0936 - val_loss: 0.0923\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.0935 - val_loss: 0.0922\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.0934 - val_loss: 0.0922\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.0933 - val_loss: 0.0921\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0933 - val_loss: 0.0921\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0932 - val_loss: 0.0921\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0932 - val_loss: 0.0919\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 3s 15ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.0931 - val_loss: 0.0920\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 5s 21ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0928 - val_loss: 0.0919\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 3s 14ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0928 - val_loss: 0.0918\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 4s 15ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.0927 - val_loss: 0.0917\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.0927 - val_loss: 0.0917\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 3s 15ms/step - loss: 0.0927 - val_loss: 0.0916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe16df86050>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_encodings(encoded_imgs,number=1):\n",
    "    n = 5  # how many digits we will display\n",
    "    original = X_test_noise\n",
    "    original = original[np.where(y_test == number)]\n",
    "    encoded_imgs = encoded_imgs[np.where(y_test==number)]\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    #plt.title('Original '+str(number)+' vs Encoded representation')\n",
    "    for i in range(min(n,len(original))):\n",
    "        # display original imgs\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display encoded imgs\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(np.tile(encoded_imgs[i],(32,1)))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(original,decoded_imgs):\n",
    "    n = 4  # How many digits we will display\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.title('Noisy vs Decoded images')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_noise = np.load(path_data + 'X_test_MNIST_noise.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABYIAAAFICAYAAADtUIrZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABY/klEQVR4nO3deZyP9fr48WuYDTPGMrYxdpU1khCOLRJZWtTUEYkkcqhEKKlUljqVQ0T6qoNKtpJyKmtUSPYsB9nHNpYxlhmz3L8/+s0cn7muqXsWy9xez8fj+/h95+Wz3DP87vfc7z7f+/JzHMcRAAAAAAAAAIBn5bnaBwAAAAAAAAAAuLzYCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOPYCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPA4fzcPSklJkejoaAkNDRU/P7/LfUzI5RzHkbi4OImIiJA8efhvDQD+h/UEmcF6AuDPsKYgM1hTAGSE9QSZkdvXE1cbwdHR0VKmTJnLfSzwmAMHDkhkZOTVPgwA1xDWE2QF6wkAC2sKsoI1BUB6rCfIity6nrjaCA4NDTV72bJlVdu/f7+rN7b+P9mRI0dUS0xMdPV6JUqUUO3o0aOqFS5cWLVTp065eo+rZfLkyao98cQTWX49tz+rli1bqrZq1SrVSpUq5fN1cnKy/P777xn+uwFw/crMeaF58+aqWQvttGnTVKtZs6ZqmzdvVq1AgQKqnTt3TjXrkwF58+ZVLSkpSTXLLbfcotr69etVK1SokGrFihVTbefOnarly5dPtQsXLqgWFBSkWkJCgmqW0qVLq3bo0CFXz7X+LcTFxbl+LABk9xqlXr16rt5nzZo1mTuwv1CpUiXVdu/enaPvcbUEBwerFhYWplpycrJqlStXVu33339X7dixY1k8uj+wpgBILzPnBX9/vY3m9hqgatWqqt12222qbd26VbW1a9e6eg+3rL2x9957T7Unn3xStfR7QCIie/bsUe3ixYtZPDr31xlt27ZVrUWLFqrNmDFDNev6y3pcz549fb52HEcuXLiQa9cTVxvBGX00Pjsfgbaem52P4Ls9ltz4sW3rYj473P4MrBOc282QjB4L4PqWmfOCdQ4KDAx09dyMzktZPR7rcdk5x2Xn+K7E9+ZWdtbUzLwv6wkAS3avUax15krIznnccZycPpwcZR2z9fdhfR/W38fluHZjTQGQ3qXnhUv/d+tcldPXANb1zZVYn6zza/78+VVzez2S0+dWt+f/gIAA1aw9NLdrr9ufwZ/1a13u2xUFAAAAAAAAAGQKG8EAAAAAAAAA4HGZ+rx5wYIFfT76vHfvXvWYO+64QzXrfovWvWbLlSunWoUKFVSrUaOGal999ZVq1sfBT5w4oVq1atVUs+7J0qpVK9W+//571XJaly5dVLP+zwesj8Rb999q166datb/6cHEiRNdHd/27dtdPQ4AMqNDhw6qTZo0ydVzo6OjXT1u2LBhqlnnvpMnT6p25swZ1YYMGaLayJEjVXN7jy/rHvbWPX0LFizo6vis+zS+//77qj388MOumnXP+Y0bN6q2dOlS1QYPHqza0KFDVQOAzLKuUaz7z/7000+uXs/63XnBggWqNW7cWLXWrVurZq09ltGjR6s2aNAgV891y+29Lm+99VbVrPkuVrOuZRYvXqzaypUrVbPuEdmmTRvVFi5cqBoAZMVf3YLH7Rwry5YtW1S75557VFu0aFGW36NIkSKqWdcyd999t6tjscTGxqr22GOPqTZ16lTVoqKiVKtVq5Zqbq8LvvzyS9V+++031Xbt2qWadW306aefqmbtaeZmfCIYAAAAAAAAADyOjWAAAAAAAAAA8Dg2ggEAAAAAAADA49gIBgAAAAAAAACP83P+6k7Y8sfAmbCwMOnTp4/PkJp33nnnsh6ciMjkyZNV++abb1Tbtm2bajt27FCta9euqi1ZskS1gwcPqjZr1izVHnjgAdWsYXYW60bhFus9rGNxy7ohtnWz7/vvv1+1OXPmuH6f2NhYc4ARgOtX6nqyePFiKVCgQFp//PHH1WPLlCmjmjWE4F//+pdq1sAZS7169VS7dChqqt27d6sWExPj6j3cKlu2rGo33XSTauvWrVOtSZMmqs2bN8/V+1oDU8+fP6+aNXzJYg3xsV5v+fLlqt11110+XyclJcmiRYtYTwCYUteU9KpUqaKaNdy4fv36qlnDzqwB1Dk9LNk6Zut7K1mypGrW+dka3GmxrjOsIdzt27dXzboesQb+WMdy6e8AqVq0aOHqWBo0aKCaNYi8X79+Pl8nJCTIpEmTWFMAKKnrSYMGDXyGaObJoz87Wa5cOdWs64e4uDjV3P5+Xrx4cfMY0wsICHD1vtWrV1ctb968qllD0SIjI1XbtGmTataQa0vt2rVV27Bhg6vH3Xbbbap98MEHrt730KFDqlkDSW+88UbV0q8ZycnJsn79+ly7nvCJYAAAAAAAAADwODaCAQAAAAAAAMDj2AgGAAAAAAAAAI9jIxgAAAAAAAAAPC5Tw+LS3zjbGsZjDV1Yv369atYwmWXLlqlWokQJ1UqVKqVa3759VbOGH1iioqJUGzhwoGp169ZVrXLlyqrt2rXL1ftmR+/evVWbOHFill+vUKFCqp0+fdrVc9MPe3AcR86fP59rb5wN4PLJaLBPTrMGNljD544dO6baxYsXVbMGBX388ceqWevda6+9ppq1flprTK9evVRr166dag8++KCr17OGLkyZMkW1nDZy5EjVhgwZ4vr5rCcALNldU95++23Vnn322Sw/zhIYGKiatc5Y50Tr3Pnwww+r9vnnn6uWnJzs6visIXDWgDa3Xn75ZdU+/PBD1Q4cOJDl93DrkUce8fn64sWL8vnnn7OmAFAys55Y1xkRERGqBQUFqfb777+r5nYAZqtWrVT7/vvvVbO+jxo1aqj2448/qmYNr96/f79qbtcst6zrlv79+6vWo0cP1ayBedbPdPHixa6OxRrC/cMPP5iPza3rCZ8IBgAAAAAAAACPYyMYAAAAAAAAADyOjWAAAAAAAAAA8Dg2ggEAAAAAAADA4zI1LC69t956S7XnnntOtZCQENXOnj3r9hhd6dq1q2rR0dGqLVq0SLW8efOq5nbAQvXq1VU7cuSIanfccYdq1mCHyMhI1Q4ePOjqWNIPRBARSUpKcnV81qA+a9jerFmzVEs/gCklJUUOHTqUa2+cDeDySV1PBg0a5DNAwRqo5mJ5EhGRxo0bq2YNY8sOa6jNyZMnVbv55ptz9H0t1kDSG264QbVRo0apNnjwYNWs4QzWsFBrsF7nzp1VW7FihWqXDppNlSeP/m/R69atU00k9w5iAHB5pa4ppUuX9jmn3HPPPeqx48aNy/L7lCxZUrXjx4+rZl0/jBkzRrVBgwZl+Vis4W4dOnRQrWfPnq5er169eqpZA4kSEhJUs4bo3Xjjjapt377d1bHktKlTp/p8feHCBenTpw9rCgAloz2vuXPnqmZdt2T0O2x61vnV+j154cKFrl4vp0VFRan2/PPPq3bLLbeoZg1Rfeedd1y97+jRo1Wzhs9Zg/pOnTrl6j2aNm2qmvWzdztUTiT3XqPwiWAAAAAAAAAA8Dg2ggEAAAAAAADA49gIBgAAAAAAAACPYyMYAAAAAAAAADwuU8Pi8uTJ43NzZmsggjVMwRpO9swzz6jm9kbSjz/+uGpTpkxx9dyOHTuqVqpUKdXef/99V683fPhw1V555RVXzw0NDVXtgw8+UM266faZM2dUs4YU/fTTT6qNGDFCtVWrVqn2n//8RzXr7zz9oKHk5GRZv359rr1xNoDLJ3U9KVy4sM96Yg1ec6tEiRKqHT16VLUhQ4ao9t5776lmDfe01pgTJ06o9u9//1u1ChUqqNa9e3fVPvvsM9Ueeugh1ayBCPfff79qS5cuVe1KcPv3Yf0ekH6gUGJioixatIj1BIApo+E+xYsXV80aIGcNwbSGsfXo0SNrB5hNxYoVU61AgQKq3Xvvvaq5va6yBr5ZA0jnz5+vmnXd4vbnfP78edU2bNigmvX3YQ0qtdbL8uXL+3ydkpIi+/fvZ00BoKSuJ3369PEZaG2dSy/981TWPti+ffuyfDy9e/dWzRq8mdO/71v7PdaW4YULF1Tr2rWravPmzXP1vkWKFFHN7fXhG2+8odrQoUNV69u3r2rjx4939R4RERE+X6ekpMiRI0dy7XrCJ4IBAAAAAAAAwOPYCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPC4TA2Lu/XWW8Xf3z+t7927Vz22YcOGqq1YsUK1mJgY1W688UbVrEFzAQEBquXJo/e0jx8/rtqV0KVLF9WmTZvm6rl33XWXatbQNuv7tQYcff/9967et0mTJqr98MMPqlnDKNIPgEhKSpLVq1fn2htnA7h8Mhrs06BBA9WsIZZuVatWTbWtW7eq9sILL6j22muvuXoP67xuDcSxhjh8/fXXrt7DxRItIiLNmjVTzRrQZg2ZSD/wU0QkPj5etS1btqhWuXJl1Xbt2pXRYfrInz+/avXq1fP5OikpSVauXMl6AsCUuqZERUVJYGBgWrfOuy1btlStatWqql06yDTVoEGDsnyMzZs3V806t//666+qWcN4rPXSOu9a11DWQJ06deqo1rp1a9Wsn4vF7cBQa+jdggULVLPW5MWLF7s6loywpgBIL6NrlIEDB6r25ptvqlauXDnV3A6LswacWtcU1qDlsWPHqpaSkqJaSEiIan369FFt9OjRGR7npS5evKja7bffrtqmTZtUS0pKcvUe+fLlU61s2bKq7dixQzVr2GrFihVVW716tatjyUhuXU/4RDAAAAAAAAAAeBwbwQAAAAAAAADgcWwEAwAAAAAAAIDHsREMAAAAAAAAAB6XqWFxFStW9BlS5nYgTPv27VX76quvVLNusN22bVvVnn/+edW++OIL1T799FPVrJtBpx9OI2J/bzVr1lRt+fLlqlkaNWqkmjU4we3P1K3q1aur9ttvv+Xoe2Qkt944G8Dlk9EgBresoQHWMB2Lda63hhVYQxKsIUNlypRR7fz586qdPHlSNWuwaq1atVSbMGGCatYxW8MPLAcOHHD1uGsN6wkAS+qaEhgY6DPMzForTp8+rVrJkiVVs4as9ezZUzVr0GavXr3+6pAzxRogZA3p/Omnn7L8Htal4Oeff65aVFSUq9ezBvncc889qm3btk01t0Ous4s1BUB6mblGqVKlimrWQOZChQqpZq1FOe3ll19Wbfjw4Vl+vXnz5qlmDb7esGGDatb5f8qUKap17NhRtZdeekm18PDwDI7yr/Xo0UO1Dz/8ULWgoCDVEhISzNfMresJnwgGAAAAAAAAAI9jIxgAAAAAAAAAPI6NYAAAAAAAAADwODaCAQAAAAAAAMDjMjUsLr0OHTqoNn/+fNX8/f1Vu+mmm1Szhgbccsstqq1du1a1b7/9VrXbbrtNNWtQWkxMjGpjx45VbejQoaoVLlxYtXPnzqkWHx+vmjXIzbp5uDWkzmLdAPzVV19V7aGHHlLts88+Uy0iIkK1gIAA1fbt22ceT269cTaAyyej9cQaIGoNGi1SpIhq1iC3/fv3q7Zq1SrVHn/8cdWsAQZuWQPkEhMTVbN+BtbxWefXH3/8UbW9e/eq9uabb2Z0mDmmcePGqtWvX1+1iRMnqmYN1ssI6wkAS+qacuedd/r8jmoNsbF+r7Wadd49fvy4q+N54YUXVHv99ddVa9OmjWrWcBpr/bC+N4v1O7u19ljfW2RkpGqHDh1SrVOnTqrNnj07y49r0KCBaqVLl1Ztzpw5qt14442q/fe//1VNhDUFgJbdgdZuWcOrmzRpopp1bv74449dvYeLLb4MvfXWW6pZv+9b52vLkiVLVGvRooVqy5YtU6158+aqWft0/fv3d3UsbgfEWvtl6ffu4uPj5fXXX8+16wmfCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOMyNSyuSJEikifP//aOrSFrlooVK6r2+++/qxYeHq6a9R6bN29WLS4uTrWffvpJtbfffls1a/jBzz//rNqBAwdUe/DBB1WzpKSkqGZ9b8WLF1ftxRdfVK1Zs2aqDRw4ULUNGza4Oj63qlSpolpUVJTP1wkJCTJq1Khce+NsAJdP6nryxhtvSHBwcFp/9tln1WOtwQQrV668rMcnIlKoUCHVQkNDVbv11ltVy5s3r2rWUBvrudYgVLdq166t2saNG1091+3g10vX/1TW2nY5sJ4AsGQ03McavPPDDz+oZl0XWOuRZfHixa6ea60L1vk5OTlZNWugqfV9WEPRrIF01mBW6/j8/PxUswa4Wq93JVhrsnUtmBHWFADpZXdYnHVO6dmzp2r//Oc/s/weFmv4snVubtu2rWoHDx5UzdrvceuJJ55QbfLkyVl+PWstshQtWlS1EydOuHpu+iFwIiK//fabq+eK5N71hE8EAwAAAAAAAIDHsREMAAAAAAAAAB7HRjAAAAAAAAAAeBwbwQAAAAAAAADgcf6ZefD9998vgYGBaV9bN2W2BhgsW7bM1etbw9Puvvtu1WrWrKnaY489ptrevXtVi4yMVG3NmjWqNWzYMKPD9LFv3z7VrEFz8fHxqn355ZeqWTfE7tatm2otW7ZUzRr4Yw2L+/TTT1WzbuxtDV3Yvn27amfPnvX5OiEhQT0GAC61bNkyCQgI+NPHuB0M9/TTT6v27rvvqlauXDnVbrjhBtU2bdqkmjUs1Bq6YA0XsIberVixQrVjx46ptn79etX69u2r2q5du1SzWEMrPvjgA1fPrVGjhmrWzyp//vyuXs8abgEAOaFFixaqWdco1nA363fsRYsWqXbHHXdk8ejEXP+KFCmimvV7vDUYztKuXTvVrMFw1trToEED1axrKMsrr7yi2tSpU1WzrtOs9cNaK/z99SWsdQ312muv+XwdHx8vI0aMUI8DgIxYv/+GhISotmrVKtWsc65bDz74oGrW/ky+fPlcPddi7VFNmTJFtVq1aqk2ZswY1aw9uSeffFK1Z555RrVTp05leJx/pXDhwqq5HRbndjBc+r9Lx3Gu2NDsy4FPBAMAAAAAAACAx7ERDAAAAAAAAAAex0YwAAAAAAAAAHgcG8EAAAAAAAAA4HF+juM4f/WgM2fOSFhYmKsXtAYdnDx5UrXBgwerNmrUKFfvYbGG3UycOFE1a/CO9SN44403VBs6dKirYylWrJhqpUuXVq1JkyauXi88PFy1YcOGqRYREaHa4cOHVStevLhq1qAI632tln6IQ0pKihw8eFBiY2OlYMGC6vEArl8ZrSfWsMv58+er5naQjKVQoUKqDRgwQDVr7YiOjnb1HhZryMT//d//qXbbbbepVqVKFdWeeuop1aw11RrE8Prrr2d4nJeyjnnLli2unvv222+rNmHCBNWsv7cePXr4fJ2QkCBjxoxhPQFgysw1ilu9evVSbdKkSa6eW6pUKdWs38WbNm2q2vLly129hyVPHv3ZHmuIjYvLPhGx16MjR46odvDgQVevZw0a2rhxo6vn9uvXT7V//etfrp5bvXp1n6+Tk5Nl+/btrCkAlNT15L777vMZ6Dlz5kz1WGuvaMGCBar99NNPOXqM1jnc2t+65ZZbXL3e8ePHVXM7WHX//v2qVapUSbXdu3erZg36to65efPmqm3fvl01a7DqN998o9rmzZtVcztULiO5dT3hE8EAAAAAAAAA4HFsBAMAAAAAAACAx7ERDAAAAAAAAAAex0YwAAAAAAAAAHicf06/oDUYrmTJkqp9/vnnrl7vkUceUW369Omq9ezZU7U77rhDNesG271791Zt8uTJro7PYg1EWLt2rWrWgIV33nlHtfbt26t27733qla0aFHVrAEV1mA4y5tvvqmaNaQu/TCjpKQk18MjAFyf6tevL/7+/1uCli5d6up5bgfD9e/fX7WxY8eqNm/ePNWswXDWGmMNKR09erRqgwYNUs0anGDZsWOHar/99ptqgYGBqrkdDGetJ9bPpUuXLqpZw4isgRLW8LmbbrpJtT179vh8ffHiRfUYAEjvoYce8jkP/vvf/3b1vLx586pm/e48ZcoU1R5//HFXz7VYg+EuXRNTpR+gKWJ/b2XLllXNGqhjraGdOnVSLTg4WDXrd/vQ0FDVOnfurNr777+vWsOGDVWz1i3rOs2tG2+80efrxMRE8+cCAKnmzp3r8/X999+vHjN06FBXr1W+fHnV9u7d6+q5CxcuVO2XX35RbdmyZaq5HRZXvHhx1dq0aaOaNRjOGnqa/pwrIlKtWjXVrH2rrl27Znicl1q1apVqlw73S2X9XCx169ZVzVqPrffNzfhEMAAAAAAAAAB4HBvBAAAAAAAAAOBxbAQDAAAAAAAAgMexEQwAAAAAAAAAHufnWNPT0jlz5oyEhYWpbt38ukGDBqrt2rVLNWt4mtthPJaWLVuqtmjRItWsm1Vv3brV1Xu4FRkZqZo1YMG64XT6wWsiIn5+fqqNGDFCtQMHDqg2c+ZM1QoXLqxa9erVVVu5cqVqjRs3Vi39zbmTkpJkxYoVEhsbKwULFlSPB3D9ymg9catDhw6qzZ8/PzuHpFjnXBdLpYiIHDp0SDVrvbvttttUi4iIcPUeUVFRqlnn+pxWuXJl1az1vUmTJqpZw/GsgaSffvqpz9eJiYnyn//8h/UEgCm7a0qlSpVcPS4kJES1jRs3qmYNXjt9+rRq1nWBNcSsQIECqp07d061Xr16qWYNaPv9999V+/rrr1Xr16+fatYgH2twnXWdcerUKdUs1rVl8+bNVZs6daqr18sIawqA9DKznlh7J9bvuvHx8dk+rkuNGTNGtYEDB7p6rjWkrkKFClk+lueff141a2i2xTqvW0NPP/roI9Ws9dP6Pqx9Nes6zfo7v/nmm1XL6Hozt64nfCIYAAAAAAAAADyOjWAAAAAAAAAA8Dg2ggEAAAAAAADA49gIBgAAAAAAAACP88/Ok60bTlvt4YcfVs0anuN2MJzFGgxnKVeunGrWsDi3w+esoW3Dhg1T7b333lPNGgBh/fyOHz+uWkxMjGpuhwVZAxu2bdvm6rnWADkAyC5rQIx1PtyxY0eW36Nv376qWedma1hBYmKiatbAAcumTZtUczs4x/Lbb7+5elyPHj1U+/DDD7P8vsHBwarVr19ftR9++MHV6/3973/P8rEAwKWqVq0qefPmTfvaGrxWo0YN1TZs2JCjx2ENrLEGY1rnU2tgaO/evVWzrjPOnDnj6viswXX/93//p9rChQtVa9Omjav3sNa3r776SrXJkyertmzZMtWWL1+uWmhoqGrWwB9rqBAA/JnOnTtLYGBg2tezZs1Sj3H7u3jDhg1V++mnn1w9d9SoUapZA+meffZZ1ax9oejoaNUuXTdTJScnq2adcwcMGKCaNUD08OHDqi1dulS1I0eOqGaJjIxUzbpOs4Z/W/Lnz69a7dq1VatTp47P1/Hx8ebfUW7BJ4IBAAAAAAAAwOPYCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPC4TA2LK1WqlOTJ87+9Y7eDcqyBCBm9fnoPPPCAauvWrVOtQ4cOqg0aNEi14cOHq/b444+rdv/996sWHh6umnXTbUtCQoJq8+fPV61Xr16qWTerzo7ixYurduzYMVfPrVu3rmrW4D8AyAxrMJylatWqqlkD5KKiolQLCgpSzXEc1fbt26daly5dVLMGNpQpU0a1F198UbVatWqp5taWLVtcPS47g+EsVapUUa1BgwaqrV692tXrhYSEqHb27NnMHxiA696dd97pc463zpPZGQz3888/q3b77berZp0nreHQ586dU80admMNhrMG5VSuXFk1y6233urqcd27d1etRIkSqllrsjXwrX379q5aXFycq+OzHme19GtySkqK6+tXANenBx980Oe8PXXqVPUYt0Pg3A6GK1iwoGpFixZVzdobs46vRYsWqk2fPl01azCcxTq/WntKzZo1Uy0lJUU1a2C0tTfWrl071aw9Q+u677HHHlPNuha0Bty9+uqrqqX/+7C+r9yETwQDAAAAAAAAgMexEQwAAAAAAAAAHsdGMAAAAAAAAAB4HBvBAAAAAAAAAOBxfo41KSedM2fOSFhYmDRo0ED8/f83X+7kyZPqsVu3bs3ZA/TzU83FIYuIyMiRI1UbMmSIai+88IJqr7/+uqv3sG5CXbZsWdVefvllV69nfb9uWcdsfW9uWTcoP3HihOvnx8bGmjc+B3D9Sl1P3Lj33ntVmzdvXpbf2xr4GRMT4+q5btcdy/PPP6/amDFjsvx6FmuoaLdu3VR79913VXM7qO/mm29WrVixYqodPXpUNbcD7tIPI0pJSZHjx4+zngAwpa4pDRs29LlG+eGHH9RjrUHQc+bMyfJ733333arFxsaqtnLlSlevZ10/WGtU8+bNVXvvvfdUe/LJJ1XbtWuXataAuwULFqhmXct88cUXqmVnKF92PPfcc6rNnTvX5+uUlBTZu3cvawoAJaNrlEKFCql2+vRpV6/Zpk0b1erVq6faiBEjVHM7yM3idk/p4YcfVu3StTTVtGnTXL2e9b1VqlRJtU8++US1Tz/9VDXrZ2/9TFu2bKmatfZaP9OkpCTVLDfddJN6rV27duXa9YRPBAMAAAAAAACAx7ERDAAAAAAAAAAex0YwAAAAAAAAAHgcG8EAAAAAAAAA4HH6TtB/YtWqVT5fR0REZPmNn3nmGdW++eYb1X7//XfVEhMTVatbt65q1mA4a7DD+PHjVbNu2L1//37VPvjgA1fva3n11VddPc4aAPHUU0+pZg2GK1eunGr79u1z9b5uB8Olv8l4YmKizJ4929VzAVyfOnToIAEBAWlfWwN7rMFw1hCHpk2bqjZ//nzV6tSpo5o12MwaKHT+/HnVHnroIdV69+6tWk4PhrOG3lnDeazvzRoMZ63ljRo1Um3WrFmuji8oKEg1a7BDYGCgal9++aWr9wCAS/30009/+RjrnNi6dWvVrPNk4cKFVbMGw23cuFE1t8OcreuMu+66SzVrKJo1uCglJUU1a1jcsGHDVFu0aJFqbgdflyxZUrUjR46oZl0vWYO+reF4ffr0Ue2BBx5wdXwA8GdatmzpMzBtyZIlrp5n/b4/aNAg1ay1qEKFCpk4Ql9uh51ZrAFt2VGkSBHVrMFwlnbt2qnWq1cv1axrBWvdTkhIUM0aZrdmzRrVgoODVXv00Ud9vo6Pj3e9n3ct4hPBAAAAAAAAAOBxbAQDAAAAAAAAgMexEQwAAAAAAAAAHsdGMAAAAAAAAAB4nJ/jOM5fPejMmTMSFhYmnTp18hnuc+zYMfXYm2++2dUbz5gxQzXr9Szt27dX7eLFi6p9++23qlk3fo6Pj3f1vhZrWFyDBg1cva814Cg6OjrLx5Id1nAf6wbbmREbGysFCxbM1msA8JbU9SQyMlLy5Pnz/xZpDc6xzq+lSpVS7ZZbblHtpZdecnWM1rL4xRdfqNaiRQvVrCFrW7ZsUe2NN95QbejQoarlzZtXteTkZNVCQ0NVs4YbHT58WLUmTZq4etzWrVtVK1u2rGrWEDhrQNEdd9yhWtGiRX2+TkxMlHnz5rGeADClrim1atXyOV+uX79ePbZSpUqqFSpUSDXrXGcNDLVYwzxjYmJUswafWsPnLh1YlKpTp06qWQN/rCFwr732mmrWgB5r8I414Oj48eOqPfLII6pNnz5dNeva47777lMtO8OMBgwY4PN1QkKCjB8/njUFgJK6nnTu3Nnnd9nFixerx1rXKMWLF3fVrOsCy9dff+3qfZ988knV/Pz8VOvfv79qFy5cUG3y5Mmuju/9999XrWfPnqpZ13vW3p11/TB8+HDVrKF81n6edX1jDdY7efKkata15dKlS1UTyb17XnwiGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOPYCAYAAAAAAAAAj8vUsLiKFSv63OzZGv5SokQJ1azhAs2aNVPNOpRTp06pdujQIdVOnDihmjXExhrGVr58edWs780afmDdwNoahGcdszVorlu3bqrNmTNHtbi4ONWsm5FbN8mePXu2agUKFFDt3Llzqlk3zk7/c05KSpJff/011944G8Dlk7qelC9f3mc9+f3339Vj27Rpo9rChQtVswYiuFjaMuT2udaAhfz586vWo0cP1ebPn69aYmKiaqdPn1ZtypQpqqUfiCNiDxmy1tSUlBTVKlasqNpHH32kmrWmWkMXrHW2bdu2ql06kFbkj7+LpKQk1hMAptQ1Jb0hQ4aoZg2TWbFihWpr165V7ZVXXlHNGuSzZs2aDI81p1hrlDV4xxrGZhk3bpxq1kBTa4ioNSC1VatWqr388suujsUaDGcNQrIGWmdm+DdrCoD0UteT6tWr+wwftfa3rN+nd+7cqVqxYsVUs/aZLO+8845q3bt3V806l1nXRhZrPbGGrVpDuK2hotbv+9Z1gTWA1dp7qlKlimq1atVSrV+/fqqNGDFCtdatW6s2adIk1SzpB86mpKTInj17cu16wieCAQAAAAAAAMDj2AgGAAAAAAAAAI9jIxgAAAAAAAAAPI6NYAAAAAAAAADwuEwNi3Nj7ty5qg0cOFC13bt3u3q97ChcuLBqDRo0UM0aqGYNmejdu7dqEyZMUO3tt99Wbfny5apZw5GioqJU++WXX1Szhs/9+uuvqln+8Y9/qGYNisiu3HrjbACXT0brSXh4uGrWUFFrjbGGolmDc5o3b67a0qVLVRs0aJBqo0ePVm3dunWqHThwQLWOHTuqZtmwYYNqBw8eVM0aRmENBdq4caNq1kCE9APaROzvwxoUZA1x+Omnn1S78cYbVevSpYtq1lopwnoCwJa6pjRo0MBnLVi5cqV6rDWANCQkRLVZs2apVqRIEdWsc1Ljxo1VswagJScnq2axzs/W+mYZOnSoaiNHjnT1XGuQjzUM1Roiba0VV0vt2rV9vk5OTpbNmzezpgBQUteTggUL+gxbi42NzfJrWudca5/Juja6/fbbVbOGZk+dOlW1smXLqlajRg1X73vvvfe6et+c1rdvX9Ws781an66EevXq+XydnJwsv/76a65dT/hEMAAAAAAAAAB4HBvBAAAAAAAAAOBxbAQDAAAAAAAAgMexEQwAAAAAAAAAHpetYXGVK1dWzRq6YA3ASX/z/tT3SS8mJka1Ro0aqZbTN7Du3LmzamPGjFHt/fffd9X69eun2vDhw1VLSUlxdXx58+ZVzRo8Ua5cOdUKFCig2tatW1WrVauWatbwofQ3FE9MTJQFCxbk2htnA7h8UteTevXq+Qz2sQaMuVW/fn3VEhMTVStRooRqx48fV+2hhx5SrVOnTqpZ51e3Vq1apZo1zNRiHXOxYsWy/Dhr8JA1GGPHjh2qWYORrCF61q8a1kDX9IMxUlJSJCYmhvUEgCmja5Rq1aqp5vZ33UuHBKUqWrSoatbv8dYA6ptvvlm1O+64Q7V33nlHtTfffFO15557TrX4+HjV8uXLp5pb1vexdu1a1azrPmud+e2331SLjIxUzbqWsYbPWT8/a8jpf//7X9VEGEAKQMtoPbEGjVrnpQoVKqh2ww03qPbrr7+qZq0xn3/+uWoutu5ERGT16tWqWddLCxYsUK1du3au3sMta3BpRESEagMGDFDN7XDU7LD2Ja39y/TDUVNSUuTo0aO5dj3hE8EAAAAAAAAA4HFsBAMAAAAAAACAx7ERDAAAAAAAAAAex0YwAAAAAAAAAHhcpobFFShQwGeAwtmzZ9Vj69Wrp9quXbtUO3/+vGrWIJ+goCDVMrrxvxtDhgxRbeTIka7e46233lJt8uTJqn377beqvfLKK6qdOHFCNWsYj6Vly5aqWTfT/uGHH1y93p133qnauXPnVLOGDzGIAYBbGQ1iSD90UkRk27Ztqr388suqWcPdLNZgM2sYgLUmvPbaa6pduHBBtSJFirg6lvRD0UREPvnkE1fPtYbpPPDAA6p99dVXqg0bNky1NWvWqPb111+rVr58edX27t2rWocOHVQ7dOiQatawjIywngCwZLSmZMeTTz6pmnV+toZcDxw4ULX9+/erln7ojIjIu+++q9qIESNcPXfevHmqWcOhrfO9dR63hIeHq3bbbbepZg3wvnQ4bCprQPaoUaNUGzRokGqhoaGqxcXFqZYR1hQA6aWuJyVKlJA8ef73eUlrjbGuKazho//4xz+yfDwVK1ZUrV+/fqpZe23jxo1TrUuXLqpNmzZNNevayFpPjh49qpr1/T799NOujs9aY6x9tdatW6u2dOlS1ay9MWuNGT9+vGpt27ZVzdr3E8m96wmfCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOMyNSyuYcOGPjf7dzuIzDJx4kTVevfurVpERIRqNWrUUO27775TzbqptTUArXPnzqo1atRItV9++UW1rVu3qrZ69WrVLNZwn1mzZrl6rlvVq1d39bjffvvN1eMCAwNVs27ELZJ7b5wN4PLJzGCfatWqqWadc63hDNbQNmuQQJUqVVRr06aNap999plq1hpjDWxo166dauvWrVPt0sEUqQ4ePKhaTuvbt69q1uAEt1588UXV5s+fr9qmTZtcvybrCQBL6ppy1113SUBAQFq3hlEOGDBAteHDh6tmDcO2NG/eXDVrnenRo4dqH374oav3sAbI5c2bVzVrkKo1UM3tYDiLNcA7ISFBtbJly6pmrW/Wsbgd6m3p1KmTaiVLlvT5+uLFizJ58mTWFADK5Rg+mtNq166tmtuBpNY6cd9996m2ePFi1e68807VSpcurZo1HLp48eKqHTt2TDWLtSdnvUeFChVUswaSLl++XDVrGLa1zu7evds8xty6nvCJYAAAAAAAAADwODaCAQAAAAAAAMDj2AgGAAAAAAAAAI9jIxgAAAAAAAAAPM7/rx/yPz/99JPP14899ph6zJEjR1RbuHChatZgOEt0dLRq1iCfSwdEpLIGwz3//POq9e/fX7X33ntPNesG29YAA2vIhDWMze1guHr16qm2Zs0a1Vq3bq3a9u3bVXv77bdVe+GFF1S7cOGCavv27cvwOAEgJz3yyCOqDR06VDXr5v0bN2509R7WOTI4OFi1w4cPu3o9y4IFC1Tr0KGDasuWLXP1eo0bN1Zt5cqVrp5bt25d1RYtWuTquXPmzFHNGqK6YsUK1azBcNbfb/rHJScnux5mCuD6lZSUJH5+fmlfW9cP1rC4UaNGqTZ48GBX72kNhrNYw0utoWjWOfbpp59WzbpGOXHihGrW9YPbYXHWOmMNL7VezxoMZF1TWNwOhrOcOnVKtfTXpUlJSVl+fQDXh7///e8SGBiY9nX6PTARkZMnT6oWExOj2ujRo109zjo3vfPOO6pZ1zeO46h25swZ1awha9ZQNIs1mNvttZbbwXCWH3/80dXj0g8GFXG/7vj76y1Ra/j3M8884/N1QkKCTJgwwdV7XIv4RDAAAAAAAAAAeBwbwQAAAAAAAADgcWwEAwAAAAAAAIDHsREMAAAAAAAAAB7n51h3l07nzJkzEhYWluU3KVSokGrWDbGtIWvt27dX7dJhEKnmz5+vWseOHVWzbjhtDbaxBu9UqFBBtenTp6tWsGBB1azhbtaNwq0bU9evX1+11atXq2Zp1aqValu2bFGtbNmyrt6jRIkSqqX/+01OTpZdu3ZJbGys+bMAcP3K7npiPdca7jZu3DjVHnzwQdUqV66smjVo1Dpf16xZU7V169apZg0j2rFjh2r//e9/VbMGVISEhKgWFxenmqVZs2aquR1SZ5kxY4Zqffr0US02Nla1yZMnq5Z+EIPjOHL+/HnWEwCm1DXliSee8BnuY51Pv/vuO9WsNcU6X1nKly+vmjVU2cWlloiItGzZUjW3wzzdXis88MADqlnDq63rh++//97VsfzjH/9QzVqTreFD1gCmAwcOqPbxxx+r9uijj7o6PhFhTQGgZHSNYu09Wdce99xzj2rW+dXaB8vOfk+NGjVUs/Z7rMHNw4YNU+3LL79Uzfp+69Spo5p1vWStx5YCBQqo1rlzZ9Ws780aEJsnj/7Mq/Wzsp67du3aDI8zvdy6nvCJYAAAAAAAAADwODaCAQAAAAAAAMDj2AgGAAAAAAAAAI9jIxgAAAAAAAAAPC7Hh8UVK1ZMtbx586p25MgRV69XpkwZ1ayhARZr0MGbb76pmjWswBqIcOedd7p6X8ulAyxSWTcKtwbczZs3T7VGjRqpduONN7p6j7lz56r21FNPqWb9rFq3bq3aL7/84vN1SkqKnDp1KtfeOBvA5ZO6njz44IM+50Vr8GbJkiVVs278f/z4cdUaNmyoWkJCgmpbt241j9GNDh06qGYNLs2OunXrqmYNVrWGM1jNLbcDlLp166baF198oZr1d2kNzOvUqZPP14mJifLFF1+wngAwpa4pJUqU8FkfDh8+rB5bqlQp1azHdenSRbVvv/1WtdDQUNWs6xZrIKd1jWIdizUUxzoXWoN8AgICVLOGPlsDf6ZMmaKadcloDVGyrhXc/vzcDj51q1KlSj5fp6SkyJ49e1hTACiZWU/ceuKJJ1SzBi1bg6obN25sHmN61uDSDRs2qFaoUCHVrAFy1uvt3btXNbfcnmv9/f1VswaIPv3006q9++67qjVv3ly1zZs3q2YNGv3nP/+ZwVFquXU94RPBAAAAAAAAAOBxbAQDAAAAAAAAgMexEQwAAAAAAAAAHsdGMAAAAAAAAAB4XI4Pi8tp1k2y169f7+q51rdmDU+7ePGiagUKFFDNGgCxfft2V8diDUSoWLGiajt37lTt9ttvV61s2bKqTZ06VTVrSJ31/d50002qvfrqq6r9/e9/Vy05OVk1kdx742wAl09G60l4eLhq1sCed955J8vvbQ0sczu41GINgJg8eXKWX8/y+OOPq2YNAPryyy9Vu+GGG1Rbvny5ar1791bNGqbw448/ujoWa42pX7++q/d96623fL5OTk6Wbdu2sZ4AMF3NaxS3rHPdxIkTL/v71qtXT7U1a9aoZl1rrVy5UjVraHa7du1UW7t2rWpBQUGqWddz1nPdGjBggGrvvfeez9eO40hCQgJrCgAldT3p1KmTz++3n376qavnFylSRDXrd2Jr6PPMmTNVi4qKcvW+TZs2Vc0a2jl8+HDVEhMTXb3Hww8/rNq8efNUi4+PV61y5cqq7dq1SzW3a5HbYXbWNYrb79dSvXp1n6+Tk5Nl+/btuXY94RPBAAAAAAAAAOBxbAQDAAAAAAAAgMexEQwAAAAAAAAAHufv5kEubiN82Vj39HV7PNbjsvPcjO6Fm5Ovl537Grt9X4t1LOfPn8/y62X2sQCuDxmdF1JSUlRLSEjI0fe23iM73J6Hc/o9rJ+h9b1Za4fb93C79ro9z1uvd+HCBdXSr0WpX7OeALDkhnPDlVgrLG6vW9yuFW7P49b36+fnp1p2rqss1u8M6f99pH6dG/7dALiyUs8LWb2PbHZ+T7b2Xdyyzs3WvXqzc96zfiZuX8/t9Zfbtcjt6+X0ed5r1yiuhsUdPHjQHJQG/JkDBw5IZGTk1T4MANcQ1hNkBesJAAtrCrKCNQVAeqwnyIrcup642ghOSUmR6OhoCQ0NNf+LLnApx3EkLi5OIiIiJE8e7j4C4H9YT5AZrCcA/gxrCjKDNQVARlhPkBm5fT1xtREMAAAAAAAAAMi9ct/WNQAAAAAAAAAgU9gIBgAAAAAAAACPYyMYAAAAAAAAADyOjWAAAAAAAAB4QrNmzaRZs2ZX+zDw/+3du1f8/Pzko48+yrHX/Oijj8TPz0/27t37p497+eWXGQCYDhvBAAAAAAAAuGJSN/KCg4Pl0KFD6s+bNWsmNWrUuApHdu1J/Vml/k9wcLBERERI69at5V//+pfExcVd7UNELsJGMAAAAAAAAK64hIQEGTVqVI6+5nfffSffffddjr7mteDVV1+VadOmycSJE+Uf//iHiIg8/fTTUrNmTdm0adNVPrpr04svvigXLly42odxTfG/2gcAAAAAAACA60/t2rXlgw8+kCFDhkhERESOvGZgYGCOvM61pk2bNlK3bt20r4cMGSJLliyRdu3aSYcOHWTbtm2SL1++q3iE1x5/f3/x92fr81J8IhgAAAAAAABX3NChQyU5OdnVp4KTkpJkxIgRUqlSJQkKCpLy5cvL0KFDJSEhwedx1j2Cx40bJ9WrV5f8+fNL4cKFpW7duvLJJ5+IiMjSpUvFz89P5s2bp97zk08+ET8/P/n555/NY1q7dq34+fnJxx9/rP7s22+/FT8/P1mwYIGIiMTFxcnTTz8t5cuXl6CgIClevLi0atVK1q1b95ffe0ZatGghw4YNk3379sn06dN9/mz79u3SqVMnKVKkiAQHB0vdunVl/vz56jVOnz4tzzzzTNpxRUZGSteuXSUmJibtMceOHZMePXpIiRIlJDg4WGrVqmV+z6dPn5Zu3bpJWFiYFCpUSB599FE5ffq0eexuj++3336TFi1aSL58+SQyMlJee+01SUlJcfXzse4R7OfnJ3379pVZs2ZJtWrVJF++fHL77bfL5s2bRURk0qRJUrlyZQkODpZmzZqp+xCvWLFCHnjgASlbtqwEBQVJmTJl5JlnnjE/eZz6HsHBwVKjRg2ZN2+edOvWTcqXL+/zuJSUFHn33XelevXqEhwcLCVKlJBevXrJqVOnfB63du1aad26tYSHh0u+fPmkQoUK0r17d1c/i1RsiwMAAAAAAOCKq1ChgnTt2lU++OADGTx48J9+Kvjxxx+Xjz/+WDp16iQDBgyQ1atXy8iRI2Xbtm3mJm6qDz74QPr16yedOnWS/v37S3x8vGzatElWr14tf//736VZs2ZSpkwZmTFjhtx7770+z50xY4ZUqlRJbr/9dvO169atKxUrVpTPP/9cHn30UZ8/mzlzphQuXFhat24tIiJPPvmkzJ49W/r27SvVqlWTEydOyMqVK2Xbtm1Sp04dtz8ypUuXLjJ06FD57rvvpGfPniLyx+Zpo0aNpHTp0jJ48GApUKCAfP7553LPPffInDlz0r7Ps2fPyt/+9jfZtm2bdO/eXerUqSMxMTEyf/58OXjwoISHh8uFCxekWbNmsmvXLunbt69UqFBBZs2aJd26dZPTp09L//79RUTEcRzp2LGjrFy5Up588kmpWrWqzJs3T/1cMnN8R44ckebNm0tSUlLa4yZPnpztTz6vWLFC5s+fL0899ZSIiIwcOVLatWsngwYNkgkTJkifPn3k1KlTMmbMGOnevbssWbIk7bmzZs2S8+fPS+/evaVo0aKyZs0aGTdunBw8eFBmzZqV9rivv/5aoqKipGbNmjJy5Eg5deqU9OjRQ0qXLq2Op1evXvLRRx/JY489Jv369ZM9e/bI+PHjZf369fLjjz9KQECAHDt2TO68804pVqyYDB48WAoVKiR79+6VuXPnZu6bdwAAAAAAAIArZOrUqY6IOL/88ouze/dux9/f3+nXr1/anzdt2tSpXr162tcbNmxwRMR5/PHHfV7nueeec0TEWbJkic9zmzZtmvZ1x44dfV7LMmTIECcoKMg5ffp0Wjt27Jjj7+/vDB8+/C+fGxAQ4Jw8eTKtJSQkOIUKFXK6d++e1sLCwpynnnrqT1/LcunPKiNhYWHOLbfckvb1HXfc4dSsWdOJj49PaykpKU7Dhg2dG264Ia299NJLjog4c+fOVa+ZkpLiOI7jvPvuu46IONOnT0/7s4sXLzq33367ExIS4pw5c8ZxHMf54osvHBFxxowZk/a4pKQk529/+5sjIs7UqVMzfXxPP/20IyLO6tWr09qxY8ecsLAwR0ScPXv2ZPgzcRzHGT58uJN+61NEnKCgIJ/nTpo0yRERp2TJkmnfj+P88Xeb/n3Onz+v3mfkyJGOn5+fs2/fvrRWs2ZNJzIy0omLi0try5Ytc0TEKVeuXFpbsWKFIyLOjBkzfF7zP//5j0+fN2/eX/47cINbQwAAAAAAAOCqqFixonTp0kUmT54shw8fNh/zzTffiIjIs88+69MHDBggIn98+jIjhQoVkoMHD8ovv/yS4WO6du0qCQkJMnv27LQ2c+ZMSUpKkkceeeRPjz8qKkoSExN9Ppn53XffyenTpyUqKsrnOFavXi3R0dF/+npZERISInFxcSIicvLkSVmyZIk8+OCDEhcXJzExMRITEyMnTpyQ1q1by86dO+XQoUMiIjJnzhypVauW+iS0iKTdUuGbb76RkiVLysMPP5z2ZwEBAdKvXz85e/asLF++PO1x/v7+0rt377TH5c2bN22wXarMHN8333wjDRo0kHr16qU9v1ixYtK5c+ds/bzuuOMOn9sz1K9fX0RE7r//fgkNDVX9999/T2uXfhr53LlzEhMTIw0bNhTHcWT9+vUiIhIdHS2bN2+Wrl27SkhISNrjmzZtKjVr1vQ5llmzZklYWJi0atUq7WcRExMjt956q4SEhMjSpUtF5I9/PyIiCxYskMTExCx/72wEAwAAAAAA4Kp58cUXJSkpKcN7Be/bt0/y5MkjlStX9uklS5aUQoUKyb59+zJ87eeff15CQkKkXr16csMNN8hTTz0lP/74o89jqlSpIrfddpvMmDEjrc2YMUMaNGig3jO9WrVqSZUqVWTmzJlpbebMmRIeHi4tWrRIa2PGjJEtW7ZImTJlpF69evLyyy/7bDBmx9mzZ9M2MHft2iWO48iwYcOkWLFiPv8zfPhwEfnjnr8iIrt375YaNWr86Wvv27dPbrjhBsmTx3cLsWrVqml/nvr/lipVymfjU0Tkpptu8vk6M8eX+t7ppX/NzCpbtqzP12FhYSIiUqZMGbNfeq/e/fv3S7du3aRIkSISEhIixYoVk6ZNm4qISGxsbNpxi4j5byd927lzp8TGxkrx4sXVz+Ps2bNpP4umTZvK/fffL6+88oqEh4dLx44dZerUqeoe2X+FewQDAAAAAADgqqlYsaI88sgjMnnyZBk8eHCGj0s/+MuNqlWryo4dO2TBggXyn//8R+bMmSMTJkyQl156SV555ZW0x3Xt2lX69+8vBw8elISEBFm1apWMHz/e1XtERUXJ66+/LjExMRIaGirz58+Xhx9+WPz9/7ft9uCDD8rf/vY3mTdvnnz33Xfy5ptvyujRo2Xu3LnSpk2bTH9fqQ4ePCixsbFpG4ypg9See+65tPsTp/dXm9uX07VwfHnz5s1UdxxHRESSk5OlVatWcvLkSXn++eelSpUqUqBAATl06JB069bN9RC7S6WkpEjx4sV9/iPEpYoVKyYif/zbnz17tqxatUq++uor+fbbb6V79+7yz3/+U1atWqU24DPCRjAAAAAAAACuqhdffFGmT58uo0ePVn9Wrlw5SUlJkZ07d6Z9ElVE5OjRo3L69GkpV67cn752gQIFJCoqSqKiouTixYty3333yeuvvy5DhgyR4OBgERF56KGH5Nlnn5VPP/1ULly4IAEBAT63dvgzUVFR8sorr8icOXOkRIkScubMGXnooYfU40qVKiV9+vSRPn36yLFjx6ROnTry+uuvZ2sjeNq0aSIiaZuqFStWFJE/bt/QsmXLP31upUqVZMuWLX/6mHLlysmmTZskJSXF51PB27dvT/vz1P938eLFcvbsWZ9NyR07dvi8XmaOr1y5crJz507V07/mlbJ582b573//Kx9//LF07do1rX///fc+j0v9mezatUu9RvpWqVIlWbRokTRq1MjVELwGDRpIgwYN5PXXX5dPPvlEOnfuLJ999pk8/vjjrr4Hbg0BAAAAAACAq6pSpUryyCOPyKRJk+TIkSM+f9a2bVsREXn33Xd9+ttvvy0iInfffXeGr3vixAmfrwMDA6VatWriOI7PvVbDw8OlTZs2Mn36dJkxY4bcddddEh4e7urYq1atKjVr1pSZM2fKzJkzpVSpUtKkSZO0P09OTk67bUCq4sWLS0RERKb/T/svtWTJEhkxYoRUqFAh7b65xYsXl2bNmsmkSZPMey4fP3487X+///77ZePGjTJv3jz1uNRPwbZt21aOHDnic+uLpKQkGTdunISEhKTdFqFt27aSlJQkEydO9Pm+x40bp75vt8fXtm1bWbVqlaxZs8bnzzP69OzllvqJ4dSfTer/PnbsWJ/HRURESI0aNeTf//63nD17Nq0vX75cNm/e7PPYBx98UJKTk2XEiBHq/ZKSkuT06dMi8sftKS59XxGR2rVri4hk6t8QnwgGAAAAAADAVffCCy/ItGnTZMeOHVK9evW0XqtWLXn00Udl8uTJcvr0aWnatKmsWbNGPv74Y7nnnnukefPmGb7mnXfeKSVLlpRGjRpJiRIlZNu2bTJ+/Hi5++67fQaDifxxe4hOnTqJiJgbc38mKipKXnrpJQkODpYePXr4fHo2Li5OIiMjpVOnTlKrVi0JCQmRRYsWyS+//CL//Oc/Xb3+woULZfv27ZKUlCRHjx6VJUuWyPfffy/lypWT+fPnp32yWUTkvffek8aNG0vNmjWlZ8+eUrFiRTl69Kj8/PPPcvDgQdm4caOIiAwcOFBmz54tDzzwgHTv3l1uvfVWOXnypMyfP1/ef/99qVWrljzxxBMyadIk6datm/z6669Svnx5mT17tvz444/y7rvvpv0M27dvL40aNZLBgwfL3r17pVq1ajJ37ly1AZ6Z4xs0aJBMmzZN7rrrLunfv78UKFBAJk+enPYp5SutSpUqUqlSJXnuuefk0KFDUrBgQZkzZ47PPYRTvfHGG9KxY0dp1KiRPPbYY3Lq1CkZP3681KhRw2dzuGnTptKrVy8ZOXKkbNiwQe68804JCAiQnTt3yqxZs2Ts2LHSqVMn+fjjj2XChAly7733SqVKlSQuLk4++OADKViwYNp/KHHFAQAAAAAAAK6QqVOnOiLi/PLLL+rPHn30UUdEnOrVq/v0xMRE55VXXnEqVKjgBAQEOGXKlHGGDBnixMfH+zyuadOmTtOmTdO+njRpktOkSROnaNGiTlBQkFOpUiVn4MCBTmxsrHrvhIQEp3Dhwk5YWJhz4cKFTH1PO3fudETEERFn5cqV6nUHDhzo1KpVywkNDXUKFCjg1KpVy5kwYcJfvm7qzyr1fwIDA52SJUs6rVq1csaOHeucOXPGfN7u3budrl27OiVLlnQCAgKc0qVLO+3atXNmz57t87gTJ044ffv2dUqXLu0EBgY6kZGRzqOPPurExMSkPebo0aPOY4895oSHhzuBgYFOzZo1nalTp6r3PHHihNOlSxenYMGCTlhYmNOlSxdn/fr1joiox7s9vk2bNjlNmzZ1goODndKlSzsjRoxwPvzwQ0dEnD179vzpz2748OFO+q1PEXGeeuopn7Znzx5HRJw333zTpy9dutQREWfWrFlpbevWrU7Lli2dkJAQJzw83OnZs6ezceNG83v87LPPnCpVqjhBQUFOjRo1nPnz5zv333+/U6VKFXWskydPdm699VYnX758TmhoqFOzZk1n0KBBTnR0tOM4jrNu3Trn4YcfdsqWLesEBQU5xYsXd9q1a+esXbv2T38G6fn9/x8CAAAAAAAAcN1KSkqSiIgIad++vXz44YdX+3DgQbVr15ZixYqp+wpfKdwjGAAAAAAAANe9L774Qo4fP+4zCAzIisTERElKSvJpy5Ytk40bN0qzZs2uzkGJCJ8IBgAAAAAAwHVr9erVsmnTJhkxYoSEh4fLunXrrvYhIZfbu3evtGzZUh555BGJiIiQ7du3y/vvvy9hYWGyZcsWKVq06FU5LobFAQAAAAAA4Lo1ceJEmT59utSuXVs++uijq3048IDChQvLrbfeKlOmTJHjx49LgQIF5O6775ZRo0ZdtU1gET4RDAAAAAAAAACexz2CAQAAAAAAAMDj2AgGAAAAAAAAAI9zdY/glJQUiY6OltDQUPHz87vcx4RcznEciYuLk4iICMmTh//WAOB/WE+QGawnAP4MawoygzUFQEZYT5AZuX09cbURHB0dLWXKlLncxwKPOXDggERGRl7twwBwDWE9QVawngCwsKYgK1hTAKTHeoKsyK3riaut69DQ0Mt9HPAg/t0ASI/zArKCfzcALJwbkBX8uwGQHucFZEVu/XfjaiOYj8YjK/h3AyA9zgvICv7dALBwbkBW8O8GQHqcF5AVufXfTe67mQUAAAAAAAAAIFPYCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOPYCAYAAAAAAAAAj2MjGAAAAAAAAAA8zv9qHwAAALlRnjz6v6X6+fmp5jiOaikpKZflmHKK2+8DAAAAAJB78IlgAAAAAAAAAPA4NoIBAAAAAAAAwOPYCAYAAAAAAAAAj2MjGAAAAAAAAAA8jmFxAABcIjAwULUGDRqoNmrUKNUqVKig2vHjx1V7/vnnVVu2bJlqFy9eVM0aNJedQW558+Z19R4AAAAAgNyNTwQDAAAAAAAAgMexEQwAAAAAAAAAHsdGMAAAAAAAAAB4HBvBAAAAAAAAAOBxDIsDAFy3/Pz8VAsICFCtTJkyqiUkJKh25swZ1WJjY1WLiIhQrXDhwqpZg+ZyepBbcnJyjr4eAAAAAODaxCeCAQAAAAAAAMDj2AgGAAAAAAAAAI9jIxgAAAAAAAAAPI6NYAAAAAAAAADwOIbFAQCuW3ny6P8eGhISolp0dLRqY8eOVc0a7rZ161bV4uLiVHM7tM1xHFePAwAAAADgUnwiGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOPYCAYAAAAAAAAAj8uVw+L8/PxcNX9//e1Zw3jcDugBAHiLNSzu4sWLqlkD32JjY1VLSEhQLTvD3azjCwwMVM06ZobKAQAAALD2y6xrCutxiYmJqrGHlrvxiWAAAAAAAAAA8Dg2ggEAAAAAAADA49gIBgAAAAAAAACPYyMYAAAAAAAAADzuqg2LczvwzWrW8JwCBQqoVqhQIdXi4+NVO3XqlGrZGbxjHV9QUJBqefPmdXV81o24GQIEAJljrSeWpKQk1c6fP6+aNRguO6w1YdiwYap17NjR1eMWLFiQMwcGAPAEazDQjTfeqNrp06dVO3TokGpcjwDAtcfaj7rttttUe/bZZ1ULCQlRbeHChapt27ZNNWuQ9pEjR1Q7fPiwagyfu7L4RDAAAAAAAAAAeBwbwQAAAAAAAADgcWwEAwAAAAAAAIDHsREMAAAAAAAAAB531YbFuR0u4PZx/v76WylZsqRq+fLlU+348eOqxcXFqZaSkqJaw4YNVXvttddUi4iIUG3NmjWqDRw4ULWtW7eqZg0p4gbbAJAxaz2xzpvWsFCr5bTBgwer9tJLL6lmDb0bNGiQatZgB9YJALg2uR2a7VZ4eLhq06ZNU61x48aqjR8/XrUhQ4aoxrA4ALi6rHWiS5cuqk2cOFG1oKAg1RITE1WrU6eOavHx8aoFBwerduHCBdU++ugj1caMGePqPZAz+EQwAAAAAAAAAHgcG8EAAAAAAAAA4HFsBAMAAAAAAACAx7ERDAAAAAAAAAAed9WGxVncDhywht2cPXtWtZMnT6pWrlw51apUqaJabGysatawuNq1a6tm3SR7//79qo0dO1a16Oho1fLk0fv1efPmdXV8DHEAgIxZ501rMFxOn0uLFCmiWvfu3VWzBkBYQxeGDx+uGoPhAODaZJ3bAwICVLN+37fO7dbrVa5cWbW6deuqlj9/ftXWrVunGtcUAHDtqVatmmrvv/++atYelcVaYw4fPqyadS1jDZ87d+6capUqVVItNDRUtYSEBNVYi3IGnwgGAAAAAAAAAI9jIxgAAAAAAAAAPI6NYAAAAAAAAADwODaCAQAAAAAAAMDjrqlhcdlh3Ujauqm19TirWcPirBtn//jjj6q99dZbqlnDfayBRNZQCOtG3NZAiYMHD6rGzbQBIHNy+rxpDfEJDw9XzVpPYmJiVBswYIBqP//8cxaPDgBwpVmDoK2WmJiomttBoLt27VJt48aNqiUlJam2ePFi1bimAICry1onPvnkE9XcDoY7f/68aoMHD1Zt+fLlqtWsWVO1ggULqrZ3717Vtm3bppq1X2btjVlrFjKPTwQDAAAAAAAAgMexEQwAAAAAAAAAHsdGMAAAAAAAAAB4HBvBAAAAAAAAAOBxnhkWZw0wOHfunGrWDbFTUlJcvZ5b8fHxrh5nDRCKjIxULSoqSrX169erdujQIVfvCwDIHOt8bbHWDmuwg7U+ffrpp6pZwxQOHDigmtvhQQCAq8+69rCGV2fnesQaNLdmzRrVfvnlF9Xi4uKy/L4AgMujaNGiqlWsWNHVc609qvvuu0+1JUuWqGatRdu3b3f1OOs6yBoCFxAQoFq+fPlUs/bzuA7KPD4RDAAAAAAAAAAex0YwAAAAAAAAAHgcG8EAAAAAAAAA4HFsBAMAAAAAAACAx3lmWJzFGsRwLQkLC1Nt3LhxqlWtWlW1Pn36qJadgRIAgD9Yg+H8/fVyaZ1zreEHhQoVUq1cuXKqHT161FWz1jZrEIPlWl8XAeB6kNO/s1trT4MGDVQrX768arNnz1bNWgetZn0f2RmuCgDIWPHixVWzhsBZv++PGTNGtaVLl6qWlJTk6ljcPs7tsLgyZcqoVrBgQdV27typWkxMjGqsMX+OTwQDAAAAAAAAgMexEQwAAAAAAAAAHsdGMAAAAAAAAAB4HBvBAAAAAAAAAOBxnh4Wdy2xboj9yCOPqHbXXXep9ttvv6m2bt061RgCBACXR2hoqGrVqlVTrUWLFqpFRkaqduHCBdW++eYb1TZv3uzq+Kw1Jjg4WLVz586pxtoBALmHdb6//fbbVZsyZYpq+/fvV80aNGSxBv6wfgDAlWOdrxcvXqza4cOHVZswYYJqiYmJquX0kDXr9YKCglSzBpxa6531Mzh58qRqycnJbg/xusQnggEAAAAAAADA49gIBgAAAAAAAACPYyMYAAAAAAAAADyOjWAAAAAAAAAA8DhPD4vz8/NTzd9ff8vW8IOkpCTV3A5EsN63cOHCqt13332q7dq1S7Xu3burduLECVfHAgDIvsqVK6s2fvx41SpUqKCaNaBt5cqVqu3Zs0c1ay2yhi5Y7UoMgAAA5Azr+iEgIEC1UqVKqTZs2DDVihQpotqSJUtUO3bsmGpur4NYUwDg8rDWBGsP6LXXXlPt4MGDqsXGxqp2Jc7h1nskJCSoZg0zLVasmGrnz5939R74c3wiGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOPYCAYAAAAAAAAAj/P0sDhrwEJ4eLhq1s2qT58+7eo9rJt458+fX7W7775bNWuA0NNPP63ahg0bVOOG2ABw5ZQuXVq1okWLqmYN2Pn1119VGzNmjGq7d+9Wze25Pjk52VVj7QCA3MMa0HbmzBnV5s+fr9rRo0dVmzp1qmpnz5519b6sHwBw5VjnXOv8v337dtWs65FribWHZn1vefPmVe3UqVOqsT5lHp8IBgAAAAAAAACPYyMYAAAAAAAAADyOjWAAAAAAAAAA8Dg2ggEAAAAAAADA4zw9LM66CbU1oM0akmAN2bFYN7Bu1aqVakOGDFHt+++/V+2HH35QzRrYAAC4PNwOMFi0aJFqX3/9tWoLFy5U7fz586rl9KADBicAQO5hnbOtgT/WoBxrCNxXX32l2sWLF7N4dACAq83aF7qW9oqsa6iAgADVihUrplqVKlVU27t3r2rW3h3XPJnHJ4IBAAAAAAAAwOPYCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPA4Tw+LS0xMVM0akpCdm0uHhISoNnr0aNVuuOEG1ebOnauadcwAgCunQIECqoWHh6s2btw41TZu3Kia2+GjFmvoQlBQkGrWQCGrXQnWMbvFsAcAyLz4+HjVrKFywcHBqgUGBqpmrVvWNcq1NKQIALzO7e/YV+L36bx586pWpkwZ1fr27ata69atVbPWrOHDh6tmrXfIPD4RDAAAAAAAAAAex0YwAAAAAAAAAHgcG8EAAAAAAAAA4HFsBAMAAAAAAACAx3l6WFxODzDIk0fvm7dv3161SpUqqXb27FnVNmzYoBqDcgDgyrHO67Vq1VLt1ltvVW316tWqZWfdsQZAlCpVSrUaNWqoZq0nMTExql2JddEaHuH2uQkJCdk+JgC43ljDq4sVK6aadZ1x7tw51az1yHqcNYQbAHB5WOdmq2VnULX1+3n+/PlV69Kli2ojRoxQrVChQqpZa9HWrVtVy+kh3PgfPhEMAAAAAAAAAB7HRjAAAAAAAAAAeBwbwQAAAAAAAADgcWwEAwAAAAAAAIDHeXpYXHZYN92uW7euak899ZRq0dHRqk2aNEm1JUuWqJbTg3wAABkLDAxU7d5771WtZs2aqjVq1Ei1Y8eOqWYNQMuXL59qzZs3V613796qnT59WrWZM2eq9sMPP6hmDfux1h1/f/3rgfWzCg4OVi0oKEi1CxcuqFayZEnVtmzZohoA4H+qVaumWv/+/VVbtGiRasuXL1ctLi5ONWuQj7VWWNdLDL4GgMsjICBANet3cbfn5sKFC6vWpEkT1R544AHVWrZsqZp1DWAdi3VdMGjQINVOnjypGnIGnwgGAAAAAAAAAI9jIxgAAAAAAAAAPI6NYAAAAAAAAADwODaCAQAAAAAAAMDjGBYn9g2sK1asqNrgwYNVswbv9OnTR7XvvvtONWuAEADgyrHO/9bgnDJlyqj2wgsvqNarVy/Vzp49q1pkZKRqJUqUUC05OVm1TZs2qWatWUlJSaodPXpUtYsXL6pWsGBB1awBENYaeObMGdXCwsJU+9vf/qYaw+IA4H8aNGig2pw5c1SzzrvvvPOOajExMaoxqBoAcgfruqVu3bqqdenSRbX69eurZv1+HhISopp1DWANlrauW6Kjo1Vr06aNalu3blUNlw+fCAYAAAAAAAAAj2MjGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOMYFicigYGBqlmDgebPn6/a4sWLVTt48KBqjuNk8egAAJdLfHy8av/+979Vq1q1qmrW0IUaNWqoZg1jS0xMVO3EiROq7dixQ7U1a9aoduTIEdWCg4NVK1WqlGrnz59XLX/+/Krt3LlTNWvwkKV8+fKqzZo1y9VzAeB6YA3jefTRR1U7cOCAav3791fNWj+4HgGA3Msa7lmnTh3VmjVrplrx4sVVs4a7WdctsbGxqu3atUu1119/XbWlS5e6el9cWXwiGAAAAAAAAAA8jo1gAAAAAAAAAPA4NoIBAAAAAAAAwOPYCAYAAAAAAAAAj/NzXEwNOHPmjISFhV2J47kq8ubNq1q+fPlUs26cbTX8ITY2VgoWLHi1DwPANSS3rid58uj/bmqtEwEBAar5+fmpZg17sAYnuB2mYK1jVrOOzxpcZw3Rs47Zahbr52e9b0ZYTwBYcuuaYrHO2YUKFVLt3LlzqlnnbGSMNQVAerl1PbGOOSoqSrUqVaqotm7dOtVWrFih2uHDh1Wzfo+/HgeS5tb1hE8EAwAAAAAAAIDHsREMAAAAAAAAAB7HRjAAAAAAAAAAeBwbwQAAAAAAAADgcf5X+wCuBdYwHmsQw/V482sAgD0UzVonYHM79A4ArlfWefLEiRNX4UgAALlFbGysapMnT74KR4LchE8EAwAAAAAAAIDHsREMAAAAAAAAAB7HRjAAAAAAAAAAeBwbwQAAAAAAAADgcQyLywCD4QAAAAAAAAB4BZ8IBgAAAAAAAACPYyMYAAAAAAAAADyOjWAAAAAAAAAA8Dg2ggEAAAAAAADA4667YXF+fn6qFS1aVLWUlBTVTp48eVmOCQAAAAAAAAAuJz4RDAAAAAAAAAAex0YwAAAAAAAAAHgcG8EAAAAAAAAA4HFsBAMAAAAAAACAx3l6WJw1GK506dKqbd68WbWxY8eq9tprr6nmOI5qycnJbg8xy/z99V9dUlLSZX9fAAAAAAAAALkPnwgGAAAAAAAAAI9jIxgAAAAAAAAAPI6NYAAAAAAAAADwODaCAQAAAAAAAMDjPD0szhrkdvDgQdUKFy58JQ4nRzEYDgAAAAAAAIBbfCIYAAAAAAAAADyOjWAAAAAAAAAA8Dg2ggEAAAAAAADA41xtBFv32gX+Cv9uAKTHeQFZwb8bABbODcgK/t0ASI/zArIit/67cbURHBcXd7mPAx7EvxsA6XFeQFbw7waAhXMDsoJ/NwDS47yArMit/278HBdb2CkpKRIdHS2hoaHi5+d3JY4LuZjjOBIXFycRERGSJw93HwHwP6wnyAzWEwB/hjUFmcGaAiAjrCfIjNy+nrjaCAYAAAAAAAAA5F65b+saAAAAAAAAAJApbAQDAAAAAAAAgMexEQwAAAAAAAAAHsdGMAAAAAAAAAB4HBvBAAAAAAAAAOBxbAQDAAAAAAAAgMexEQwAAAAAAAAAHvf/AGyENL5m77g6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nAmazing! The noise is gone now! You could get a better reconstruction by using a convolutional autoencoder. I hope this new model opened up your mind to the many possible architectures and non-classical ML problems that neural networks can solve :)\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "De-noising like an autoencoder\n",
    "\n",
    "Okay, you have just built an autoencoder model. Let's see how it handles a more challenging task.\n",
    "\n",
    "First, you will build a model that encodes images, and you will check how different digits are represented with show_encodings(). To build the encoder you will make use of your autoencoder, that has already being trained. You will just use the first half of the network, which contains the input and the bottleneck output. That way, you will obtain a 32 number output which represents the encoded version of the input image.\n",
    "\n",
    "Then, you will apply your autoencoder to noisy images from MNIST, it should be able to clean the noisy artifacts.\n",
    "\n",
    "X_test_noise is loaded in your workspace. The digits in this noisy dataset look like this:\n",
    "\n",
    "Apply the power of the autoencoder!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Build an encoder model with the first layer of your trained autoencoder model.\n",
    "    Predict on X_test_noise with your encoder and show the results with show_encodings().\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict(X_test_noise)\n",
    "#show_encodings(encodings, number = 1)\n",
    "\n",
    "# Predict on the noisy images with your autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# Plot noisy vs decoded images\n",
    "compare_plot(X_test_noise, decoded_imgs)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Amazing! The noise is gone now! You could get a better reconstruction by using a convolutional autoencoder. I hope this new model opened up your mind to the many possible architectures and non-classical ML problems that neural networks can solve :)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 16)        4624      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                92170     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,114\n",
      "Trainable params: 97,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! You can see that the key concepts are the same, you just have to use new layers!\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Building a CNN model\n",
    "\n",
    "Building a CNN model in Keras isn't much more difficult than building any of the models you've already built throughout the course! You just need to make use of convolutional layers.\n",
    "\n",
    "You're going to build a shallow convolutional model that classifies the MNIST digits dataset. The same one you de-noised with your autoencoder! The images are 28 x 28 pixels and just have one channel, since they are black and white pictures.\n",
    "\n",
    "Go ahead and build this small convolutional model!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the Conv2D and Flatten layers and instantiate your model.\n",
    "    Add a first convolutional layer with 32 filters of size 3x3 and the corresponding 3D tuple as input_shape.\n",
    "    Add a second convolutional layer with 16 filters of size 3x3 with relu activation.\n",
    "    Flatten the previous layer output to create a one-dimensional vector.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3\n",
    "model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "# Flatten the previous layer output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "#----------------------------------#\n",
    "model.summary()\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You can see that the key concepts are the same, you just have to use new layers!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"/home/nero/.keras/datasets/mnist.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 09:23:14.095773: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 865280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nHurrah! Each neuron filter of the first layer learned a different convolution. The 15th filter (a.k.a convolutional mask) learned to detect horizontal traces in your digits. On the other hand, filter 18th seems to be checking for vertical traces.\\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Looking at convolutions\n",
    "\n",
    "Inspecting the activations of a convolutional layer is a cool thing. You have to do it at least once in your lifetime!\n",
    "\n",
    "To do so, you will build a new model with the Keras Model object, which takes in a list of inputs and a list of outputs. The outputs you will provide to this new model is the first convolutional layer outputs when given an MNIST digit as input image.\n",
    "\n",
    "The convolutional model you built in the previous exercise has already been trained for you. It can now correctly classify MNIST handwritten images. You can check it with model.summary() in the console.\n",
    "\n",
    "Let's look at the convolutional masks that were learned in the first convolutional layer of this model!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Obtain a reference to the outputs of the first convolutional layer in the model.\n",
    "    Build a new model using the model's first layer input and the first_layer_output as outputs.\n",
    "    Use this first_layer_model to predict on X_test.\n",
    "    Plot the activations of the first digit of X_test for the 15th and the 18th neuron filter.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Obtain a reference to the outputs of the first layer\n",
    "first_layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model's input and the first layer output\n",
    "first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(x_test)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "# Plot the activations of first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,17], cmap = 'viridis')\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Hurrah! Each neuron filter of the first layer learned a different convolution. The 15th filter (a.k.a convolutional mask) learned to detect horizontal traces in your digits. On the other hand, filter 18th seems to be checking for vertical traces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = path_data + 'dog.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAlright! Ivy is now ready for ResNet50. Do you know this dog's breed? Let's see what this model thinks it is!\\n\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Preparing your input image\n",
    "\n",
    "The original ResNet50 model was trained with images of size 224 x 224 pixels and a number of preprocessing operations; like the subtraction of the mean pixel value in the training set for all training images. You need to pre-process the images you want to predict on in the same way.\n",
    "\n",
    "When predicting on a single image you need it to fit the model's input shape, which in this case looks like this: (batch-size, width, height, channels),np.expand_dims with parameter axis = 0 adds the batch-size dimension, representing that a single image will be passed to predict. This batch-size dimension value is 1, since we are only predicting on one image.\n",
    "\n",
    "You will go over these preprocessing steps as you prepare this dog's (named Ivy) image into one that can be classified by ResNet50.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import image from tensorflow.keras.preprocessing and preprocess_input from tensorflow.keras.applications.resnet50.\n",
    "    Load the image with the right target_size for your model.\n",
    "    Turn it into an array with image.img_to_array().\n",
    "    Pre-process img_expanded the same way the original ResNet50 training images were processed with preprocess_input().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import image and preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image, this is so that it fits the expected model input format\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Alright! Ivy is now ready for ResNet50. Do you know this dog's breed? Let's see what this model thinks it is!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102967424/102967424 [==============================] - 57s 1us/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "35363/35363 [==============================] - 0s 2us/step\n",
      "Predicted: [('n02088364', 'beagle', 0.9073764), ('n02089867', 'Walker_hound', 0.06626669), ('n02089973', 'English_foxhound', 0.018850882)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAmazing! Check the console! Now you know Ivy is quite probably a Beagle and that deep learning models that have already been trained for you are easy to use!\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Using a real world model\n",
    "\n",
    "Okay, so Ivy's picture is ready to be used by ResNet50. It is stored in img_ready and now looks like this:\n",
    "\n",
    "ResNet50 is a model trained on the Imagenet dataset that is able to distinguish between 1000 different labeled objects. ResNet50 is a deep model with 50 layers, you can check it in 3D here. (https://tensorspace.org/html/playground/resnet50.html)\n",
    "\n",
    "ResNet50 and decode_predictions have both been imported from tensorflow.keras.applications.resnet50 for you.\n",
    "\n",
    "It's time to use this trained model to find out Ivy's breed!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Instantiate a ResNet50 model, setting the weights parameter to be 'imagenet'.\n",
    "    Use the model to predict on your processed image.\n",
    "    Decode the first 3 predictions with decode_predictions().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode the first 3 predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Amazing! Check the console! Now you know Ivy is quite probably a Beagle and that deep learning models that have already been trained for you are easy to use!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'it is not the strength of the body but the strength of the spirit it is useless to meet revenge with revenge it will heal nothing even the smallest person can change the course of history all we have to decide is what to do with the time that is given us the burned hand teaches best after that advice about fire goes to the heart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: \n",
      " ['it is not the', 'is not the strength', 'not the strength of', 'the strength of the', 'strength of the body'] \n",
      " Sequences: \n",
      " [[5, 2, 42, 1], [2, 42, 1, 6], [42, 1, 6, 4], [1, 6, 4, 1], [6, 4, 1, 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! Your sentences are now sequences of numbers, check that identical words are assigned the same number.\\n'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 12\n",
    "\n",
    "\"\"\"\n",
    "Text prediction with LSTMs\n",
    "\n",
    "During the following exercises you will build a toy LSTM model that is able to predict the next word using a small text dataset. This dataset consist of cleaned quotes from the The Lord of the Ring movies. You can find them in the text variable.\n",
    "\n",
    "You will turn this text into sequences of length 4 and make use of the Keras Tokenizer to prepare the features and labels for your model!\n",
    "\n",
    "The Keras Tokenizer is already imported for you to use. It assigns a unique number to each unique word, and stores the mappings in a dictionary. This is important since the model deals with numbers but we later will want to decode the output numbers back into words.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split the text into an array of words using .split().\n",
    "    Make sentences of 4 words each, moving one word at a time.\n",
    "    Instantiate a Tokenizer(), then fit it on the sentences with .fit_on_texts().\n",
    "    Turn sentences into a sequence of numbers calling .texts_to_sequences().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# solution\n",
    "\n",
    "# Split text into an array of words \n",
    "words = text.split()\n",
    "\n",
    "# Make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "  sentences.append(' '.join(words[i-4:i]))\n",
    "\n",
    "# Instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sentences: \\n {} \\n Sequences: \\n {}\".format(sentences[:5],sequences[:5]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Your sentences are now sequences of numbers, check that identical words are assigned the same number.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 8)              528       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                5248      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 66)                2178      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,010\n",
      "Trainable params: 9,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 10:06:28.554067: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-26 10:06:28.557913: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-26 10:06:28.560759: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThat's a nice looking model you've built! You'll see that this model is powerful enough to learn text relationships, we aren't using a lot of text in this tiny example and our sequences are quite short. This model is to be trained as usual, you would just need to compile it with an optimizer like adam and use crossentropy loss. This is because we have modeled this next word prediction task as a classification problem with all the unique words in our vocabulary as candidate classes.\\n\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 13\n",
    "\n",
    "\"\"\"\n",
    "Build your LSTM model\n",
    "\n",
    "You've already prepared your sequences of text. It's time to build your LSTM model!\n",
    "\n",
    "Remember your sequences had 4 words each, your model will be trained on the first three words of each sequence, predicting the 4th one. You are going to use an Embedding layer that will essentially learn to turn words into meaningful vectors. These vectors will then be passed to a simple LSTM layer. Our output is a Dense layer with as many neurons as words in the vocabulary and softmax activation. This is because we want to obtain the highest probable next word out of all possible words.\n",
    "\n",
    "The size of the vocabulary of words (the unique number of words) is stored in vocab_size.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the Embedding, LSTM and Dense layer from tensorflow.keras layers.\n",
    "    Add an Embedding() layer of the vocabulary size, that will turn words into 8 number vectors and receive sequences of length 3.\n",
    "    Add a 32 neuron LSTM() layer.\n",
    "    Add a hidden Dense() layer of 32 neurons and an output layer of vocab_size neurons with softmax.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the Embedding, LSTM and Dense layer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim = vocab_size, input_length = 3, output_dim = 8, ))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That's a nice looking model you've built! You'll see that this model is powerful enough to learn text relationships, we aren't using a lot of text in this tiny example and our sequences are quite short. This model is to be trained as usual, you would just need to compile it with an optimizer like adam and use crossentropy loss. This is because we have modeled this next word prediction task as a classification problem with all the unique words in our vocabulary as candidate classes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGreat job! It's finally time to try out your model and see how well it does!\\n\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 14\n",
    "\n",
    "\"\"\"\n",
    "Decode your predictions\n",
    "\n",
    "Your LSTM model has already been trained (details in the previous exercise success message) so that you don't have to wait. It's time to define a function that decodes its predictions. The trained model will be passed as a default parameter to this function.\n",
    "\n",
    "Since you are predicting on a model that uses the softmax function, numpy's argmax() can be used to obtain the index/position representing the most probable next word out of the output vector of probabilities.\n",
    "\n",
    "The tokenizer you previously created and fitted, is loaded for you. You will be making use of its internal index_word dictionary to turn the model's next word prediction (which is an integer) into the actual written word it represents.\n",
    "\n",
    "You're very close to experimenting with your model!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use texts_to_sequences() to turn the test_text parameter into a sequence of numbers.\n",
    "    Get the model's next word prediction by passing in test_seq . The index/position representing the word with the highest probability is obtained by calling .argmax(axis=1)[0] on the numpy array of predictions.\n",
    "    Return the word that maps to the prediction using the tokenizer's index_word dictionary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def predict_text(test_text, model = model):\n",
    "  if len(test_text.split()) != 3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "  \n",
    "  # Turn the test_text into a sequence of numbers\n",
    "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "  test_seq = np.array(test_seq)\n",
    "  \n",
    "  # Use the model passed as a parameter to predict the next word\n",
    "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "  # Return the word that maps to the prediction\n",
    "  return tokenizer.index_word[pred]\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! It's finally time to try out your model and see how well it does!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
