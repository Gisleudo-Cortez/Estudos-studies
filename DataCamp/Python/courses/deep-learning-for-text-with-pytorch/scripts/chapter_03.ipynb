{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "data = 'The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.'\n",
    "\n",
    "chars = ['r', 'l', 'n', 'k', 's', 'c', 'T', 'd', 'y', ',', 'o', '.', 'u', '-', 'h', 'f', 'g', 'm', 'a', 'e', 'b', 'p', 'v', 'i', ' ', 'A', 't', 'w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGreat job! By successfully creating this RNN model, you have taken the first step towards developing an advanced text generation system. By adding the fully connected layer for text generation, you have allowed RNN to predict the next element in a sequence. Let's further train and evaluate the model. Keep going!\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Creating a RNN model for text generation\n",
    "\n",
    "At PyBooks, you've been tasked to develop an algorithm that can perform text generation. The project involves auto-completion of book names. To kickstart this project, you decide to experiment with a Recurrent Neural Network (RNN). This way, you can understand the nuances of RNNs before moving to more complex models.\n",
    "\n",
    "The following has been imported for you: torch, torch.nn as nn.\n",
    "\n",
    "The data variable has been initialized with an excerpt from Alice's Adventures in Wonderland by Lewis Carroll.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Include an RNN layer and linear layer in RNNmodel class\n",
    "\n",
    "    Instantiate the RNN model with input size as length of chars, hidden size of 16, and output size as length of chars.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Include an RNN layer and linear layer in RNNmodel class\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "      h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "      out, _ = self.rnn(x, h0)  \n",
    "      out = self.fc(out[:, -1, :])  \n",
    "      return out\n",
    "\n",
    "# Instantiate the RNN model\n",
    "model = RNNmodel(len(chars), 16, len(chars))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! By successfully creating this RNN model, you have taken the first step towards developing an advanced text generation system. By adding the fully connected layer for text generation, you have allowed RNN to predict the next element in a sequence. Let's further train and evaluate the model. Keep going!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Text generation using RNN - Training and Generation\n",
    "\n",
    "The team at PyBooks now wants you to train and test the RNN model, which is designed to predict the next character in the sequence based on the provided input for auto-completion of book names. This project will help the team further develop models for text completion.\n",
    "\n",
    "The model instance for the RNNmodel class is preloaded for you. The data variable has been preprocessed and encoded as a sequence.\n",
    "\n",
    "The inputs and targets variable are preloaded for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Instantiate the loss function which will be used to compute the error of our model.\n",
    "\n",
    "    Instantiate the optimizer from PyTorch's optimization module.\n",
    "\n",
    "    Run the model training process by setting the model to the train mode and zeroing the gradients before performing an optimization step.\n",
    "\n",
    "    After the training process, switch the model to evaluation mode to test it on a sample input.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_input = char_to_ix['r']\n",
    "test_input = nn.functional.one_hot(torch.tensor(test_input).view(-1, 1), num_classes=len(chars)).float()\n",
    "predicted_output = model(test_input)\n",
    "predicted_char_ix = torch.argmax(predicted_output, 1).item()\n",
    "print(f\"Test Input: 'r', Predicted Output: '{ix_to_char[predicted_char_ix]}'\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Your RNN model has been successfully trained and tested. The model looks to be predicting the word 'rabbit'. You can also explore the same example with LSTMs or GRUs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Building a generator and discriminator\n",
    "\n",
    "At PyBooks, you're tasked with working on an automatic text generator to help writers overcome writer's block. By using GANs, or Generative Adversarial Networks, you believe you can create a system where one network, the generator, creates new text while the other network, the discriminator, evaluates its authenticity. To do this, you need to initialize both a generator and discriminator network. These networks will then be trained against each other to create new, believable text.\n",
    "\n",
    "The following has been imported for you: torch, torch.nn as nn.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the Generator class with a linear layer for sequential data and a sigmoid activation function.\n",
    "\n",
    "    Pass the input through the defined model in the forward() method of the Generator class.\n",
    "\n",
    "    Define a Discriminator class with the same layers and activation function, taking care when defining the dimensions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define the generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length, seq_length), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the discriminator networks\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length, 1), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! You have successfully defined a generator and a discriminator for your GAN. These networks will be the backbone of your text generation system. Now you're ready to move on to the next step, which is to train these networks against each other so they can start generating and evaluating new text!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Training a GAN model\n",
    "\n",
    "Your team at PyBooks has made good progress in building the text generator using a Generative Adversarial Network (GAN). You have successfully defined the generator and discriminator networks. Now, it's time to train them. The final step is to generate some fake data and compare it with the real data to see how well your GAN has learned. We have used tensors as an input and the output would try to resemble the input tensors. The team at PyBooks can then use this synthetic data for text analysis as the features will have same relationship as text data.\n",
    "\n",
    "The generator and discriminator have been initialized and saved to generator and discriminator, respectively.\n",
    "\n",
    "The following variables have been initialized in the exercise:\n",
    "\n",
    "    seq_length = 5: Length of each synthetic data sequence\n",
    "    num_sequences = 100: Total number of sequences generated\n",
    "    num_epochs = 50: Number of complete passes through the dataset\n",
    "    print_every = 10: Output display frequency, showing results every 10 epochs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the loss function for binary classification and the Adam optimizer.\n",
    "\n",
    "    Train the discriminator by unsqueezing real_data and preventing gradient recalculations.\n",
    "---\n",
    "\n",
    "    Train the generator by calculating the loss and zeroing the gradients.\n",
    "---\n",
    "\n",
    "    Detach the tensor to prevent further computation and print data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in data:\n",
    "        # Unsqueezing real_data and prevent gradient recalculations\n",
    "        real_data = real_data.unsqueeze(0)\n",
    "        noise = torch.rand((1, seq_length))\n",
    "        fake_data = generator(noise)\n",
    "        disc_real = discriminator(real_data)\n",
    "        disc_fake = discriminator(fake_data.detach())\n",
    "        loss_disc = criterion(disc_real, torch.ones_like(disc_real)) + criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        optimizer_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # Train the generator\n",
    "        disc_fake = discriminator(fake_data)\n",
    "        loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "        optimizer_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\\t Generator loss: {loss_gen.item()}\\t Discriminator loss: {loss_disc.item()}\")\n",
    "\n",
    "print(\"\\nReal data: \")\n",
    "print(data[:5])\n",
    "\n",
    "print(\"\\nGenerated data: \")\n",
    "for _ in range(5):\n",
    "    noise = torch.rand((1, seq_length))\n",
    "    generated_data = generator(noise)\n",
    "    # Detach the tensor and print data\n",
    "    print(torch.round(generated_data).detach())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You have now successfully generated and printed synthetic data using the trained generator network. Fantastic work! The team at PyBooks can then use this synthetic data for text analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Text completion with pre-trained GPT-2 models\n",
    "\n",
    "Back at PyBooks, your current project involves creating captivating narratives based on existing stories to engage customers and enhance their experience. To achieve this, you need a powerful text generation tool that can seamlessly generate compelling text continuations. You'll be using a pre-trained model to get the job done.\n",
    "\n",
    "The following has been loaded for you: torch and GPT2Tokenizer,GPT2LMHeadModel from transformers.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Initialize the pre-trained GPT-2 tokenizer for gpt2.\n",
    "    Initialize the pre-trained GPT-2 LMHead model for gpt2.\n",
    "---\n",
    "\n",
    "    Encode the seed text to get input tensors using seed_text.\n",
    "---\n",
    "\n",
    "    Generate the text by defining the parameters for the input considering a max_length of 100, temperature of 0.7, and an no_repeat_ngram_size of 2 in the GPT-2 model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize the pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "seed_text = \"Once upon a time\"\n",
    "\n",
    "# Encode the seed text to get input tensors\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(input_ids, max_length=100, temperature=0.7, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id) \n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! You have successfully completed the text completion exercise using the GPT-2 model and tokenizer. By analyzing the generated output, you can explore the fascinating world of language generation and witness the model's ability to generate imaginative and coherent text based on the provided seed text. Well done!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Language translation with pretrained PyTorch model\n",
    "\n",
    "Your team at PyBooks is working on an AI project that involves translation from one language to another. They want to leverage pre-trained models for this task, which can save a lot of training time and resources. The task for this exercise is to set up a translation model from HuggingFace's Transformers library, specifically the T5 (Text-To-Text Transfer Transformer) model, and use it to translate an English phrase to French.\n",
    "\n",
    "T5Tokenizer, T5ForConditionalGeneration have been loaded for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Initialize the tokenizer and model from the pretrained \"t5-small\" model.\n",
    " \n",
    "    Encode the input prompt using the tokenizer, making sure to return PyTorch tensors.\n",
    " \n",
    "    Translate the input prompt using model and generate the translated output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Initalize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_prompt = \"translate English to French: 'Hello, how are you?'\"\n",
    "\n",
    "# Encode the input prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the translated ouput\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\",generated_text)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! You've successfully used a pre-trained T5 model to translate an English phrase to French. This example shows the power of transformer models and how they can be leveraged for various NLP tasks, including translation. Well done!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Evaluating pretrained text generation model\n",
    "\n",
    "The PyBooks team has used a pre-trained GPT-2 model that you experimented to generate a text based on a given prompt. Now, they want to evaluate the quality of this generated text. To achieve this, they have tasked you to evaluate generated text using a reference text.\n",
    "\n",
    "BLEUScore, ROUGEScore have been loaded for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Begin by initializing the two metrics (BLEU and ROUGE) provided from torchmetrics.text.\n",
    "\n",
    "    Use these initialized metrics to calculate the scores between the generated text and the reference text.\n",
    "\n",
    "    Display the calculated BLEU and ROUGE scores.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "reference_text = \"Once upon a time, there was a little girl who lived in a village near the forest.\"\n",
    "generated_text = \"Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\"\n",
    "\n",
    "# Initialize BLEU and ROUGE scorers\n",
    "bleu = BLEUScore()\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "# Calculate the BLEU and ROUGE scores\n",
    "bleu_score = bleu([generated_text], [[reference_text]])\n",
    "rouge_score = rouge([generated_text], [[reference_text]])\n",
    "\n",
    "# Print the BLEU and ROUGE scores\n",
    "print(\"BLEU Score:\", bleu_score.item())\n",
    "print(\"ROUGE Score:\", rouge_score)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You've successfully calculated the BLEU and ROUGE scores for the text generation gpt2 model. Let's analyse the results in the next exercise. Well done!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding text generation metrics\n",
    "\n",
    "At PyBooks, the team just evaluated the performance of a pretrained model using the BLEU score and got a result of approximately 0.082 and a rouge1_fmeasure of around 0.2692. This metric is an indication of precision (how many selected items are relevant) and recall (how many relevant items are selected). How would you interpret this score in terms of the model's performance?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "The model's generated text is almost identical to the reference text\n",
    "\n",
    "\n",
    "The rouge measure indicated that model's generated text has some similarity to the reference text{Answers}\n",
    "\n",
    "\n",
    "The model's generated text bears no similarity to the reference text\n",
    "\n",
    "\n",
    "The BLUE score indicates that model's generated text has low similarity to the reference text{Answers}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
