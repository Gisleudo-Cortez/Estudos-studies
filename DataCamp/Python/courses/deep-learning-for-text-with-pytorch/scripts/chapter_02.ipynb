{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1854, -1.3150,  0.8063, -1.2092, -0.0362,  0.9742, -0.5215, -0.9613,\n",
      "          0.6066, -0.7495],\n",
      "        [-0.3593,  0.1090,  1.2951,  0.5021,  0.4282,  0.7780,  0.1923,  1.5818,\n",
      "         -0.3431,  1.4265],\n",
      "        [-0.7465,  0.9739,  0.2314,  0.1304, -0.5155,  0.7360, -0.1358, -0.6805,\n",
      "         -1.4444,  0.0841],\n",
      "        [ 0.5613, -1.6398, -0.9065,  1.4896,  1.1976,  0.3439,  0.0316, -0.0920,\n",
      "         -0.3319, -0.6781],\n",
      "        [ 1.8575, -2.0752, -0.3377, -0.5328, -1.9939, -0.1454, -0.4608,  0.2831,\n",
      "          0.5351,  0.0399],\n",
      "        [ 1.2035,  0.4615,  0.7263,  1.3418, -1.3744, -0.3792,  0.1203, -0.0482,\n",
      "          0.8152, -0.9314],\n",
      "        [-0.3259,  0.7914,  0.4382,  0.9156, -0.4747,  3.0653, -0.5814, -1.1279,\n",
      "         -2.8730,  1.8809],\n",
      "        [ 0.0557,  0.4282, -0.6123,  1.9319,  0.2307, -0.7310, -1.0255, -0.9876,\n",
      "          1.9326,  0.3179],\n",
      "        [ 0.5369,  1.4284,  0.3501, -1.0622,  0.9558, -0.5579,  0.5140, -1.2508,\n",
      "         -0.1660,  0.0384],\n",
      "        [ 0.7539, -1.9617,  0.5487,  1.5364,  0.5225,  1.3793,  0.1496, -1.0570,\n",
      "         -1.5150,  0.2413],\n",
      "        [-0.3212, -1.9189,  0.5850, -0.4093, -0.8048, -1.3530, -0.7997,  0.4476,\n",
      "         -0.6029,  0.6193],\n",
      "        [-0.1075,  0.7310,  0.1828, -1.1673,  0.2322,  1.1397,  0.3964,  0.5027,\n",
      "         -0.2354,  1.0433],\n",
      "        [-0.7465,  0.9739,  0.2314,  0.1304, -0.5155,  0.7360, -0.1358, -0.6805,\n",
      "         -1.4444,  0.0841],\n",
      "        [ 0.3990,  0.2987,  1.7255, -0.5610,  0.9367, -2.0524, -0.2640, -1.0915,\n",
      "          1.0333, -2.3054],\n",
      "        [-1.4416,  1.3065, -1.7166,  0.3268,  0.0071, -0.1477,  2.0246,  0.2160,\n",
      "         -0.0857,  0.2372]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nBrilliant! You've just implemented PyTorch's nn.Embedding for PyBooks. The output shows each elements representing the respective word embeddings. Keep up the great work!\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Embedding in PyTorch\n",
    "\n",
    "PyBooks found success with a book recommendation system. However, it doesn't account for some of the semantics found in the text. PyTorch's built-in embedding layer can learn and represent the relationship between words directly from data. Your team is curious to explore this capability to improve the book recommendation system. Can you help implement it?\n",
    "\n",
    "torch and torch.nn as nn have been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Map a unique index to each word in words, saving to word_to_idx.\n",
    "\n",
    "    Convert word_to_idx into a PyTorch tensor and save to inputs.\n",
    "\n",
    "    Initialize an embedding layer using the torch module with ten dimensions.\n",
    "\n",
    "    Pass the inputs tensor to the embedding layer and review the output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Map a unique index to each word\n",
    "words = [\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\n",
    "word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# Convert word_to_idx to a tensor\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "# Initialize embedding layer with ten dimensions\n",
    "embedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n",
    "\n",
    "# Pass the tensor to the embedding layer\n",
    "output = embedding(inputs)\n",
    "print(output)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Brilliant! You've just implemented PyTorch's nn.Embedding for PyBooks. The output shows each elements representing the respective word embeddings. Keep up the great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing text classification tasks\n",
    "\n",
    "Classification tasks can either be binary, multi-class, or multi-label.\n",
    "\n",
    "You are presented with several classification cases and you need to categorize them as one of the three types of tasks.\n",
    "\n",
    "![Answer](/home/nero/Documents/Estudos/DataCamp/Python/courses/deep-learning-for-text-with-pytorch/categorizing_text_class_tasks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWell done! You can now use an instance of this CNN class to train the model in the next step!\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Build a CNN model for text\n",
    "\n",
    "PyBooks has successfully built a book recommendation engine. Their next task is to implement a sentiment analysis model to understand user reviews and gain insight into book preferences.\n",
    "\n",
    "You'll use a Convolutional Neural Network (CNN) model to classify text data (book reviews) based on their sentiment.\n",
    "\n",
    "torch, torch.nn as nn, and torch.nn.functional as F have been loaded for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Initialize the embedding layer in the __init__() method.\n",
    "\n",
    "    Apply the convolutional layer self.conv to the embedded text within the forward() method.\n",
    "\n",
    "    Apply the ReLU activation to this layer within the forward() method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "class TextClassificationCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TextClassificationCNN, self).__init__()\n",
    "        # Initialize the embedding layer \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        # Pass the embedded text through the convolutional layer and apply a ReLU\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2) \n",
    "        return self.fc(conved)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You can now use an instance of this CNN class to train the model in the next step!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Train a CNN model for text\n",
    "\n",
    "Well done defining the TextClassificationCNN class. PyBooks now needs to train the model to optimize it for accurate sentiment analysis of book reviews.\n",
    "\n",
    "The following packages have been imported for you: torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim.\n",
    "\n",
    "An instance of TextClassificationCNN() with arguments vocab_size and embed_dim has also been loaded and saved as model.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a loss function used for multilabel classification and save as criterion.\n",
    "\n",
    "    Zero the gradients at the start of the training loop.\n",
    "\n",
    "    Update the parameters at the end of the loop.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for sentence, label in data:     \n",
    "        # Clear the gradients\n",
    "        model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_ix.get(w, 0) for w in sentence]).unsqueeze(0) \n",
    "        label = torch.LongTensor([int(label)])\n",
    "        outputs = model(sentence)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "print('Training complete!')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent job! You've successfully built and trained a CNN for sentiment analysis. This will be a great addition to the PyBooks' recommendation engine, assisting the team in understanding users' sentiments towards different books.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Testing the Sentiment Analysis CNN Model\n",
    "\n",
    "Now that model is trained, PyBooks wants to check its performance on some new book reviews.\n",
    "\n",
    "You need to check if the sentiment in a review is positive or negative.\n",
    "\n",
    "The following packages have been imported for you: torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim.\n",
    "\n",
    "An instance of TextClassificationCNN() with arguments vocab_size and embed_dim has also been loaded and saved as model.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Iterate over the book_reviews list, converting the words in each review into a tensor.\n",
    "\n",
    "    Get the model's output for each input_tensor.\n",
    "\n",
    "    Find the index of the most likely sentiment category from the outputs.data.\n",
    "\n",
    "    Extract and convert the predicted_label item into a sentiment string where 1 is a \"Positive\" label.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "book_reviews = [\n",
    "    \"I love this book\".split(),\n",
    "    \"I do not like this book\".split()\n",
    "]\n",
    "for review in book_reviews:\n",
    "    # Convert the review words into tensor form\n",
    "    input_tensor = torch.tensor([word_to_ix[w] for w in review], dtype=torch.long).unsqueeze(0) \n",
    "    # Get the model's output\n",
    "    outputs = model(input_tensor)\n",
    "    # Find the index of the most likely sentiment category\n",
    "    _, predicted_label = torch.max(outputs.data, 1)\n",
    "    # Convert the predicted label into a sentiment string\n",
    "    sentiment = \"Positive\" if predicted_label.item() == 1 else \"Negative\"\n",
    "    print(f\"Book Review: {' '.join(review)}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fantastic work! You've successfully applied the trained CNN model to predict the sentiment of a given text. Did the model predict the sentiment correctly? If not, don't worry we will look into more techniques to improve it!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Building an RNN model for text\n",
    "\n",
    "As a data analyst at PyBooks, you often encounter datasets that contain sequential information, such as customer interactions, time series data, or text documents. RNNs can effectively analyze and extract insights from such data. In this exercise, you will dive into the Newsgroup dataset that has already been processed and encoded for you. This dataset comprises articles from different categories. Your task is to apply an RNN to classify these articles into three categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The following has been loaded for you: torch, nn, optim.\n",
    "\n",
    "Additionally, the parameters input_size, hidden_size (32), num_layers (2), and num_classes have been preloaded for you.\n",
    "\n",
    "This and the following exercises use the fetch_20newsgroups dataset from sklearn.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Complete the RNN class with an RNN layer and a fully connected linear layer.\n",
    "\n",
    "    Initialize the model.\n",
    "\n",
    "    Train the RNN model for ten epochs by zeroing the gradients.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Complete the RNN class\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model for ten epochs and zero the gradients\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Model loss should always decrease as it shows how well the model has learned new patterns. Keep up the excellent work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Building an LSTM model for text\n",
    "\n",
    "At PyBooks, the team is constantly seeking to enhance the user experience by leveraging the latest advancements in technology. In line with this vision, they have assigned you a critical task. The team wants you to explore the potential of another powerful tool: LSTM, known for capturing more complexities in data patterns. You are working with the same Newsgroup dataset, with the objective remaining unchanged: to classify news articles into three distinct categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The following packages have been loaded for you: torch, nn, optim.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Set up an LSTM model by completing the LSTM and linear layers with the necessary parameters.\n",
    "\n",
    "    Initialize the model with the necessary parameters.\n",
    "\n",
    "    Train the LSTM model resetting the gradients to zero and passing the input data X_train_seq through the model.\n",
    "\n",
    "    Calculate the loss based on the predicted outputs and the true labels.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Initialize the LSTM and the output layer with parameters\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model with required parameters\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! The output presents model loss that would keep decreasing with each epoch. This information could be utilized by the team at PyBooks to compare with other models. Keep up the great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Building a GRU model for text\n",
    "\n",
    "At PyBooks, the team has been impressed with the performance of the two models you previously trained. However, in their pursuit of excellence, they want to ensure the selection of the absolute best model for the task at hand. Therefore, they have asked you to further expand the project by experimenting with the capabilities of GRU models, renowned for their efficiency and effectiveness in text classification tasks. Your new assignment is to apply the GRU model to classify articles from the Newsgroup dataset into the following categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The following packages have been loaded for you: torch, nn, optim.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Complete the GRU class with the required parameters.\n",
    "\n",
    "    Initialize the model with the same parameters.\n",
    "\n",
    "    Train the model: pass the parameters to the criterion function, and backpropagate the loss.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Complete the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model and backpropagate the loss after initialization\n",
    "for epoch in range(15): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = gru_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! You've effectively trained GRU models for text classification. The decreasing model loss across epochs is promising, and can be used by the PyBooks team for comparison with other models!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Evaluating RNN classification models\n",
    "\n",
    "The team at PyBooks now wants you to evaluate the RNN model you created and ran using the Newsgroup dataset. Recall, the goal was to classify the articles into one of three categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The model was trained and you printed epoch and loss for each model.\n",
    "\n",
    "Use torchmetrics to evaluate various metrics for your model. The following has been loaded for : Accuracy, Precision, Recall, F1Score.\n",
    "\n",
    "An instance of rnn_model trained in the previous exercise in preloaded for you, too.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create an instance of each metric for multi-class classification with num_classes equal to the number of categories.\n",
    "\n",
    "    Generate the predictions for the rnn_model using the test data X_test_seq.\n",
    "\n",
    "    Calculate the metrics using the predicted classes and the true labels.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "precision = Precision(task=\"multiclass\", num_classes=3)\n",
    "recall = Recall(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "# Generate the predictions\n",
    "outputs = rnn_model(X_test_seq)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_score = accuracy(predicted, y_test_seq)\n",
    "precision_score = precision(predicted, y_test_seq)\n",
    "recall_score = recall(predicted, y_test_seq)\n",
    "f1_score = f1(predicted, y_test_seq)\n",
    "print(\"RNN Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_score, precision_score, recall_score, f1_score))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! The model metrics provide significant insights on the effectiveness of our model. We can notice that all metrics are around 0.80 which is a good sign. Keep up the excellent work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Evaluating the model's performance\n",
    "\n",
    "The PyBooks team has been making strides on the book recommendation engine. The modeling team has provided you two different models ready for your book recommendation engine at PyBooks. One model is based on LSTM (lstm_model) and the other uses a GRU (gru_model). You've been tasked to evaluate and compare these models.\n",
    "\n",
    "The testing labels y_test and the model's predictions y_pred_lstm for lstm_model and y_pred_gru for gru_model.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define accuracy, precision, recall and F1 for multi-class classification by specifying num_classes and task.\n",
    "\n",
    "    Calculate and print the accuracy, precision, recall, and F1 score for lstm_model.\n",
    "\n",
    "    Similarly, calculate the evaluation metrics for gru_model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task='multiclass', num_classes=3)\n",
    "precision = Precision(task='multiclass', num_classes=3)\n",
    "recall = Recall(task='multiclass', num_classes=3)\n",
    "f1 = F1Score(task='multiclass', num_classes=3)\n",
    "\n",
    "# Calculate metrics for the LSTM model\n",
    "accuracy_1 = accuracy(y_pred_lstm, y_test)\n",
    "precision_1 = precision(y_pred_lstm, y_test)\n",
    "recall_1 = recall(y_pred_lstm, y_test)\n",
    "f1_1 = f1(y_pred_lstm, y_test)\n",
    "print(\"LSTM Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_1, precision_1, recall_1, f1_1))\n",
    "\n",
    "# Calculate metrics for the GRU model\n",
    "accuracy_2 = accuracy(y_pred_gru, y_test)\n",
    "precision_2 = precision(y_pred_gru, y_test)\n",
    "recall_2 = recall(y_pred_gru, y_test)\n",
    "f1_2 = f1(y_pred_gru, y_test)\n",
    "print(\"GRU Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_2, precision_2, recall_2, f1_2))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You've evaluated and compared two different models. Now, PyBooks can decide which model to deploy for their book recommendation engine. Let's analyze these results in the next exercise!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models\n",
    "\n",
    "You're comparing the performance of two classification models. The results of lstm_model and gru_model are as follows:\n",
    "\n",
    "lstm_model - Accuracy: 0.7633, Precision: 0.7633, \n",
    "             Recall: 0.7633, F1 Score: 0.7633\n",
    "\n",
    "gru_model - Accuracy: 0.8169, Precision: 0.8169, \n",
    "            Recall: 0.8169, F1 Score: 0.8169\n",
    "\n",
    "Let's compare the two models.\n",
    "\n",
    "Which of the below statements is correct?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    gru_model performed better in precision, recall, and F1 score {Answer}\n",
    "    \n",
    "    \n",
    "    lstm_model performed better in accuracy, recall, and F1 score\n",
    "    \n",
    "    \n",
    "    Both models performed identically\n",
    "    \n",
    "    \n",
    "    It's not possible to decide based on the provided metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
