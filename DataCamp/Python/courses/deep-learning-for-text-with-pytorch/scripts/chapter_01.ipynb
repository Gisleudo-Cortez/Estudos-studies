{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', ',', 'data', 'alex', 'data', '.', ',', 'alex', ',', 'the', 'data', ',', '.', 'the', 'of', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCongratulations! You have removed rare words from your text. It looks like data and alex are pretty common. In practice, you'll work with larger text and may find more meaningful words.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Word frequency analysis\n",
    "\n",
    "Congratulations! You've just joined PyBooks. PyBooks is developing a book recommendation system and they want to find patterns and trends in text to improve their recommendations.\n",
    "\n",
    "To begin, you'll want to understand the frequency of words in a given text and remove any rare words.\n",
    "\n",
    "Note that typical real-world datasets will be larger than this example.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the tokenization function from torchtext and frequency distribution function from the nltk library.\n",
    "\n",
    "    Initialize the tokenizer for English and tokenize the given text.\n",
    "\n",
    "    Calculate the frequency distribution of the tokens and remove rare words using list comprehension.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the necessary functions\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
    "\n",
    "# Initialize the tokenizer and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "threshold = 1\n",
    "# Remove rare words and print common tokens\n",
    "freq_dist = FreqDist(tokens)\n",
    "common_tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You have removed rare words from your text. It looks like data and alex are pretty common. In practice, you'll work with larger text and may find more meaningful words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The moor is very sparsely inhabited, and those who live near each other are thrown very much together. For this reason I saw a good deal of Sir Charles Baskerville. With the exception of Mr. Frankland, of Lafter Hall, and Mr. Stapleton, the naturalist, there are no other men of education within many miles. Sir Charles was a retiring man, but the chance of his illness brought us together, and a community of interests in science kept us so. He had brought back much scientific information from South Africa, and many a charming evening we have spent together discussing the comparative anatomy of the Bushman and the Hottentot.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nero/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moor', 'spars', 'inhabit', ',', 'live', 'near', 'thrown', 'much', 'togeth', '.', 'reason', 'saw', 'good', 'deal', 'sir', 'charl', 'baskervil', '.', 'except', 'mr', '.', 'frankland', ',', 'lafter', 'hall', ',', 'mr', '.', 'stapleton', ',', 'naturalist', ',', 'men', 'educ', 'within', 'mani', 'mile', '.', 'sir', 'charl', 'retir', 'man', ',', 'chanc', 'ill', 'brought', 'us', 'togeth', ',', 'commun', 'interest', 'scienc', 'kept', 'us', '.', 'brought', 'back', 'much', 'scientif', 'inform', 'south', 'africa', ',', 'mani', 'charm', 'even', 'spent', 'togeth', 'discuss', 'compar', 'anatomi', 'bushman', 'hottentot', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nBravo, Sherlock! You've cracked the case of the convoluted text. Now, you have a clean, processed list of words that you can use to analyze the text. You're officially a master in text preprocessing!\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Preprocessing text\n",
    "\n",
    "Building a recommendation system, or any model, requires text to be preprocessed first.\n",
    "\n",
    "A block of text from Sherlock Holmes is loaded here. Preprocess this text using the various techniques presented in the video to prepare it for further analysis.\n",
    "\n",
    "The text variable is an excerpt from The Hound of the Baskervilles by Arther Conan Doyle.\n",
    "\n",
    "The following packages and functions have been loaded for you: nltk, torch, get_tokenizer, PorterStemmer, stopwords.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Initialize the tokenizer with \"basic_english\".\n",
    "    Tokenize the text using the tokenizer.\n",
    "---\n",
    "\n",
    "    Create a set of English stopwords and use list comprehension to filter these stop_words out of the text, making sure to ignore capitalization.\n",
    "---\n",
    "\n",
    "    Perform stemming on the filtered_tokens using the appropriate nltk function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Initialize and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Remove any stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Perform stemming on the filtered tokens\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Bravo, Sherlock! You've cracked the case of the convoluted text. Now, you have a clean, processed list of words that you can use to analyze the text. You're officially a master in text preprocessing!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction: [1. 0. 0. 0. 0.]\n",
      "Non-fiction: [0. 1. 0. 0. 0.]\n",
      "Biography: [0. 0. 1. 0. 0.]\n",
      "Children: [0. 0. 0. 1. 0.]\n",
      "Mystery: [0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! The output matrix represents the presence of genres in a binary format. This type of encoding allows machines to better understand and use the genre data for various tasks, such as predicting book popularity or making book recommendations.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "One-hot encoded book titles\n",
    "\n",
    "PyBooks wants to catalog and analyze the book genres in its library. Apply one-hot encoding to a list of book genres to make them machine-readable.\n",
    "\n",
    "torch has been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the size of the vocabulary and save to vocab_size.\n",
    "\n",
    "    Create one-hot vectors using the appropriate torch technique and vocab_size.\n",
    "\n",
    "    Create a dictionary mapping genres to their corresponding one-hot vectors using dictionary comprehension; the dictionary keys should be the genre.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import torch\n",
    "\n",
    "genres = ['Fiction','Non-fiction','Biography', 'Children','Mystery']\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocab_size = len(genres)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "\n",
    "# Create a dictionary mapping genres to their one-hot vectors\n",
    "one_hot_dict = {genre: one_hot_vectors[i] for i, genre in enumerate(genres)}\n",
    "\n",
    "for genre, vector in one_hot_dict.items():\n",
    "    print(f'{genre}: {vector.numpy()}')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! The output matrix represents the presence of genres in a binary format. This type of encoding allows machines to better understand and use the genre data for various tasks, such as predicting book popularity or making book recommendations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1984' 'catcher' 'expectations' 'gatsby' 'great']\n",
      "[0 0 0 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFantastic work! The output matrix provides a clear picture of the word frequencies in the book titles. By analyzing the output, you can identify the frequency of words like 'catcher' and 'great' in the titles. The word frequency feature vectors can be used later by machine learning algorithms.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Bag-of-words for book titles\n",
    "\n",
    "PyBooks now has a list of book titles that need to be encoded for further analysis. The data team believes the Bag of Words (BoW) model could be the best approach.\n",
    "\n",
    "The following packages have been imported for you: torch, torchtext.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the CountVectorizer class for implementing bag-of-words.\n",
    "\n",
    "    Initialize an object of the class you imported, then use this object to transform the titles into a matrix representation.\n",
    "\n",
    "    Extract and display the first five feature names and encoded titles with the get_feature_names_out() method.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "titles = ['The Great Gatsby','To Kill a Mockingbird','1984','The Catcher in the Rye','The Hobbit', 'Great Expectations']\n",
    "\n",
    "# Initialize Bag-of-words with the list of book titles\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_titles = vectorizer.fit_transform(titles)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(bow_encoded_titles.toarray()[0, :5])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fantastic work! The output matrix provides a clear picture of the word frequencies in the book titles. By analyzing the output, you can identify the frequency of words like 'catcher' and 'great' in the titles. The word frequency feature vectors can be used later by machine learning algorithms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = ['A portrait of the Jazz Age in all of its decadence and excess.',\n",
    " 'A gripping, heart-wrenching, and wholly remarkable tale of coming-of-age in a South poisoned by virulent prejudice.',\n",
    " 'A startling and haunting vision of the world.',\n",
    " 'A story of lost innocence.',\n",
    " 'A timeless adventure story.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure' 'age' 'all' 'and' 'by']\n",
      "[0.         0.25943581 0.321564   0.21535516 0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! By examining the feature names and their corresponding TF-IDF values, you can uncover significant words that contribute to the uniqueness and relevance of each book. Your team is excited about the insights gained from your analysis. Keep up the great work!\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Applying TF-IDF to book descriptions\n",
    "\n",
    "PyBooks has collected several book descriptions and wants to identify important words within them using the TF-IDF encoding technique. By doing this, they hope to gain more insights into the unique attributes of each book to help with their book recommendation system.\n",
    "\n",
    "The following packages have been imported for you: torch, torchtext.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the class from sklearn.feature_extraction.text that converts a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "    Instantiate an object of this class, then use this object to encode the descriptions into a TF-IDF matrix of vectors.\n",
    "\n",
    "    Retrieve and display the first five feature names from the vectorizer and encoded vectors from tfidf_encoded_descriptions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Importing TF-IDF from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF encoding vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded_descriptions = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(tfidf_encoded_descriptions.toarray()[0, :5])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! By examining the feature names and their corresponding TF-IDF values, you can uncover significant words that contribute to the uniqueness and relevance of each book. Your team is excited about the insights gained from your analysis. Keep up the great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Complete Works\n"
     ]
    }
   ],
   "source": [
    "with open(path_data+'100-0.txt','r') as f:\n",
    "    text_data = f.read()\n",
    "    f.close()\n",
    "\n",
    "print(text_data[:50])\n",
    "shakespeare = text_data.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffthe project gutenberg ebook complet work william shakespear , william shakespear ebook use anyon anywher unit state part world cost almost restrict whatsoev', 'may copi , give away re-us term project gutenberg licens includ ebook onlin www', 'gutenberg', 'org', 'locat unit state , check law countri locat use ebook']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice job! You have successfully preprocessed the sentences and prepared them for encoding. Now you have a clean and transformed dataset to work with for the next step.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Shakespearean language preprocessing pipeline\n",
    "\n",
    "Over at PyBooks, the team wants to transform a vast library of Shakespearean text data for further analysis. The most efficient way to do this is with a text processing pipeline, starting with the preprocessing steps.\n",
    "\n",
    "The following have been loaded for you: torch, nltk, stopwords, PorterStemmer, get_tokenizer.\n",
    "\n",
    "The Shakespearean text data is saved as shakespeare and the sentences have already been extracted.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a list of unique English stopwords, saving to them to stop_words.\n",
    "---\n",
    "\n",
    "    Initialize the basic_english tokenizer from torch, and PorterStemmer from nltk.\n",
    "---\n",
    "\n",
    "    Complete the preprocess_sentences() function to enable tokenization, stop word removal, and stemming.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Complete the function to preprocess sentences\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "processed_shakespeare = preprocess_sentences(shakespeare)\n",
    "print(processed_shakespeare[:5]) \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job! You have successfully preprocessed the sentences and prepared them for encoding. Now you have a clean and transformed dataset to work with for the next step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000' '10' '100' '1000' '1004' '1009' '101' '1012' '1016' '102']\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You have successfully encoded the Shakespearean text data, and made it useful for your publishing company. The first ten feature representations of the first sentence in your batched data provides a numerical representation, enabling analysis and modeling of the Shakespearean language.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Shakespearean language encoder\n",
    "\n",
    "With the preprocessed Shakespearean text at your fingertips, you now need to encode it into a numerical representation. You will need to define the encoding steps before putting the pipeline together. To better handle large amounts of data and efficiently perform the encoding, you will use PyTorch's Dataset and DataLoader for batching and shuffling the data.\n",
    "\n",
    "The following has been loaded for you: torch, nltk, stopwords, PorterStemmer, get_tokenizer, CountVectorizer, Dataset, and DataLoader.\n",
    "\n",
    "The processed_shakespeare from the Shakespearean text is also available to you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a ShakespeareDataset dataset class and complete the __init__ and __getitem__ methods.\n",
    "---\n",
    "\n",
    "    Complete the encode_sentences() function to take in a list of sentences and encode them using the bag-of-words technique from sklearn.\n",
    "---\n",
    "\n",
    "    Complete and call the text_processing_pipeline() function by using preprocess_sentences(), encode_sentences(), ShakespeareDataset class, and DataLoader.\n",
    "\n",
    "    Print the first ten feature names with the get_feature_names_out() method and components of the first item of dataloader.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define your Dataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Complete the encoding function\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer\n",
    "    \n",
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = ShakespeareDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer\n",
    "\n",
    "dataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n",
    "\n",
    "# Print the vectorizer's feature names and the first 10 components of the first item\n",
    "print(vectorizer.get_feature_names_out()[:10]) \n",
    "print(next(iter(dataloader))[0] [:10])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You have successfully encoded the Shakespearean text data, and made it useful for your publishing company. The first ten feature representations of the first sentence in your batched data provides a numerical representation, enabling analysis and modeling of the Shakespearean language.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
