{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Machine_Translation_in_Python/datasets/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[('I', [1.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0]), ('cats', [0.0, 0.0, 1.0])]\n",
      "[('I', [1.0, 0.0, 0.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0, 0.0, 0.0]), ('cats', [0.0, 0.0, 1.0, 0.0, 0.0])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! Now you know how to convert words to onehot vectors and what the one-hot encoded vectors look like. This is an important base transformation of words for many NLP based deep learning models.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Understanding one-hot vectors\n",
    "\n",
    "Here you will learn to generate one-hot encoded vectors from words. One-hot encoding is a common transformation applied to words to represent them numerically.\n",
    "\n",
    "You will be using the Keras to_categorical() function to create one-hot vectors. The to_categorical() function expects a sequence of integers as the input. Therefore, a word2index dictionary is provided which can be used to convert a word to an integer.\n",
    "\n",
    "To successfully complete this exercise you will also have to use the built-in Python zip() function. The zip() function allows you to iterate multiple things at once. For example if you have two lists xx and yy of same length, by calling for x,y in zip(xx,yy) you can access each x and y elements of the lists iteratively.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a list of words I, like, cats and convert each word to an integer using word2index. Print the resulting integers.\n",
    "---\n",
    "\n",
    "    Convert the words to one-hot vectors using the Keras to_categorical() function.\n",
    "---\n",
    "\n",
    "    Print the words and their corresponding one-hot vectors using the zip() function.\n",
    "    Convert words to one-hot vectors using the number of classes as 5, assign it to the variable onehot_2 and print the result.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "word2index = {'I': 0, 'like': 1, 'cats': 2}\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Create a list of words and convert them to indices\n",
    "words = [\"I\", \"like\", \"cats\"]\n",
    "word_ids = [word2index[w] for w in words]\n",
    "print(word_ids)\n",
    "\n",
    "# Create onehot vectors using to_categorical function\n",
    "onehot_1 = to_categorical(word_ids)\n",
    "# Print words and their corresponding onehot vectors\n",
    "print([(w,ohe.tolist()) for w,ohe in zip(words, onehot_1)])\n",
    "\n",
    "# Create onehot vectors with a fixed number of classes and print the result\n",
    "onehot_2 = to_categorical(word_ids, num_classes=5)\n",
    "\n",
    "print([(w,ohe.tolist()) for w,ohe in zip(words, onehot_2)])\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! Now you know how to convert words to onehot vectors and what the one-hot encoded vectors look like. This is an important base transformation of words for many NLP based deep learning models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! Now you understand how to compute the length of produced onehot vectors. In the next exercise you will use this function to see an undesired behavior of the to_categorical() function, if you are not careful to set the num_classes argument.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Exploring the to_categorical() function\n",
    "\n",
    "Did you know that in real-world problems, the vocabulary size can grow very large (e.g. more than hundred thousand)?\n",
    "\n",
    "This exercise is broken into two parts and you will learn the importance of setting the num_classes argument of the to_categorical() function. In part 1, you will implement the function compute_onehot_length() that generates one-hot vectors for a given list of words and computes the length of those vectors.\n",
    "\n",
    "The to_categorical() function has already been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create word IDs by using words and word2index in compute_onehot_length().\n",
    "    Create onehot vectors using the to_categorical() function using the word IDs.\n",
    "    Return the length of a single onehot vector using the <array>.shape syntax.\n",
    "    Compute and print the length of onehot vectors using compute_onehot_length() for the list of words He, drank, milk.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def compute_onehot_length(words, word2index):\n",
    "  # Create word IDs for words\n",
    "  word_ids = [word2index[w] for w in words]\n",
    "  # Convert word IDs to onehot vectors\n",
    "  onehot = to_categorical(word_ids)\n",
    "  # Return the length of a single one-hot vector\n",
    "  return onehot.shape[1]\n",
    "\n",
    "word2index = {\"He\":0, \"drank\": 1, \"milk\": 2}\n",
    "# Compute and print onehot length of a list of words\n",
    "print(compute_onehot_length(['He','drank','milk'], word2index))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! Now you understand how to compute the length of produced onehot vectors. In the next exercise you will use this function to see an undesired behavior of the to_categorical() function, if you are not careful to set the num_classes argument.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_1 => 9  and length_2 =>  6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! Now you understand why it is important to pay attention to the num_classes argument when using the to_categorical() function. Deep learning models expect one-hot vectors to always have the same length. Leaving this argument unchecked can lead to ill-defined inputs and various compilation errors when fed to Keras models.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Exploring the to_categorical() function\n",
    "\n",
    "In part 1, you implemented the compute_onehot_length() function which did not use the num_classes argument while computing onehot vectors.\n",
    "\n",
    "The num_classes argument controls the length of the one-hot encoded vectors produced by the to_categorical() function. You will see that in situations where you have two different corpora (i.e. collections of texts) with different vocabularies, leaving the num_classes undefined can result in one-hot vectors of varying length.\n",
    "\n",
    "For this exercise, the compute_onehot_length() function and the word2index dictionary have been provided.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Call compute_onehot_length() on words_1.\n",
    "    Call compute_onehot_length() on words_2.\n",
    "    Print the lengths of one-hot vectors obtained for words_1 and words_2.\n",
    "\n",
    "\"\"\"\n",
    "word2index = {'I': 0, 'like': 4, 'cats': 2, 'We': 3, 'dogs': 5, 'He': 6, 'hates': 7, 'rabbits': 8}\n",
    "# solution\n",
    "\n",
    "words_1 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"He\", \"hates\", \"rabbits\"]\n",
    "# Call compute_onehot_length on words_1\n",
    "length_1 = compute_onehot_length(words_1, word2index)\n",
    "\n",
    "words_2 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"We\", \"like\", \"cats\"]\n",
    "# Call compute_onehot_length on words_2\n",
    "length_2 = compute_onehot_length(words_2, word2index)\n",
    "\n",
    "# Print length_1 and length_2\n",
    "print(\"length_1 =>\", length_1, \" and length_2 => \", length_2)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! Now you understand why it is important to pay attention to the num_classes argument when using the to_categorical() function. Deep learning models expect one-hot vectors to always have the same length. Leaving this argument unchecked can lead to ill-defined inputs and various compilation errors when fed to Keras models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', [1.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0]), ('cats', [0.0, 0.0, 1.0])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You just implemented a helper function that converts words to onehot vectors which will be fed to the encoder function. Next you will implement the encoder function.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Text reversing model - Encoder\n",
    "\n",
    "Creating a simple text reversing model is a great method to understand the mechanics of encoder decoder models and how they connect. You will now implement the encoder part of a text reversing model.\n",
    "\n",
    "The implementation of the encoder has been split over two exercises. In this exercise, you will be defining the words2onehot() helper function. The words2onehot() function should take in a list of words and a dictionary word2index and convert the list of words to an array of one-hot vectors. The word2index dictionary is available in the workspace.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Convert words to IDs using the word2index dictionary in the words2onehot() function\n",
    "    Convert word IDs to onehot vectors having length 3 (using the num_classes argument) and return the resulting array.\n",
    "    Call the words2onehot() function with the words I, like and cats and assign the result to onehot.\n",
    "    Print the words and their corresponding onehot vectors using print() and zip() functions. The zip() function allows you to iterate multiple lists at the same time.\n",
    "\n",
    "\"\"\"\n",
    "word2index = {'I': 0, 'like': 1, 'cats': 2}\n",
    "# solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def words2onehot(word_list, word2index):\n",
    "  # Convert words to word IDs\n",
    "  word_ids = [word2index[w] for w in word_list]\n",
    "  # Convert word IDs to onehot vectors and return the onehot array\n",
    "  onehot = to_categorical(word_ids, num_classes=3)\n",
    "  return onehot\n",
    "\n",
    "words = [\"I\", \"like\", \"cats\"]\n",
    "# Convert words to onehot vectors using words2onehot\n",
    "onehot = words2onehot(words, word2index)\n",
    "# Print the result as (<word>, <onehot>) tuples\n",
    "print([(w,ohe.tolist()) for w,ohe in zip(words, onehot)])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You just implemented a helper function that converts words to onehot vectors which will be fed to the encoder function. Next you will implement the encoder function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You just implemented the encoder of a text reversing model. Next you will implement the decoder that feeds on the output produced by the encoder.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Text reversing model - Encoder\n",
    "\n",
    "You will now implement the rest of the encoder of the text reversing model. The encoder feeds on the one-hot vectors produced by the words2onehot() function you implemented previously.\n",
    "\n",
    "Here you will be implementing the encoder() function. The encoder() function takes in a set of one-hot vectors and converts them to a list of word ids.\n",
    "\n",
    "For this exercise, the words2onehot() function and the word2index dictionary (having the words We, like and dogs) have been provided.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Convert onehot to an array of word IDs using np.argmax() function and return the word IDs.\n",
    "    Define a list of words with words We, like, dogs.\n",
    "    Convert the list of words to onehot vectors using the words2onehot() function. Remember that words2onehot() takes a list of words and a Python dictionary as its arguments.\n",
    "    Get the context vector of the onehot vectors using the encoder() function.\n",
    "\n",
    "\"\"\"\n",
    "word2index = {'We': 0, 'like': 1, 'dogs': 2}\n",
    "# solution\n",
    "\n",
    "def encoder(onehot):\n",
    "  # Get word IDs from onehot vectors and return the IDs\n",
    "  word_ids = np.argmax(onehot, axis=1)\n",
    "  return word_ids\n",
    "\n",
    "# Define \"We like dogs\" as words\n",
    "words = ['We','like','dogs']\n",
    "# Convert words to onehot vectors using words2onehot\n",
    "onehot = words2onehot(words, word2index)\n",
    "# Get the context vector by using the encoder function\n",
    "context = encoder(onehot)\n",
    "print(context)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You just implemented the encoder of a text reversing model. Next you will implement the decoder that feeds on the output produced by the encoder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'like', 'We']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You just implemented a text reversing encoder-decoder model. It might look like a very simple model, but it captures most of the high-level details of a complex encoder-decoder model used for machine translation. Next you will dive even deeper into encoder-decoder models and learn about the actual machine learning models used in them.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Complete text reversing model\n",
    "\n",
    "You will now implement the decoder part of the text reversing model, which will convert the context vector from the encoder to reversed words.\n",
    "\n",
    "You will be defining two functions onehot2words() and decoder(). The onehot2words() function takes in a list of ids and a dictionary index2word and converts an array of one-hot vectors to a list of words. The decoder() function takes in the context vector (i.e., list of word ids) and converts it to the reversed list of words.\n",
    "\n",
    "For this exercise, the index2word dictionary, the context vector context, the encoder() function and the words2onehot() functions will be provided.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the onehot2words() function, which obtains word IDs from one-hot vectors and then convert the IDs to words.\n",
    "---\n",
    "\n",
    "    Define the decoder() function which reverses the word IDs in the context vector and convert reversed word IDs to one-hot vectors.\n",
    "---\n",
    "\n",
    "    Get the one-hot vectors of the reversed word IDs using the decoder() function.\n",
    "    Get the reversed words from the one-hot vectors using the onehot2words() function.\n",
    "\n",
    "\"\"\"\n",
    "index2word = {0: 'We', 1: 'like', 2: 'dogs'}\n",
    "context = np.array([0,1,2])\n",
    "# solution\n",
    "\n",
    "# Define the onehot2words function that returns words for a set of onehot vectors\n",
    "def onehot2words(onehot, index2word):\n",
    "  ids = np.argmax(onehot, axis=1)\n",
    "  res = [index2word[id] for id in ids]\n",
    "  return res\n",
    "# Define the decoder function that returns reversed onehot vectors\n",
    "def decoder(context_vector):\n",
    "  word_ids_rev = context_vector[::-1]\n",
    "  onehot_rev = to_categorical(word_ids_rev, num_classes=3)\n",
    "  return onehot_rev\n",
    "# Convert context to reversed onehot vectors using decoder\n",
    "onehot_rev = decoder(context)\n",
    "# Get the reversed words using the onehot2words function\n",
    "reversed_words = onehot2words(onehot_rev, index2word)\n",
    "print(reversed_words)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You just implemented a text reversing encoder-decoder model. It might look like a very simple model, but it captures most of the high-level details of a complex encoder-decoder model used for machine translation. Next you will dive even deeper into encoder-decoder models and learn about the actual machine learning models used in them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 09:55:56.799801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-31 09:55:56.805790: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-31 09:55:56.811947: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-31 09:55:57.847185: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-31 09:55:57.853141: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-31 09:55:57.859214: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "shape (y) = (2, 10) \n",
      "y = \n",
      " [[ 0.18541092  0.17313942  0.4110512  -0.04944034  0.00204401  0.19214869\n",
      "   0.13092482  0.18884587  0.12573826 -0.73093307]\n",
      " [-0.01298199 -0.09763462 -0.2074138  -0.0813472   0.00246103 -0.13516933\n",
      "  -0.07658932 -0.06963581  0.00603478  0.13343216]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You just implemented a model that has two layers: an input layer and a GRU layer. Then you used that model to make predictions. These are components that will help you to build more complex machine translation models as we will see in the later chapters.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Understanding GRU models\n",
    "\n",
    "Did you know these models can remember even up to thousands of time steps compared to standard recurrent neural networks which can usually remember less than hundred time steps only. Understanding GRU models is essential to use them effectively to implement machine translation models.\n",
    "\n",
    "In this exercise, you will implement a simple model that has an input layer and a GRU layer. You will then use the model to produce output values for a random input array.\n",
    "\n",
    "Don't be discouraged that you are using random data. The objective of this exercise is to understand the shape of the outputs produced by the GRU layer. In later chapters, you will feed in actual sentences to GRU layers to perform translation.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a Keras input layer with batch size 2, sequence length 3 and input dimensionality 4. Remember that you can define an input layer using the keras.layers.Input(<argument>=<value>) syntax.\n",
    "---\n",
    "\n",
    "    Define a Keras GRU layer that has 10 hidden units and feeds on the previous input layer. You can use the keras.layers.GRU(<parameter>) to define a GRU layer.\n",
    "---\n",
    "\n",
    "    Define a Keras model called model where the input is the input layer and the output is the output of the GRU layer.\n",
    "    Generate the predictions for a given set of random values x.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "# Define an input layer\n",
    "inp = keras.layers.Input(batch_shape=(2,3,4))\n",
    "# Define a GRU layer that takes in the input\n",
    "gru_out = keras.layers.GRU(10)(inp)\n",
    "\n",
    "# Define a model that outputs the GRU output\n",
    "model = keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "\n",
    "x = np.random.normal(size=(2,3,4))\n",
    "# Get the output of the model and print the result\n",
    "y = model.predict(x)\n",
    "print(\"shape (y) =\", y.shape, \"\\ny = \\n\", y)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You just implemented a model that has two layers: an input layer and a GRU layer. Then you used that model to make predictions. These are components that will help you to build more complex machine translation models as we will see in the later chapters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 10:00:09.346642: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-31 10:00:09.352721: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-31 10:00:09.357371: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-07-31 10:00:10.385989: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-31 10:00:10.393055: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-31 10:00:10.397581: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "shape (y1) =  (2, 10)  shape (y2) =  (5, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You now know quite a bit about sequential GRU models. You have already used them to implement a model and also know how to modify them to accept arbitrary sized inputs. In the next exercise you will learn about various arguments you can pass to the GRU layer to modify the output it produces.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Understanding GRU models\n",
    "\n",
    "You will now see how you can use Keras models to accept arbitrary sized batches of inputs. The ability to accept arbitrary sized batches is important for many reasons. For example, this allows you to define a single Keras model and experiment with different batch sizes during the model training stage, without having to change anything in the model.\n",
    "\n",
    "For this exercise, keras and numpy (as np) have already been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define an input layer that accepts an arbitrary sized batch of data having sequence length 3 and input size 4.\n",
    "    Define a GRU layer with 10 hidden units that consumes the previous input and produces an output.\n",
    "    Define a Model called model that takes the input layer as the input and produces the output of the GRU layer as the output. Remember that you can use the keras.models.Model(<argument>=<value>) syntax to define a model.\n",
    "    Predict the model output for both x1 and x2.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define an input layer\n",
    "inp = keras.layers.Input(shape=(3,4))\n",
    "# Define a GRU layer that takes in the input\n",
    "gru_out = keras.layers.GRU(10)(inp)\n",
    "# Define a model that outputs the GRU output\n",
    "model = keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "\n",
    "x1 = np.random.normal(size=(2,3,4))\n",
    "x2 = np.random.normal(size=(5,3,4))\n",
    "\n",
    "# Get the output of the model and print the result\n",
    "y1 = model.predict(x1)\n",
    "y2 = model.predict(x2)\n",
    "print(\"shape (y1) = \", y1.shape, \" shape (y2) = \", y2.shape)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You now know quite a bit about sequential GRU models. You have already used them to implement a model and also know how to modify them to accept arbitrary sized inputs. In the next exercise you will learn about various arguments you can pass to the GRU layer to modify the output it produces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 10:08:31.703888: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-31 10:08:31.712049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-31 10:08:31.719657: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_out1.shape =  (3, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 10:08:32.669994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-31 10:08:32.674384: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-31 10:08:32.679902: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_out2.shape =  (3, 10)\n",
      "gru_state.shape =  (3, 10)\n",
      "gru_out3.shape =  (3, 20, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 10:08:33.467387: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-31 10:08:33.472806: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-31 10:08:33.478945: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! Now you understand how the GRU layer works and how to manipulate its output. Knowing how to get the correct output is very important as we move on to more complex models.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Understanding sequential model output\n",
    "\n",
    "In this exercise you will learn to use the keras.layers.GRU layer. keras.layers.GRU nicely wraps the functionality of a GRU to a Layer object.\n",
    "\n",
    "You will explore what the shape of the output of a GRU layer looks like and how it changes when different arguments are provided. It is rare to view the numerical vectors produced by a GRU in real life, but in order to use these layers in more complex models, you need to have a good understanding of the shapes of the outputs and how to get the desired output using various arguments.\n",
    "\n",
    "Here you will have keras, and numpy (as np) loaded already. You can access layers by calling keras.layers.<Layer> or a model by calling keras.models.Model.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create an Input layer of batch size 3, 20 time steps and 5 dimensions and call it inp.\n",
    "    Create a GRU layer of hidden size 10, pass the inp to this layer and print the shape of the output.\n",
    "---\n",
    "\n",
    "    Create a new GRU layer with 10 hidden units, which returns the state and pass in inp as the input, assign the state to the gru_state, and print the shape of gru_out2 and gru_state.\n",
    "---\n",
    "\n",
    "    Create a new GRU layer with return_sequences=True, pass in inp as the input, and print the shape of the output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define the Input layer\n",
    "inp = keras.layers.Input(batch_shape=(3,20,5))\n",
    "# Define a GRU layer that takes in inp as the input\n",
    "gru_out1 = keras.layers.GRU(10)(inp)\n",
    "print(\"gru_out1.shape = \", gru_out1.shape)\n",
    "\n",
    "# Define the second GRU and print the shape of the outputs\n",
    "gru_out2, gru_state = keras.layers.GRU(10, return_state=True)(inp)\n",
    "print(\"gru_out2.shape = \", gru_out2.shape)\n",
    "print(\"gru_state.shape = \", gru_state.shape)\n",
    "\n",
    "# Define the third GRU layer which will return all the outputs\n",
    "gru_out3 = keras.layers.GRU(10, return_sequences=True)(inp)\n",
    "print(\"gru_out3.shape = \", gru_out3.shape)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! Now you understand how the GRU layer works and how to manipulate its output. Knowing how to get the correct output is very important as we move on to more complex models.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
