{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Machine_Translation_in_Python/datasets/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data + 'vocab_en.txt', 'r') as en:\n",
    "    en_text = en.read().split(\"\\n\")[:150]\n",
    "    en.close\n",
    "\n",
    "with open(path_data + 'vocab_fr.txt', 'r') as fr:\n",
    "    fr_text = fr.read().split(\"\\n\")[:150]\n",
    "    fr.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "january  has id:  48\n",
      "apples  has id:  83\n",
      "summer  has id:  69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! As you can see the tokenizer automates many operations we used to do manually in previous exercises. It's a handy utility for preprocessing text. And it gets better! You have many different options when defining the Tokenizer which provide you more control.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Tokenizing sentences with Keras\n",
    "\n",
    "Here you will get your hands dirty with the Keras Tokenizer. The Keras Tokenizer is a great utility that helps you to do some crucial text processing with a few lines of code. For example, the Keras Tokenizer will automatically map the words in your vocabulary to IDs with a single function call. Here, you will learn about this in more detail.\n",
    "\n",
    "You will be creating a Keras Tokenizer object and fitting it on some text, which will allow the Tokenizer to build a dictionary of words and their corresponding IDs. The text used to train the Tokenizer is obtained from the Udacity Github Repo.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a Keras Tokenizer object.\n",
    "    Fit the tokenizer on en_text.\n",
    "    Get the word ID for each word w in the given list [\"january\", \"apples\", \"summer\"].\n",
    "    Print the word and its corresponding ID.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a Keras Tokenizer\n",
    "en_tok = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on some text\n",
    "en_tok.fit_on_texts(en_text)\n",
    "\n",
    "for w in [\"january\", \"apples\", \"summer\"]:\n",
    "  # Get the word ID of word w\n",
    "  id = en_tok.word_index[w]\n",
    "  # Print the word and the word ID\n",
    "  print(w, \" has id: \", id)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! As you can see the tokenizer automates many operations we used to do manually in previous exercises. It's a handy utility for preprocessing text. And it gets better! You have many different options when defining the Tokenizer which provide you more control.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ID sequence:  [[28, 80, 22, 71, 5, 61]]\n",
      "Word ID sequence (with UNK):  [[29, 1, 23, 1, 6, 1]]\n",
      "The ID 1 represents the word:  UNK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIt seems that the out-of-Vocabulary words are replaced with UNK as predicted. But remember that, for real world systems like Google machine translator, simply replacing unknown words with a special token is not an option. They use sophisticated methods (e.g. using character level models to model unknown words) to deal with this problem.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Controlling the vocabulary with the Tokenizer\n",
    "\n",
    "Let's drill down a bit more into the operation of the Tokenizer. In this exercise you will learn how to convert an arbitrary sentence to a sequence using a trained Tokenizer. Furthermore, you will learn to control the size of the vocabulary of the Tokenizer. You will also investigate what happens to the out-of-vocabulary (OOV) words when you limit the vocabulary size of a Tokenizer.\n",
    "\n",
    "For this exercise, you have been provided with the en_tok Tokenizer that you previously implemented. The Tokenizer has been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Convert the following sentence to a sequence using the previous en_tok Tokenizer: she likes grapefruit , peaches , and lemons .\n",
    "    Create a new Tokenizer, en_tok_new with a vocabulary size of 50 and out-of-vocabulary word UNK.\n",
    "    Fit the new tokenizer on en_text data.\n",
    "    Convert the sentence she likes grapefruit , peaches , and lemons . to a sequence with the en_tok_new.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Convert the sentence to a word ID sequence\n",
    "seq = en_tok.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n",
    "print('Word ID sequence: ', seq)\n",
    "\n",
    "# Define a tokenizer with vocabulary size 50 and oov_token 'UNK'\n",
    "en_tok_new = Tokenizer(num_words=50, oov_token='UNK')\n",
    "\n",
    "# Fit the tokenizer on en_text\n",
    "en_tok_new.fit_on_texts(en_text)\n",
    "\n",
    "# Convert the sentence to a word ID sequence\n",
    "seq_new = en_tok_new.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n",
    "print('Word ID sequence (with UNK): ', seq_new)\n",
    "print('The ID 1 represents the word: ', en_tok_new.index_word[1])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "It seems that the out-of-Vocabulary words are replaced with UNK as predicted. But remember that, for real world systems like Google machine translator, simply replacing unknown words with a special token is not an option. They use sophisticated methods (e.g. using character level models to model unknown words) to deal with this problem.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding tokens:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "After adding tokens:  sos new jersey est parfois calme pendant l' automne , et il est neigeux en avril . eos \n",
      "\n",
      "Before adding tokens:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "After adding tokens:  sos les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . eos \n",
      "\n",
      "Before adding tokens:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "After adding tokens:  sos california est généralement calme en mars , et il est généralement chaud en juin . eos \n",
      "\n",
      "Before adding tokens:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "After adding tokens:  sos les états-unis est parfois légère en juin , et il fait froid en septembre . eos \n",
      "\n",
      "Before adding tokens:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "After adding tokens:  sos votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme . eos \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! Now the French sentences have special tokens added to the beginning and the end. Though this step is optional for the model you have, this step will be quite beneficial to the model you will be implementing in chapter 4.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Adding special tokens\n",
    "\n",
    "You will now learn to add sos (marks the start) and eos (marks the end) tokens to the sentences. As already discussed, this step is optional for the model you have right now, but these will be required for a model that you'll be implementing in a later chapter.\n",
    "\n",
    "To add these special tokens, you will use the Python string.join() function. string.join() joins a list of strings to a single string using a delimiter. For example, if you want to convert ['datacamp', 'is', 'awesome'] to 'datacamp is awesome', you can use \" \".join(['datacamp', 'is', 'awesome']), where the \" \" (i.e. space character) is the delimiter.\n",
    "\n",
    "For this exercise, a small sample of 10 French sentences has already been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Loop through the list of French sentences (fr_text).\n",
    "    Add a \"sos\" token to denote the beginning and an \"eos\" token to denote the ending of each sentence using the string.join() function.\n",
    "    Append the modified sentence to fr_text_new.\n",
    "    Print the modified sentence sent_new.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "fr_text_new = []\n",
    "\n",
    "# Loop through all sentences in fr_text\n",
    "for sent in fr_text[:5]:\n",
    "  \n",
    "  print(\"Before adding tokens: \", sent)\n",
    "  \n",
    "  # Add sos and eos tokens using string.join\n",
    "  sent_new = \" \".join(['sos', sent, 'eos'])\n",
    "  # Append the modified sentence to fr_text_new\n",
    "  fr_text_new.append(sent_new)\n",
    "  \n",
    "  # Print sentence after adding tokens\n",
    "  print(\"After adding tokens: \", sent_new, '\\n')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Now the French sentences have special tokens added to the beginning and the end. Though this step is optional for the model you have, this step will be quite beneficial to the model you will be implementing in chapter 4.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNicely done! The function achieves most of the preprocessing that needs to be done before feeding the data to the model. This function will be very useful when you are training the model and generating translations from new data.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Padding sentences\n",
    "\n",
    "You will now implement a function called sents2seqs() which you will later use to transform data conveniently to the format accepted by the neural machine translation (NMT) model. sents2seqs() accepts a list of sentence strings and,\n",
    "\n",
    "    Converts the sentences to a list of sequence of IDs,\n",
    "    Pad the sentences so that they have equal length and,\n",
    "    Optionally convert the IDs to onehot vectors.\n",
    "\n",
    "You have been provided with en_tok, a Tokenizer already trained on data. Another thing to note is that when implementing sents2seqs() function you will see an unused argument called input_type. Later, this input_type will be used to change language dependent parameters such as the length of the sequence and size of the vocabulary.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Convert the sentences to sequences using the en_tok Tokenizer.\n",
    "    Pad sequences to a fixed en_len length with a specified padding type of pad_type and use post-truncating.\n",
    "    Convert the preproc_text word IDs to onehot vectors of length en_vocab using the to_categorical() function.\n",
    "    Convert sentence to a padded sequence using the sents2seqs() method using pre-padding.\n",
    "\n",
    "\"\"\"\n",
    "en_len = 15\n",
    "en_vocab = 150\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def sents2seqs(input_type, sentences, onehot=False, pad_type='post'):\n",
    "\t# Convert sentences to sequences      \n",
    "    encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "    # Pad sentences to en_len\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating=pad_type, maxlen=en_len)\n",
    "    if onehot:\n",
    "\t\t# Convert the word IDs to onehot vectors\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n",
    "    return preproc_text\n",
    "sentence = 'she likes grapefruit , peaches , and lemons .'  \n",
    "# Convert a sentence to sequence by pre-padding the sentence\n",
    "pad_seq = sents2seqs('source', [sentence], pad_type='pre')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nicely done! The function achieves most of the preprocessing that needs to be done before feeding the data to the model. This function will be very useful when you are training the model and generating translations from new data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tReversed:  july during rainy never is california\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You just covered all the preprocessing steps required before feeding data to the model. sent2seqs() is a handy function which you can conveniently call to transform data to the correct format. Note that these are only some of the fundamental preprocessing steps and there are many other things you can do to improve the performance of these models.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Reversing sentences\n",
    "\n",
    "Here you will learn how to reverse sentences for the encoder model. As discussed, reversing source sentences helps to form a strong initial connection between the encoder and the decoder, which boosts the performance of the model. However, always remember that the benefit is dependent on the two languages you are translating between. As long as they have the same subject, verb ,and object order, it will benefit the model.\n",
    "\n",
    "In this exercise you will modify the sents2seqs() function to be able to reverse sentences if needed. The user can specify a boolean keyword argument reverse which does text reversing.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write the sents2seqs() function signature by adding a new keyword argument reverse which defaults to False.\n",
    "    Reverse the returned sequence IDs on the time dimension (using ::-1 syntax), so that the first word ID becomes the last.\n",
    "    Call sents2seqs() and reverse the given sentences and keep all the other default parameters values unchanged.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "sentences = [\"california is never rainy during july .\"]\n",
    "# Add new keyword parameter reverse which defaults to False\n",
    "def sents2seqs(input_type, sentences, onehot=False, pad_type='post', reverse=False):     \n",
    "    encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n",
    "    if reverse:\n",
    "      # Reverse the text using numpy axis reversing\n",
    "      preproc_text = preproc_text[:, ::-1]\n",
    "    if onehot:\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n",
    "    return preproc_text\n",
    "# Call sents2seqs to get the padded and reversed sequence of IDs\n",
    "pad_seq = sents2seqs('source', sentences, reverse=True)\n",
    "rev_sent = [en_tok.index_word[wid] for wid in pad_seq[0][-6:]] \n",
    "print('\\tReversed: ',' '.join(rev_sent))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You just covered all the preprocessing steps required before feeding data to the model. sent2seqs() is a handy function which you can conveniently call to transform data to the correct format. Note that these are only some of the fundamental preprocessing steps and there are many other things you can do to improve the performance of these models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 13:58:35.879645: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 13:58:35.884333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 13:58:35.887931: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 15, 150)]    0           []                               \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 48)           28800       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 25, 48)       0           ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 25, 48)       14112       ['repeat_vector[0][0]',          \n",
      "                                                                  'gru[0][0]']                    \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 25, 125)     6125        ['gru_1[0][0]']                  \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 49,037\n",
      "Trainable params: 49,037\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 13:58:36.524132: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 13:58:36.528846: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 13:58:36.533625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, RepeatVector, TimeDistributed\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (15, 150)\n",
    "\n",
    "# Define the number of units in the GRU layer\n",
    "gru_units = 48\n",
    "\n",
    "# Define the number of output units\n",
    "output_units = 125\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=input_shape, name='input_1')\n",
    "\n",
    "# Define the GRU layer\n",
    "gru_layer = GRU(gru_units, name='gru')(input_layer)\n",
    "\n",
    "# Define the RepeatVector layer\n",
    "repeat_layer = RepeatVector(25, name='repeat_vector')(gru_layer)\n",
    "\n",
    "# Define the second GRU layer\n",
    "gru_layer_2 = GRU(gru_units, return_sequences=True, name='gru_1')(repeat_layer, initial_state=gru_layer)\n",
    "\n",
    "# Define the TimeDistributed layer to produce the final output\n",
    "output_layer = TimeDistributed(tf.keras.layers.Dense(output_units), name='time_distributed')(gru_layer_2)\n",
    "\n",
    "# Create the model\n",
    "nmt = Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "nmt.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "# Print the model summary\n",
    "nmt.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 13:58:39.350930: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [150,15,150]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-08-01 13:58:40.074071: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 13:58:40.078770: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 13:58:40.084051: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-08-01 13:58:40.794539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 13:58:40.800340: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 13:58:40.805490: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (150, 15, 150) and (150, 25, 125) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m de_y \u001b[39m=\u001b[39m sents2seqs(\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m, fr_text[i:i\u001b[39m+\u001b[39mbsize], onehot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Train the model on a single batch of data\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m nmt\u001b[39m.\u001b[39;49mtrain_on_batch(en_x, de_y) \n\u001b[1;32m     36\u001b[0m \u001b[39m# Obtain the eval metrics for the training data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m res \u001b[39m=\u001b[39m nmt\u001b[39m.\u001b[39mevaluate(en_x, de_y, batch_size\u001b[39m=\u001b[39mbsize, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:2510\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2507\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2508\u001b[0m     )\n\u001b[1;32m   2509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2510\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   2512\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file3njtn52r.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1270\u001b[0m     outputs,\n\u001b[1;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1273\u001b[0m )\n\u001b[1;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1051\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m   1050\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1051\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[1;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1109\u001b[0m, in \u001b[0;36mModel.compute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the total loss, validate it, and return it.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \n\u001b[1;32m   1060\u001b[0m \u001b[39mSubclasses can optionally override this method to provide custom loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m  is the case when called by `Model.test_step`).\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mdel\u001b[39;00m x  \u001b[39m# The default implementation does not use `x`.\u001b[39;00m\n\u001b[0;32m-> 1109\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompiled_loss(\n\u001b[1;32m   1110\u001b[0m     y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlosses\n\u001b[1;32m   1111\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py:265\u001b[0m, in \u001b[0;36mLossesContainer.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    263\u001b[0m y_t, y_p, sw \u001b[39m=\u001b[39m match_dtype_and_rank(y_t, y_p, sw)\n\u001b[1;32m    264\u001b[0m sw \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mapply_mask(y_p, sw, losses_utils\u001b[39m.\u001b[39mget_mask(y_p))\n\u001b[0;32m--> 265\u001b[0m loss_value \u001b[39m=\u001b[39m loss_obj(y_t, y_p, sample_weight\u001b[39m=\u001b[39;49msw)\n\u001b[1;32m    267\u001b[0m total_loss_mean_value \u001b[39m=\u001b[39m loss_value\n\u001b[1;32m    268\u001b[0m \u001b[39m# Correct for the `Mean` loss metrics counting each replica as a\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py:142\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     call_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    139\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0m losses \u001b[39m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[1;32m    144\u001b[0m in_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(y_pred)\n\u001b[1;32m    145\u001b[0m out_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(losses)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py:268\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    261\u001b[0m     y_pred, y_true \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    262\u001b[0m         y_pred, y_true\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    265\u001b[0m ag_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m \u001b[39mreturn\u001b[39;00m ag_fn(y_true, y_pred, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fn_kwargs)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py:1984\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   1976\u001b[0m     \u001b[39mreturn\u001b[39;00m y_true \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m label_smoothing) \u001b[39m+\u001b[39m (\n\u001b[1;32m   1977\u001b[0m         label_smoothing \u001b[39m/\u001b[39m num_classes\n\u001b[1;32m   1978\u001b[0m     )\n\u001b[1;32m   1980\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39msmart_cond\u001b[39m.\u001b[39msmart_cond(\n\u001b[1;32m   1981\u001b[0m     label_smoothing, _smooth_labels, \u001b[39mlambda\u001b[39;00m: y_true\n\u001b[1;32m   1982\u001b[0m )\n\u001b[0;32m-> 1984\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcategorical_crossentropy(\n\u001b[1;32m   1985\u001b[0m     y_true, y_pred, from_logits\u001b[39m=\u001b[39;49mfrom_logits, axis\u001b[39m=\u001b[39;49maxis\n\u001b[1;32m   1986\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py:5559\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   5557\u001b[0m target \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m   5558\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m-> 5559\u001b[0m target\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49massert_is_compatible_with(output\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m   5561\u001b[0m output, from_logits \u001b[39m=\u001b[39m _get_logits(\n\u001b[1;32m   5562\u001b[0m     output, from_logits, \u001b[39m\"\u001b[39m\u001b[39mSoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5563\u001b[0m )\n\u001b[1;32m   5564\u001b[0m \u001b[39mif\u001b[39;00m from_logits:\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (150, 15, 150) and (150, 25, 125) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Training the model\n",
    "\n",
    "You will train the previously implemented model in this exercise. Do you know that the Google's encoder-decoder based machine translation model took 2-4 days to train?\n",
    "\n",
    "For this exercise you will be using a small dataset of 1500 sentences (i.e. en_text and fr_text) to train the model. This amount will hardly be enough to see good performance, but the method will remain the same. It is a matter of training on more data for longer. You have also been provided with the model nmt, and sents2seqs() function that you implemented previously. You will be reversing the encoder text to get better performance in this exercise. Here, en_x refers to the encoder input, where de_x refers to the decoder input.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Get a single batch of encoder inputs (English sentences from index i to i+bsize) using the sents2seqs() function. Inputs need to be reversed and onehot encoded.\n",
    "    Get a single batch of decoder outputs (French sentences from index i to i+bsize) using the sents2seqs() function. Inputs need to be onehot encoded.\n",
    "    Train the model on a single batch of data containing en_x and de_y.\n",
    "    Obtain the evaluation metrics for en_x and de_y by evaluating the model with a batch_size of bsize.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "n_epochs, bsize = 3, 250\n",
    "data_size = len(en_text)\n",
    "\n",
    "for ei in range(n_epochs):\n",
    "  for i in range(0,data_size,bsize):\n",
    "    # Get a single batch of encoder inputs\n",
    "    en_x = sents2seqs('source', en_text[i:i+bsize], onehot=True, reverse=True)\n",
    "    # Get a single batch of decoder outputs\n",
    "    de_y = sents2seqs('target', fr_text[i:i+bsize], onehot=True)\n",
    "    \n",
    "    # Train the model on a single batch of data\n",
    "    nmt.train_on_batch(en_x, de_y) \n",
    "    # Obtain the eval metrics for the training data\n",
    "    res = nmt.evaluate(en_x, de_y, batch_size=bsize, verbose=0)\n",
    "    print(\"{} => Train Loss:{}, Train Acc: {}\".format(ei+1,res[0], res[1]*100.0))  \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! This is how you would train a machine translation model. You iterate several epochs, where in each epoch you traverse the full dataset batch by batch. However in order to make sure the model performs well and does not overfit, training needs to happen in joint with a validation step. You will create datasets for that in the next exercise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (EN):\n",
      " ['new jersey is sometimes nice during winter , but it is hot in spring .', 'i like peaches , grapes , and lemons .', 'she likes strawberries , oranges , and bananas .'] \n",
      "Training (FR):\n",
      " [\"new jersey est parfois agréable pendant l' hiver , mais il est chaud au printemps .\", \"j'aime les pêches , les raisins et les citrons .\", 'elle aime les fraises , les oranges et les bananes .']\n",
      "\n",
      "Valid (EN):\n",
      " [] \n",
      "Valid (FR):\n",
      " []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! You now have two datasets, a training dataset and a validation dataset. You'll be using the training dataset to train the model as you would normally, while monitoring the accuracy on the validation dataset. This is good because when you see the accuracy on the validation dataset dropping, you can stop the training and prevent overfitting.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Splitting data to training and validation sets\n",
    "\n",
    "You learned that using only the training data without a validation dataset leads to a problem called overfitting. When overfitting occurs, the model will be very good at predicting data for training inputs, however generalize very poorly to unseen data. This means the model will not be very useful, as it cannot generalize. To avoid this you can use a validation dataset.\n",
    "\n",
    "In this exercise, you will create a training and validation set from the dataset you have (i.e. en_text containing 1000 English sentences and fr_text containing the 1000 French sentences). You will be using 80% of the dataset for training data and 20% as validation data.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a sequence of indices using np.arange(), that starts with 0 and has size of en_text.\n",
    "    Define valid_inds as the last valid_size indices from the sequence of indices.\n",
    "    Define tr_en and tf_fr, which contains the sentences found at train_inds indices, in the lists en_text and fr_text.\n",
    "    Define v_en and v_fr which contains the sentences found at valid_inds indices, in the lists en_text and fr_text.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# solution\n",
    "\n",
    "train_size, valid_size = 800, 200\n",
    "# Define a sequence of indices from 0 to len(en_text)\n",
    "inds = np.arange(len(en_text))\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:train_size]\n",
    "# Define valid_inds: last valid_size indices\n",
    "valid_inds = inds[train_size:]\n",
    "# Define tr_en (train EN sentences) and tr_fr (train FR sentences)\n",
    "tr_en = [en_text[ti] for ti in train_inds]\n",
    "tr_fr = [fr_text[ti] for ti in train_inds]\n",
    "# Define v_en (valid EN sentences) and v_fr (valid FR sentences)\n",
    "v_en = [en_text[vi] for vi in valid_inds]\n",
    "v_fr = [fr_text[vi] for vi in valid_inds]\n",
    "print('Training (EN):\\n', tr_en[:3], '\\nTraining (FR):\\n', tr_fr[:3])\n",
    "print('\\nValid (EN):\\n', v_en[:3], '\\nValid (FR):\\n', v_fr[:3])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You now have two datasets, a training dataset and a validation dataset. You'll be using the training dataset to train the model as you would normally, while monitoring the accuracy on the validation dataset. This is good because when you see the accuracy on the validation dataset dropping, you can stop the training and prevent overfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 13:58:52.706492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [150,15,150]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-08-01 13:58:53.400382: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 13:58:53.406420: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 13:58:53.413948: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-08-01 13:58:54.098577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 13:58:54.103789: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 13:58:54.108302: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (150, 15, 150) and (150, 25, 125) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m   de_y \u001b[39m=\u001b[39m sents2seqs(\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m, tr_fr[i:i\u001b[39m+\u001b[39mbsize], onehot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m   \u001b[39m# Train the model on a single batch of data\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m   nmt\u001b[39m.\u001b[39;49mtrain_on_batch(en_x, de_y)    \n\u001b[1;32m     38\u001b[0m \u001b[39m# Evaluate the trained model on the validation data\u001b[39;00m\n\u001b[1;32m     39\u001b[0m res \u001b[39m=\u001b[39m nmt\u001b[39m.\u001b[39mevaluate(v_en_x, v_de_y, batch_size\u001b[39m=\u001b[39mvalid_size, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:2510\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2507\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2508\u001b[0m     )\n\u001b[1;32m   2509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2510\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   2512\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file3njtn52r.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1270\u001b[0m     outputs,\n\u001b[1;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1273\u001b[0m )\n\u001b[1;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1051\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m   1050\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1051\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[1;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py:1109\u001b[0m, in \u001b[0;36mModel.compute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the total loss, validate it, and return it.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \n\u001b[1;32m   1060\u001b[0m \u001b[39mSubclasses can optionally override this method to provide custom loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m  is the case when called by `Model.test_step`).\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mdel\u001b[39;00m x  \u001b[39m# The default implementation does not use `x`.\u001b[39;00m\n\u001b[0;32m-> 1109\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompiled_loss(\n\u001b[1;32m   1110\u001b[0m     y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlosses\n\u001b[1;32m   1111\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py:265\u001b[0m, in \u001b[0;36mLossesContainer.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    263\u001b[0m y_t, y_p, sw \u001b[39m=\u001b[39m match_dtype_and_rank(y_t, y_p, sw)\n\u001b[1;32m    264\u001b[0m sw \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mapply_mask(y_p, sw, losses_utils\u001b[39m.\u001b[39mget_mask(y_p))\n\u001b[0;32m--> 265\u001b[0m loss_value \u001b[39m=\u001b[39m loss_obj(y_t, y_p, sample_weight\u001b[39m=\u001b[39;49msw)\n\u001b[1;32m    267\u001b[0m total_loss_mean_value \u001b[39m=\u001b[39m loss_value\n\u001b[1;32m    268\u001b[0m \u001b[39m# Correct for the `Mean` loss metrics counting each replica as a\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py:142\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     call_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    139\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0m losses \u001b[39m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[1;32m    144\u001b[0m in_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(y_pred)\n\u001b[1;32m    145\u001b[0m out_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(losses)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py:268\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    261\u001b[0m     y_pred, y_true \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    262\u001b[0m         y_pred, y_true\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    265\u001b[0m ag_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m \u001b[39mreturn\u001b[39;00m ag_fn(y_true, y_pred, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fn_kwargs)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py:1984\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   1976\u001b[0m     \u001b[39mreturn\u001b[39;00m y_true \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m label_smoothing) \u001b[39m+\u001b[39m (\n\u001b[1;32m   1977\u001b[0m         label_smoothing \u001b[39m/\u001b[39m num_classes\n\u001b[1;32m   1978\u001b[0m     )\n\u001b[1;32m   1980\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39msmart_cond\u001b[39m.\u001b[39msmart_cond(\n\u001b[1;32m   1981\u001b[0m     label_smoothing, _smooth_labels, \u001b[39mlambda\u001b[39;00m: y_true\n\u001b[1;32m   1982\u001b[0m )\n\u001b[0;32m-> 1984\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcategorical_crossentropy(\n\u001b[1;32m   1985\u001b[0m     y_true, y_pred, from_logits\u001b[39m=\u001b[39;49mfrom_logits, axis\u001b[39m=\u001b[39;49maxis\n\u001b[1;32m   1986\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py:5559\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   5557\u001b[0m target \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m   5558\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m-> 5559\u001b[0m target\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49massert_is_compatible_with(output\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m   5561\u001b[0m output, from_logits \u001b[39m=\u001b[39m _get_logits(\n\u001b[1;32m   5562\u001b[0m     output, from_logits, \u001b[39m\"\u001b[39m\u001b[39mSoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5563\u001b[0m )\n\u001b[1;32m   5564\u001b[0m \u001b[39mif\u001b[39;00m from_logits:\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (150, 15, 150) and (150, 25, 125) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Training the model with validation\n",
    "\n",
    "Here you will learn how to train the neural machine translator model with a validation step.\n",
    "\n",
    "You are provided with the nmt model that you created in the last chapter. Furthermore, you will train the model on a English and French sentences obtained from the Udacity Github Repo. You are provided with training English text (tr_en) and French text (tf_fr) as well as validation English text (v_en) and French text (v_fr) from the previous exercise.\n",
    "\n",
    "Training the model takes a bit of time so your code will take a little longer to run.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create validation data by transforming v_en and v_fr using sents2seqs function.\n",
    "    Get a properly transformed batch of inputs and outputs using the sents2seqs function.\n",
    "    Use the inputs (en_x) and outputs (de_y) to train the nmt on a single batch.\n",
    "    Use v_en_x and v_de_y along with valid_size as batch_size to evaluate the nmt model and obtain the validation accuracy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Convert validation data to onehot\n",
    "v_en_x = sents2seqs('source', v_en, onehot=True, reverse=True)\n",
    "v_de_y = sents2seqs('target', v_fr, onehot=True)\n",
    "\n",
    "n_epochs, bsize = 3, 250\n",
    "for ei in range(n_epochs):\n",
    "  for i in range(0,train_size,bsize):\n",
    "    # Get a single batch of inputs and outputs\n",
    "    en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, reverse=True)\n",
    "    de_y = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n",
    "    # Train the model on a single batch of data\n",
    "    nmt.train_on_batch(en_x, de_y)    \n",
    "  # Evaluate the trained model on the validation data\n",
    "  res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "  print(\"{} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You now know how to train a machine translation model and evaluate it on a validation set as it trains. Here we only used 750 sentences. To get better results you will need to train this model on a lot more data for longer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction  1 :  go to the dragon cave\n",
      "Instruction  2 :  find the stone dragon\n",
      "Instruction  3 :  push the head down\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou just uncovered a great treasure full of gold and diamonds! But best of all you learned to decode a 3 dimensional onehot vector output and extract the sentences. This knowledge will help you to generate translations from the output of the decoder prediction layer.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Treasure hunt\n",
    "\n",
    "You recently won a all-paid trip to a lush tropical island. While you were wandering around, you found an ancient treasure map pointing to a great treasure, which had a few secret messages written using 1s and 0s. Having just taken this course, you instantly recognize that it is a sequence of onehot encoded vectors. You have also been lucky to find the word to index mapping to know which word refers to which ID.\n",
    "\n",
    "Now you need to decrypt the secret message and find out what this map is saying. You have been provided with a treasure_map which is a number of sentences by number of words by onehot vector length matrix. You have also been provided with the index2word Python dictionary that maps an ID to a word.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Get the word IDs for the onehot encoded vectors in the treasure_map (onehot vector dimension is the last dimension).\n",
    "    Get the sequence length (i.e. number of time steps) from treasure_map and assign to seq_len.\n",
    "    Get the word ID in the i-th sentence at t-th position.\n",
    "    Append the String word (i.e. not the word ID) corresponding to wid to the list words.\n",
    "\n",
    "\"\"\"\n",
    "index2word = {0: 'PAD', 1: 'go', 2: 'to', 3: 'the', 4: 'cave', 5: 'find', 6: 'stone', 7: 'dragon', 8: 'push', 9: 'head', 10: 'down'}\n",
    "treasure_map = np.array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "       [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
    "# solution\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get the word IDs from the treasure map\n",
    "word_ids = np.argmax(treasure_map, axis=-1)\n",
    "\n",
    "# Get the sequence length from the treasure map\n",
    "seq_len = treasure_map.shape[1]\n",
    "\n",
    "for i in range(treasure_map.shape[0]):\n",
    "    words = []\n",
    "    for t in range(seq_len):\n",
    "        # Get the word ID for the i-th sentence and t-th position\n",
    "        wid = word_ids[i, t]\n",
    "        if wid != 0:\n",
    "            # Append the word corresponding to wid\n",
    "            words.append(index2word[wid])\n",
    "    print(\"Instruction \", i+1, \": \", ' '.join(words))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You just uncovered a great treasure full of gold and diamonds! But best of all you learned to decode a 3 dimensional onehot vector output and extract the sentences. This knowledge will help you to generate translations from the output of the decoder prediction layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction  1 :  go to the dragon cave\n",
      "Instruction  2 :  find the stone dragon\n",
      "Instruction  3 :  push the head down\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work! The only thing better than a tropical vacation is finding gold during the tropical vacation. It is a good idea to stick with list comprehension when possible. List comprehensions are in fact more efficient than for loops and they make the code more compact.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Treasure hunt\n",
    "\n",
    "Now there's a little twist to the treasure hunt. You have forgotten to pack your laptop and you only have a device with limited memory on you. The code you write should be less than 4 lines of code (excluding comments). Since you need to make the code as compact as possible you will be using list comprehension.\n",
    "\n",
    "List comprehension is a great way to loop through data with a single line of code. For example, if you want to get all the even numbers from a list of numbers, you can do [n for n in range(100) if n%2==0]. So as you can see, list comprehension allows you to combine for loops and if statements in a single line of code.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Get the word IDs for the onehot encoded vectors in the treasure_map.\n",
    "    Get the batch size (the very first dimension) of the treasure map and use that to create a for loop.\n",
    "    Get the words of the i-th sentence by iterating the i-th row of the word_ids while ignoring word IDs equal to zero.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Get the word IDs from the treasure map\n",
    "word_ids = np.argmax(treasure_map, axis=-1)\n",
    "# Get the batch size from the treasure map\n",
    "for i in range(treasure_map.shape[0]):\n",
    "  \t# Get all the words of the i-th sentence using list comprehension\n",
    "\twords = [index2word[wid] for wid in word_ids[i] if wid != 0]\n",
    "\tprint(\"Instruction \", i+1, \": \", ' '.join(words))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! The only thing better than a tropical vacation is finding gold during the tropical vacation. It is a good idea to stick with list comprehension when possible. List comprehensions are in fact more efficient than for loops and they make the code more compact.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Generating English-French translations\n",
    "\n",
    "Did you know that HSBC bank once spent $10 million on re-branding its slogan, due to a translation mistake?\n",
    "\n",
    "We will use the trained model to predict the French translation of an English sentence using model.predict(). You will be provided with the trained model (model). This model has been trained for 50 epochs on 100,000 sentences which achieved around 90% accuracy on a 35000+ word validation set. It might take longer for this exercise to load as the trained model is loaded before the exercise. Furthermore, you will also be provided with a dictionary (fr_id2word) which you can use to convert word indices to words. Finally, you will use the sents2seqs function that you implemented earlier to preprocess the data before feeding it to the model.\n",
    "\n",
    "You can use help(sents2seqs) to remind yourself what's accepted by the sents2seqs() function.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Preprocess source en_st so that it is converted to a onehot encoded numpy array using the previously defined sents2seqs function.\n",
    "    Predict the output for en_seq using the provided trained model.\n",
    "    Extract the maximum index for each prediction of fr_pred using np.argmax and assign it to fr_seq.\n",
    "    Convert the French sequence IDs to a sentence using list comprehension (remember to ignore 0s) and assign it to fr_sent.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "en_st = ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n",
    "print('English: {}'.format(en_st))\n",
    "\n",
    "# Convert the English sentence to a sequence\n",
    "en_seq = sents2seqs('source', en_st, onehot=True, reverse=True)\n",
    "\n",
    "# Predict probabilities of words using en_seq\n",
    "fr_pred = model.predict(en_seq)\n",
    "\n",
    "# Get the sequence indices (max argument) of fr_pred\n",
    "fr_seq = np.argmax(fr_pred, axis=-1)[0]\n",
    "\n",
    "# Convert the sequence of IDs to a sentence and print\n",
    "fr_sent = [fr_id2word[i] for i in fr_seq if i != 0]\n",
    "print(\"French (Custom): {}\".format(' '.join(fr_sent)))\n",
    "print(\"French (Google Translate): les etats-unis sont parfois froids en décembre, mais parfois gelés en juin\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Bien joué! Here you learned to translate an English sentence to French using an encoder-decoder model. However the results can be further improved by using techniques like teacher forcing, which we will discuss in the next chapter.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
