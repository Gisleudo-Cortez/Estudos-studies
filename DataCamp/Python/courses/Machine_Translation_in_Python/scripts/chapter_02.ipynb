{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Machine_Translation_in_Python/datasets/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data + 'vocab_en.txt', 'r') as en:\n",
    "    en_text = en.read().split(\"\\n\")\n",
    "    en.close\n",
    "\n",
    "with open(path_data + 'vocab_fr.txt', 'r') as fr:\n",
    "    fr_text = fr.read().split(\"\\n\")\n",
    "    fr.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\tFrench:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "English:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\tFrench:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "English:  california is usually quiet during march , and it is usually hot in june .\n",
      "\tFrench:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "English:  the united states is sometimes mild during june , and it is cold in september .\n",
      "\tFrench:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "English:  your least liked fruit is the grape , but my least liked is the apple .\n",
      "\tFrench:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "First sentence:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\tWords:  ['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations, you now know how to tokenize a sentence using the split() function. This is a very important step in feeding the data to machine translation models, as these models consume individual words.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Exploring the dataset\n",
    "\n",
    "You will now explore the dataset a little bit. You will first get a feel of what the data looks like. You will print some of the data and learn how to tokenize the sentences in the data to individual words. For the English language, tokenization appears to be a trivial task, however, there are languages such as Japanese, which are not as consistently delimited as English.\n",
    "\n",
    "For this exercise, you have been provided with two datasets: en_text and fr_text. The en_text contains a list of English sentences, where the fr_text contains the corresponding list of French sentences.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Write a zip() function that iterates through the first 5 sentences of the English sentences (en_text) and French sentences (fr_text).\n",
    "    Get the first English sentence from en_text.\n",
    "    Tokenize the obtained sentence using the split() function and the space character and assign it to first_words.\n",
    "    Print the tokenized words.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Iterate through the first 5 English and French sentences in the dataset\n",
    "for en_sent, fr_sent in zip(en_text[:5], fr_text[:5]):  \n",
    "  print(\"English: \", en_sent)\n",
    "  print(\"\\tFrench: \", fr_sent)\n",
    "\n",
    "# Get the first sentence of the English dataset\n",
    "first_sent = en_text[0]\n",
    "print(\"First sentence: \", first_sent)\n",
    "# Tokenize the first sentence\n",
    "first_words = first_sent.split(\" \")\n",
    "# Print the tokenized words\n",
    "print(\"\\tWords: \", first_words)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations, you now know how to tokenize a sentence using the split() function. This is a very important step in feeding the data to machine translation models, as these models consume individual words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English) Mean sentence length:  13.225589543090503\n",
      "(English) Vocabulary size:  228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You have explored some crucial attributes of the dataset. As you can see it is a very small dataset with a few hundreds of words in the vocabulary. In real life datasets, the vocabulary can reach 100,000 quite easily. As a rule of thumb, you should do some exploratory data analysis on the dataset before jumping ahead and implementing the model.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Exploring the dataset\n",
    "\n",
    "Now you will explore some attributes of the dataset. Specifically, you will determine the average length (i.e. number of words) of all sentences and the size of the vocabulary for the English dataset.\n",
    "\n",
    "For this exercise, the English dataset en_text containing a list of English sentences has been provided. In this exercise you will be using a Python list-related function called <list>.extend() which is a different variant of the function <list>.append(). Let's understand the difference through an example. Say a=[1,2,3] and b=[4,5]. a.append(b) would result in a list [1,2,3,[4,5]] where a.extend(b) would result in [1,2,3,4,5].\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Compute the lengths of each sentence using the split() function and the len() function, while iterating through en_text.\n",
    "    Compute the mean length of sentences using numpy.\n",
    "    Populate the list all_words, in the for loop body, by adding in all the words found in sentences after tokenizing.\n",
    "    Convert the list all_words, to a set object and compute the length/size of the set.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import numpy as np\n",
    "# Compute length of sentences\n",
    "sent_lengths = [len(en_sent.split(\" \")) for en_sent in en_text]\n",
    "# Compute the mean of sentences lengths\n",
    "mean_length = np.mean(sent_lengths)\n",
    "print('(English) Mean sentence length: ', mean_length)\n",
    "\n",
    "all_words = []\n",
    "for sent in en_text:\n",
    "  # Populate all_words with all the words in sentences\n",
    "  all_words.extend(sent.split(\" \"))\n",
    "# Compute the length of the set containing all_words\n",
    "vocab_size = len(set(all_words))\n",
    "print(\"(English) Vocabulary size: \", vocab_size)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You have explored some crucial attributes of the dataset. As you can see it is a very small dataset with a few hundreds of words in the vocabulary. In real life datasets, the vocabulary can reach 100,000 quite easily. As a rule of thumb, you should do some exploratory data analysis on the dataset before jumping ahead and implementing the model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 09:17:28.417929: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-01 09:17:28.671243: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-01 09:17:28.673955: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 09:17:31.739357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-01 09:17:36.641918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-01 09:17:36.645110: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 15, 150)]         0         \n",
      "                                                                 \n",
      " gru (GRU)                   [(None, 48),              28800     \n",
      "                              (None, 48)]                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,800\n",
      "Trainable params: 28,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 09:17:37.480802: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 09:17:37.489850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 09:17:37.498702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nYou've done well! You just implemented the encoder of a machine translation model and you are on your way to implementing the full model. In the next video you will learn how to implement the decoder.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Defining the encoder\n",
    "\n",
    "Here you'll be taking your first step towards creating a machine translation model: implementing the encoder. The encoder that you will implement is a very simple model compared to the complex models that are used in real-world applications such as the Google machine translation service. But don't worry, though the model is simple, the concepts are the same as of those complex models. Here we will use the prefix en (e.g. en_gru) to indicate anything encoder related and de to indicate decoder related things (e.g. de_gru).\n",
    "\n",
    "You will see that we are choosing en_vocab to be smaller (150) than the actual value (228) that we found. Making the vocabulary smaller reduces the memory footprint of the model. Reducing the vocabulary slightly is fine as we are removing the rarest words when we are doing so. For machine translation tasks, rare words usually have less value than common words.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define an Input layer for an input which has a vocabulary size en_vocab and a sequence length en_len, using the shape argument.\n",
    "    Define a keras.layers.GRU layer that has hsize hidden units and returns its state.\n",
    "    Get the outputs from the GRU layer by feeding in en_inputs and assign the GRU state to en_state and the output to en_out.\n",
    "    Define a keras.models.Model whose input is en_inputs and the output is the en_state and print the model summary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "en_len = 15\n",
    "en_vocab = 150\n",
    "hsize = 48\n",
    "\n",
    "# Define an input layer\n",
    "en_inputs = keras.layers.Input(shape=(en_len,en_vocab))\n",
    "# Define a GRU layer which returns the state\n",
    "en_gru = keras.layers.GRU(hsize, return_state=True)\n",
    "# Get the output and state from the GRU\n",
    "en_out, en_state = en_gru(en_inputs)\n",
    "# Define and print the model summary\n",
    "encoder = keras.models.Model(inputs=en_inputs, outputs=en_out)\n",
    "print(encoder.summary())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You've done well! You just implemented the encoder of a machine translation model and you are on your way to implementing the full model. In the next video you will learn how to implement the decoder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 163ms/step\n",
      "x.shape =  (2, 2) \n",
      "y.shape =  (2, 6, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! You now understand what the RepeatVector layer does and how it transforms a given input. In the next exercise you will use this to create an input for the decoder of the model.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Understanding the RepeatVector layer\n",
    "\n",
    "You will now explore how the RepeatVector layer works. The RepeatVector layer adds an extra dimension to your dataset. For example if you have an input of shape (batch size, input size) and you want to feed that to a GRU layer, you can use a RepeatVector layer to convert the input to a tensor with shape (batch size, sequence length, input size).\n",
    "\n",
    "In this exercise, you will define a model that repeats a given input a fixed number of times. You will then feed a numpy array to the model and investigate how the model changes the output.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a RepeatVector layer that repeats the input 6 times.\n",
    "    Define a Model that takes the input layer in and produces the repeat vector output.\n",
    "    Define a numpy array object that has data [[0,1], [2,3]].\n",
    "    Predict the output of the model by feeding x as an input.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.layers import Input, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "inp = Input(shape=(2,))\n",
    "# Define a RepeatVector that repeats the input 6 times\n",
    "rep = RepeatVector(6)(inp)\n",
    "# Define a model\n",
    "model = Model(inputs=inp, outputs=rep)\n",
    "# Define input x\n",
    "x = np.array([[0,1],[2,3]])\n",
    "# Get model prediction y\n",
    "y = model.predict(x)\n",
    "print('x.shape = ',x.shape,'\\ny.shape = ',y.shape)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You now understand what the RepeatVector layer does and how it transforms a given input. In the next exercise you will use this to create an input for the decoder of the model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 15, 150)]    0           []                               \n",
      "                                                                                                  \n",
      " gru (GRU)                      [(None, 48),         28800       ['input_1[0][0]']                \n",
      "                                 (None, 48)]                                                      \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVector)  (None, 20, 48)      0           ['gru[0][1]']                    \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 20, 48)       14112       ['repeat_vector_1[0][0]',        \n",
      "                                                                  'gru[0][1]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42,912\n",
      "Trainable params: 42,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 09:17:38.916088: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 09:17:38.920586: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 09:17:38.924316: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCongratulations! You're almost there and very close to having the full encoder-decoder model. You have to introduce an additional layer on top of the decoder to produce a probabilistic output of target words.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Defining the decoder\n",
    "\n",
    "In this exercise, you will implement the decoder and define an end-to-end model going from encoder inputs to the decoder GRU outputs. The decoder uses the same model as the encoder. However there are differences in the inputs and states fed to the decoder, compared to the encoder. For example, the decoder consumes the context vector produced by the encoder as inputs as well as the initial state to the decoder. Remember that we will use the prefix en (e.g. en_gru) to indicate anything encoder related and de to indicate decoder related things (e.g. de_gru).\n",
    "\n",
    "To implement the decoder you will use RepeatVector and GRU layers.\n",
    "\n",
    "For this exercise you have been provided with the encoder model and the various layers of the encoder that you have already implemented. For example, the encoder inputs are provided as en_inputs and the context vector as en_state. Also note that the GRU and Model objects have been already imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define an RepeatVector layer that takes en_state as an input and repeats it fr_len times.\n",
    "    Define a GRU layer, decoder_gru, which has hidden units equal to hsize and returns all the outputs produced.\n",
    "    Get the output of the decoder_gru layer by feeding in de_inputs as the input and en_state as the initial state of the decoder.\n",
    "    Define a model that takes en_inputs as the input and gru_outputs as the output.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "hsize = 48\n",
    "fr_len = 20\n",
    "\n",
    "# Define a RepeatVector layer\n",
    "de_inputs = RepeatVector(fr_len)(en_state)\n",
    "\n",
    "# Define a GRU model that returns all outputs\n",
    "decoder_gru = GRU(hsize, return_sequences=True)\n",
    "\n",
    "# Get the outputs of the decoder\n",
    "gru_outputs = decoder_gru(de_inputs, initial_state=en_state)\n",
    "\n",
    "# Define a model with the correct inputs and outputs\n",
    "enc_dec = Model(inputs=en_inputs, outputs=gru_outputs)\n",
    "enc_dec.summary()\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You're almost there and very close to having the full encoder-decoder model. You have to introduce an additional layer on top of the decoder to produce a probabilistic output of target words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n",
      "Mark has probabilities [0.3929537  0.37995604 0.22709025] and wins Gift voucher\n",
      "John has probabilities [0.33233336 0.34169823 0.32596847] and wins Car\n",
      "Kelly has probabilities [0.35587627 0.35802534 0.28609842] and wins Car\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNow the hosts can move on with the Gameshow. The managers are really happy with your work. Here you learned how to use a Dense layer to make predictions from a batch of input data, which is essential for classification based models such as machine translation models.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Enter to win amazing prizes\n",
    "\n",
    "In this exercise, you will learn about the Dense layer. Why not do that with a fun exercise? Imagine there's a game show where prizes are determined by a neural network. The contestant enters\n",
    "\n",
    "    the number of siblings,\n",
    "    the number of coffees had today and\n",
    "    if they like tomatoes or not,\n",
    "\n",
    "and the model predicts what the contestant will win.\n",
    "\n",
    "To implement this, you will be using Keras. You will need to create a model with an input layer which accepts three features (the number of siblings as an integer, the number of coffees as an integer and if they like tomatoes or not as a 0 or 1). Then the input goes through a Dense layer which outputs 3 probabilities (i.e. probabilities of winning a car, a gift voucher or nothing).\n",
    "\n",
    "Input and Dense layers as well as a Model object from Keras are already imported. You are also provided a weight initializer called init to initialize the Dense layer.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define an input layer which only accepts 3 contestants (batch size), where each contestant has 3 inputs: sibling count, number of coffees and tomato preference (input size).\n",
    "    Define a Dense layer which has 3 outputs, softmax activation and init as the initializer.\n",
    "    Compute the model predictions for x using the defined model.\n",
    "    Get the most probable prize (as an integer) for each contestant.\n",
    "\n",
    "\"\"\"\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "init = RandomNormal(mean=0.0, stddev=0.05, seed=6000)\n",
    "from keras.layers import Dense\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define an input layer with batch size 3 and input size 3\n",
    "inp = Input(batch_shape=(3,3))\n",
    "# Get the output of the 3 node Dense layer\n",
    "pred = Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init)(inp)\n",
    "model = Model(inputs=inp, outputs=pred)\n",
    "\n",
    "names = [\"Mark\", \"John\", \"Kelly\"]\n",
    "prizes = [\"Gift voucher\", \"Car\", \"Nothing\"]\n",
    "x = np.array([[5, 0, 1], [0, 3, 1], [2, 2, 1]])\n",
    "# Compute the model prediction for x\n",
    "y = model.predict(x)\n",
    "# Get the most probable class for each sample\n",
    "classes = np.argmax(y, axis= -1)\n",
    "print(\"\\n\".join([\"{} has probabilities {} and wins {}\".format(n,p,prizes[c]) \\\n",
    "                 for n,p,c in zip(names, y, classes)]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Now the hosts can move on with the Gameshow. The managers are really happy with your work. Here you learned how to use a Dense layer to make predictions from a batch of input data, which is essential for classification based models such as machine translation models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=\n",
      " [['Mark' 'John' 'Kelly']\n",
      " ['Jenny' 'Shan' 'Sarah']] \n",
      "x=\n",
      " [[[5 0 1]\n",
      "  [1 1 0]]\n",
      "\n",
      " [[0 3 1]\n",
      "  [0 4 0]]\n",
      "\n",
      " [[2 2 1]\n",
      "  [6 0 1]]] \n",
      "x.shape= (3, 2, 3)\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "Game 1: Mark has probs [0.3929537  0.37995604 0.22709025] and wins Gift voucher\n",
      "\n",
      "Game 1: John has probs [0.33233336 0.34169823 0.32596847] and wins Car\n",
      "\n",
      "Game 1: Kelly has probs [0.35587627 0.35802534 0.28609842] and wins Car\n",
      "\n",
      "Game 2: Jenny has probs [0.34050465 0.3426381  0.31685725] and wins Car\n",
      "\n",
      "Game 2: Shan has probs [0.3069249  0.32335538 0.36971974] and wins Nothing\n",
      "\n",
      "Game 2: Sarah has probs [0.3994818  0.38477215 0.21574609] and wins Gift voucher\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done! The game show managers will be very happy with your work. Now you know how to use dense layers to process time series inputs. You also learned how to slice data on the time dimension so that you can analyse time-series data. In the next exercise you will complete the rest of the decoder.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Let's play a few more games\n",
    "\n",
    "Great work on the last project. This time, you will need to simulate multiple game shows hosted over several days. This means that your data will have a time dimension to it. More specifically, your data will have the shape (number of contestants, game shows, inputs size).\n",
    "\n",
    "You will need to extend your model to incorporate this new feature. For this you will be using a TimeDistributed layer to allow the Dense layer to accept contestants from multiple game shows.\n",
    "\n",
    "You have been provided with the weight initializer init, the prizes list from the previous exercise, a time-series input x and names which contains the names of the contestants. x is a (3,2,3) numpy array where names is a (2,3) Python list. In other words, you have 2 game shows (i.e. sequence length), each with 3 contestants (batch size) where each contestant has 3 attributes (input size).\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print the names, x and shape of x.\n",
    "---\n",
    "\n",
    "    Create a TimeDistributed layer which wraps a 3 node Dense layer with softmax activation.\n",
    "    Get the most probable classes for all the samples of the predictions.\n",
    "    In the second for loop, provide the tth slice (on time dimension) of y and classes (i.e. information of the tth game show).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "names = np.array([['Mark', 'John', 'Kelly'], ['Jenny', 'Shan', 'Sarah']])\n",
    "x = np.array([[[5, 0, 1],\n",
    "        [1, 1, 0]],\n",
    "\n",
    "       [[0, 3, 1],\n",
    "        [0, 4, 0]],\n",
    "\n",
    "       [[2, 2, 1],\n",
    "        [6, 0, 1]]])\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "# Print names and x\n",
    "print('names=\\n',names, '\\nx=\\n',x, '\\nx.shape=', x.shape)\n",
    "\n",
    "inp = Input(shape=(2, 3))\n",
    "# Create the TimeDistributed layer (the output of the Dense layer)\n",
    "dense_time = TimeDistributed(Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init))\n",
    "pred = dense_time(inp)\n",
    "model = Model(inputs=inp, outputs=pred)\n",
    "\n",
    "y = model.predict(x)\n",
    "# Get the most probable class for each sample\n",
    "classes = np.argmax(y, axis = -1)\n",
    "for t in range(2):\n",
    "  # Get the t-th time-dimension slice of y and classes\n",
    "  for n, p, c in zip(names[t], y[:, t, :], classes[:, t]):\n",
    "  \tprint(\"Game {}: {} has probs {} and wins {}\\n\".format(t+1,n,p,prizes[c]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! The game show managers will be very happy with your work. Now you know how to use dense layers to process time series inputs. You also learned how to slice data on the time dimension so that you can analyse time-series data. In the next exercise you will complete the rest of the decoder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 10:18:15.231108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 10:18:15.235665: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 10:18:15.238992: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-08-01 10:18:15.897114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-08-01 10:18:15.902224: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-08-01 10:18:15.905872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, GRU, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "\n",
    "en_inputs = Input(shape=(en_len, en_vocab))\n",
    "en_gru = GRU(hsize, return_state=True)\n",
    "en_out, en_state = en_gru(en_inputs)\n",
    "\n",
    "de_inputs = RepeatVector(fr_len)(en_state)\n",
    "de_gru = GRU(hsize, return_sequences=True)\n",
    "de_out = de_gru(de_inputs, initial_state=en_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (None, 20, 250)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat work! Now you have impelemented the full encoder-decoder model using the Keras Functional API. This was a much more complex model than a typical feed-forward neural network or a convolutional neural network and really tests your knowledge about the Keras Functional API. You should feel good about yourself. Now let's wrap this as a Keras model object.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Defining the full model\n",
    "\n",
    "Here you will be implementing the last few layers of the encoder-decoder model. You will be using Dense and TimeDistributed layers to get the final predictions (i.e. predicted French word probabilities) of the encoder-decoder model.\n",
    "\n",
    "You are provided with the encoder and decoder (without the top-part) you implemented so far. The decoder GRU layer's output de_out is provided. We use the prefix en (e.g. en_gru) to indicate anything encoder related and de to indicate decoder related things (e.g. de_gru).\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import Dense and TimeDistributed layers from Keras.\n",
    "    Define a Dense layer with softmax activation which has fr_vocab outputs.\n",
    "    Wrap the Dense layer in a TimeDistributed layer.\n",
    "    Get the final prediction of the model by passing de_out to the de_dense_time layer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "fr_vocab = 250\n",
    "# Import Dense and TimeDistributed layers\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "# Define a softmax dense layer that has fr_vocab outputs\n",
    "de_dense = Dense(fr_vocab, activation='softmax')\n",
    "# Wrap the dense layer in a TimeDistributed layer\n",
    "de_dense_time = TimeDistributed(de_dense)\n",
    "# Get the final prediction of the model\n",
    "de_pred = de_dense_time(de_out)\n",
    "print(\"Prediction shape: \", de_pred.shape)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Now you have impelemented the full encoder-decoder model using the Keras Functional API. This was a much more complex model than a typical feed-forward neural network or a convolutional neural network and really tests your knowledge about the Keras Functional API. You should feel good about yourself. Now let's wrap this as a Keras model object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 15, 150)]    0           []                               \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    [(None, 48),         28800       ['input_5[0][0]']                \n",
      "                                 (None, 48)]                                                      \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVector)  (None, 20, 48)      0           ['gru_2[0][1]']                  \n",
      "                                                                                                  \n",
      " gru_3 (GRU)                    (None, 20, 48)       14112       ['repeat_vector_2[0][0]',        \n",
      "                                                                  'gru_2[0][1]']                  \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 20, 250)     12250       ['gru_3[0][0]']                  \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,162\n",
      "Trainable params: 55,162\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! Now you have implemented a encoder-decoder based neural machine translator. You now know how to create a complex deep learning model using the advance Keras functional API. In the next lesson we will learn how to train this model and generate translations from it.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Defining the full model\n",
    "\n",
    "Did you know that it took around 6 days and 96 GPUs to train a variant of the Google Neural Machine Translator just on the English to French translation task.\n",
    "\n",
    "In this exercise you will define a similar but much simpler encoder-decoder based neural machine translator model. Specifically, you will use the previously defined inputs and outputs and define a Keras Model object and compile the model with a given loss function and an optimizer.\n",
    "\n",
    "Here you are provided with en_inputs (encoder input layer), en_out and en_state (encoder GRU output), de_out (decoder GRU output) and de_pred (decoder prediction) that you previously defined.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a Keras Model which takes in the en_inputs as inputs and the decoder predictions (de_pred) as the output.\n",
    "    Compile the defined model by calling <model>.compile with the 'adam' optimizer, cross entropy loss and accuracy (acc) as a metric.\n",
    "    Print the summary of the model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "# Define a model with encoder input and decoder output\n",
    "nmt = Model(inputs=en_inputs, outputs=de_pred)\n",
    "\n",
    "# Compile the model with an optimizer and a loss\n",
    "nmt.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# View the summary of the model \n",
    "nmt.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! Now you have implemented a encoder-decoder based neural machine translator. You now know how to create a complex deep learning model using the advance Keras functional API. In the next lesson we will learn how to train this model and generate translations from it.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
