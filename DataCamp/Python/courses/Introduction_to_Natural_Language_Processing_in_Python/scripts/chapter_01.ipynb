{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:03:05.491248Z",
     "iopub.status.busy": "2023-06-16T11:03:05.490904Z",
     "iopub.status.idle": "2023-06-16T11:03:07.229372Z",
     "shell.execute_reply": "2023-06-16T11:03:07.227917Z",
     "shell.execute_reply.started": "2023-06-16T11:03:05.491226Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Introduction_to_Natural_Language_Processing_in_Python/datasets/'\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:03:07.270229Z",
     "iopub.status.busy": "2023-06-16T11:03:07.267352Z",
     "iopub.status.idle": "2023-06-16T11:03:07.295697Z",
     "shell.execute_reply": "2023-06-16T11:03:07.287390Z",
     "shell.execute_reply.started": "2023-06-16T11:03:07.270118Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:03:07.306510Z",
     "iopub.status.busy": "2023-06-16T11:03:07.305586Z",
     "iopub.status.idle": "2023-06-16T11:03:07.334099Z",
     "shell.execute_reply": "2023-06-16T11:03:07.330826Z",
     "shell.execute_reply.started": "2023-06-16T11:03:07.306436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 's', 'write', 'RegEx']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done!\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Which pattern?\n",
    "\n",
    "Which of the following Regex patterns results in the following text?\n",
    "\n",
    ">>> my_string = \"Let's write RegEx!\"\n",
    ">>> re.findall(PATTERN, my_string)\n",
    "['Let', 's', 'write', 'RegEx']\n",
    "\n",
    "In the IPython Shell, try replacing PATTERN with one of the below options and observe the resulting output. \n",
    "The re module has been pre-imported for you and my_string is available in your namespace.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "my_string = \"Let's write RegEx!\"\n",
    "PATTERN = r\"\\w+\"\n",
    "print(re.findall(PATTERN, my_string))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:26:02.649592Z",
     "iopub.status.busy": "2023-06-16T11:26:02.648684Z",
     "iopub.status.idle": "2023-06-16T11:26:02.659721Z",
     "shell.execute_reply": "2023-06-16T11:26:02.657351Z",
     "shell.execute_reply.started": "2023-06-16T11:26:02.649529Z"
    }
   },
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:27:59.226786Z",
     "iopub.status.busy": "2023-06-16T11:27:59.225678Z",
     "iopub.status.idle": "2023-06-16T11:27:59.243812Z",
     "shell.execute_reply": "2023-06-16T11:27:59.242646Z",
     "shell.execute_reply.started": "2023-06-16T11:27:59.226496Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNicely done! Practice is the key to mastering RegEx.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Practicing regular expressions: re.split() and re.findall()\n",
    "\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. \n",
    "Take a look at my_string first by printing it in the IPython Shell, \n",
    "to determine how you might best match the different steps.\n",
    "\n",
    "Note: \n",
    "It's important to prefix your regex patterns with r to ensure that your patterns are interpreted \n",
    "in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. \n",
    "For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, \n",
    "it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - \n",
    "and not as a new line.\n",
    "\n",
    "The regular expression module re has already been imported for you.\n",
    "\n",
    "Remember from the video that the syntax for the regex library is to always to pass the pattern first, \n",
    "and then the string second.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Split my_string on each sentence ending. To do this:\n",
    "    Write a pattern called sentence_endings to match sentence endings (.?!).\n",
    "    Use re.split() to split my_string on the pattern and print the result.\n",
    "Find and print all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall().\n",
    "    Remember the [a-z] pattern shown in the video to match lowercase groups? \n",
    "    Modify that pattern appropriately in order to match uppercase groups.\n",
    "    Write a pattern called spaces to match one or more spaces (\"\\s+\") and then use re.split() to split my_string on this pattern, \n",
    "    keeping all punctuation intact. Print the result.\n",
    "    Find all digits in my_string by writing a pattern called digits (\"\\d+\") and using re.findall(). Print the result.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[\\.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nicely done! Practice is the key to mastering RegEx.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:45:20.536079Z",
     "iopub.status.busy": "2023-06-16T11:45:20.535797Z",
     "iopub.status.idle": "2023-06-16T11:45:20.540757Z",
     "shell.execute_reply": "2023-06-16T11:45:20.539867Z",
     "shell.execute_reply.started": "2023-06-16T11:45:20.536055Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(path_data + 'grail.txt') as file:\n",
    "    scene_one = file.read()\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:45:56.310436Z",
     "iopub.status.busy": "2023-06-16T11:45:56.309881Z",
     "iopub.status.idle": "2023-06-16T11:46:02.209492Z",
     "shell.execute_reply": "2023-06-16T11:46:02.208673Z",
     "shell.execute_reply.started": "2023-06-16T11:45:56.310400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nero/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T11:46:12.442994Z",
     "iopub.status.busy": "2023-06-16T11:46:12.442573Z",
     "iopub.status.idle": "2023-06-16T11:46:13.154128Z",
     "shell.execute_reply": "2023-06-16T11:46:13.152953Z",
     "shell.execute_reply.started": "2023-06-16T11:46:12.442959Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sneaking', 'sense', 'basic', 'demand', '18', 'Um', 'uhh', 'Ector', 'three', 'collective', \"'ve\", 'bottoms', 'seen', 'liar', 'ANIMATOR', 'MAN', 'MONKS', 'CROWD', 'siren', 'because', 'RIGHT', 'eisrequiem', 'through', 'ARTHUR', 'twenty-four', 'fatal', 'wings', 'called', 'terribly', 'hello', 'middle', 'He', 'WOMAN', 'temptation', 'Hooray', 'varletesses', 'haste', 'lives', 'daring', 'end', 'throughout', 'Chapter', 'tiny-brained', 'yours', 'saw', \"'Course\", 'union', 'Midget', 'death', 'BRIDE', 'people', 'does', 'leap', 'capital', 'blow', 'Umhm', 'appease', 'whether', 'arm', 'individually', 'A', 'problems', 'suspenseful', 'living', 'bonk', 'mortally', 'scott', 'bloody', 'running', 'Not', 'distress', 'grail-shaped', 'weather', ':', \"'To\", \"'\", 'was', 'rich', 'private', 'Aaaaaaaah', 'changed', 'angels', 'brain', 'are', 'tackle', 'tale', 'relax', \"'Til\", 'And', 'feint', 'head', 'None', 'squeak', 'string', 'watery', 'scene', 'traveller', 'hast', 'rock', 'tea', 'ere', 'idea', 'frontal', 'doing', 'cope', 'animal', 'aptly', 'they', 'taunting', 'Four', 'legally', 'have', 'saying', 'enjoying', 'aloft', 'Peril', 'Cut', 'soiled', 'van', 'Actually', 'cruel', 'together', 'imperialist', 'Yeah', 'sell', 'ridden', '17', 'Try', 'nothing', ']', 'Aauuugh', 'Help', 'my', 'Which', 'Uuh', 'bravest', 'longer', 'glad', 'shit', 'men', 'class', 'As', 'ever', 'asks', 'reasonable', 'dunno', 'j', 'master', 'Quiet', 'must', 'Remove', 'PIGLET', 'thy', 'non-migratory', 'Eh', 'climes', 'shrubberies', 'To', 'uuup', 'bride', 'girl', 'words', 'OFFICER', 'anything', 'best', 'Leaving', 'dad', 'FRENCH', 'Yeaah', 'fellows', 'fwump', 'she', 'smack', 'outside', \"e'er\", 'harmless', 'just', 'Open', 'Hic', 'conclusion', 'you', 'excuse', 'going', \"'Ni\", 'Crapper', 'purpose', '--', 'shelter', 'Be', 'town', 'if', 'twin', 'Peng', 'dona', 'friend', 'Found', 'obviously', 'guiding', 'spake', 'asking', 'Welcome', \"'m\", 'spoken', 'awaaay', 'GALAHAD', 'talk', 'Robinson', 'present', 'minute', 'riding', 'name', 'watch', 'thine', 'U', 'FATHER', 'formidable', 'performance', 'thump', 'married', 'where', 'SHRUBBER', 'Mud', 'enter', 'lose', 'simple', 'looking', 'look', 'Psalms', 'Gallahad', 'know', 'islands', 'hell', 'SIR', 'By', 'strategy', 'examine', 'Fetchez', 'outdoors', 'sister', 'supposed', 'dynamite', 'supports', 'dramatic', 'returns', 'clop', 'flint', 'eet', 'DINGO', 'majority', 'seek', 'ruffians', 'burned', 'Lie', 'Aah', 'tinder', 'crossed', 'moistened', 'ju', 'creep', 'die', 'welcome', 'five', 'fine', 'two', 'Holy', 'Cherries', 'alive', 'Why', 'Defeat', 'bed-wetting', 'couple', 'distributing', 'Umm', 'Eternal', 'listen', 'totally', \"'Dennis\", 'follow', 'acting', '6', 'agree', '!', 'LAUNCELOT', 'Nador', 'mac', 'Joseph', 'Hmm', 'Uther', 'doors', 'request', 'Aaaaaah', 'ill.', 'keeper', 'getting', 'stay', 'understanding', 'setting', 'NARRATOR', 'gave', 'high', 'Mercea', 'ZOOT', 'Divine', 'Hey', 'writing', 'maintain', 'bois', 'home', 'Victory', 'zone', 'carry', 'samite', 'Augh', 'those', 'move', 'occasion', 'whinny', 'bless', 'behold', 'life', 'unhealthy', 'nostrils', 'general', 'Hallo', 'very', 'ha', 'gurgle', 'CARTOON', 'who', 'scenes', 'exciting', 'brave', 'became', 'gon', 'no', 'birds', 'Huy', 'peasant', 'social', 'Hoa', 'classes', 'Knights', 'tops', 'Use', 'argue', 'major', 'disheartened', 'GOD', 'electric', 'which', 'cry', 'lived', 'conclusions', 'tragic', 'hand', '20', 'Guy', 'Forward', 'PERSON', 'sing', 'Hill', 'O', 'nick', 'some', 'gravy', 'ugly', 'with', 'Ohh', 'suddenly', 'comin', 'Even', 'Patsy', 'Run', 'MINSTREL', 'LOVELY', 'excepting', 'huge', 'threw', 'rest', 'Idiom', 'lady', 'Grail', 'owns', 'deal', 'bite', 'Build', 'elbows', 'higher', 'compared', 'workers', 'water', 'bastard', 'temperate', 'TIM', 'over', 'violence', 'strewn', 'Hurry', 'entering', 'Perhaps', 'away', 'although', 'fought', 'chickened', 'answers', 'groveling', 'gay', 'Pin', 'plan', 'art', 'k-nnniggets', 'looked', 'Great', 'ferocity', 'Summer', 'centuries', 'allowed', 'Away', 'Book', 'legendary', 'Hello', 'tap-dancing', 'stew', 'NI', 'Saxons', 'Or', 'We', 'nineteen-and-a-half', 'helpful', 'round', 'Wayy', 'scots', 'knew', '10', 'her', 'question', 'Gable', 'grovel', 'place', 'ours', 'ponds', 'ride', 'discovers', 'nervous', 'yet', 'spooky', 'Do', 'Get', '23', 'kingdom', 'forced', 'heroic', 'lobbed', 'stayed', 'sample', 'looney', 'winter', 'Burn', 'carries', 'war', 'na', 'Bon', 'Speak', 'clunk', 'convinced', 'band', 'met', 'CHARACTERS', 'fair', 'spirit', 'Castle', 'absolutely', 'mercy', 'Until', 'entered', 'deeds', 'sawwwww', 'Must', 'Arimathea', 'snows', 'face', 'ehh', 'filth', 'SUN', 'Shrubberies', 'put', 'bats', 'seemed', 'bridges', 'direction', 'pause', 'k-nnnnniggets', 'So', 'busy', 'offensive', 'Said', 'eis', 'been', 'lord', 'scribble', 'really', 'bint', 'fortune', 'Hang', 'ratios', 'Surely', 'else', 'rode', 'cart', 'today', 'Behold', 'depressing', 'Aaaaugh', 'verses', 'pure', 'scales', 'wet', 'shrubber', 'marry', 'sweet', 'last', 'Tall', 'test', 'economic', 'happens', 'other', 'clap', 'headoff', 'Neee-wom', 'freedom', 'remembered', 'repressed', 'feel', 'further', 'making', 'door-opening', 'much', 'whop', 'Attila', 'Heh', 'Chaste', 'entrance', 'blondes', 'almost', 'guided', 'breakfast', 'Very', 'w', 'dressed', 'un', 'matter', 'DENNIS', 'More', 'note', 'dark', 'anchovies', 'shrubbery', 'Today', 'illegitimate-faced', 'maybe', 'Nay', 'thou', 'favorite', 'woman', 'walk', 'uh', 'Schools', 'headed', 'pull', 'Himself', 'can', 'wonderful', 'bad-tempered', 'broken', 'horrendous', \"'til\", 'Follow', 'little', 'chord', 'Iesu', 'unladen', 'beds', 'Oui', 'Silly', '14', 'assist', 'out-clever', 'Arthur', 'fart', 'sniff', 'lucky', 'stood', 'auntie', 'either', 'Round', 'MIDDLE', 'score', 'Walk', 'ladies', 'color', 'training', 'KING', 'answer', 'removed', 'Heee', 'built', 'confuse', 'Court', 'young', 'left', 'dare', 'help', 'says', 'awaaaaay', 'father', 'weighs', 'join', 'cough', 'Together', 'heads', 'packing', 'afoot', 'vests', 'warning', 'Dis-mount', 'chickening', 'routines', 'lair', 'its', 'Brother', 'persons', 'hiyaah', 'clue', 'Spring', 'stand', 'guarded', 'Ives', 'Aaaugh', 'lambs', 'An', 'fifty', 'when', 'peril', 'him', 'proved', 'and', 'fallen', 'grail', '8', 'wayy', 'Consult', 'ignore', 'OF', 'liver', 'stops', 'Other', 'trade', 'Aaaah', 'remember', 'Tower', 'Picture', 'boil', 'there', 'crying', 'bottom', 'velocity', 'mandate', 'c', 'house', 'Aramaic', 'show', 'biters', 'Aaah', 'SECOND', 'covered', 'shall', 'between', 'bells', 'reached', 'Ridden', 'blanket', 'creak', 'worst', 'Bedevere', 'pweeng', 'keen', 'unplugged', 'chastity', 'only', 'witches', 'personally', 'valleys', 'Winter', 'fooling', 'sink', 'body', 'doubt', 'Go', 'sight', 'Olfin', 'creeper', 'horn', 'cover', 'sacrifice', 'will', 'about', 'What', 'dance', 'praised', 'shut', 'easy', \"'round\", 'trough', 'warned', 'wipers', 'refuse', 'Bors', 'behind', 'punishment', 'example', 'while', 'Oooo', 'breath', '19', 'Ewing', 'emperor', \"'Aauuuuugh\", 'Thsss', 'ROBIN', 'sometimes', 'largest', 'this', 'inherent', 'medieval', 'warmer', 'Whoa', 'questions', 'woods', 'vache', 'Armaments', 'Waa', 'CART-MASTER', 'jump', 'each', 'Honestly', 'Never', 'thought', 'Chop', 'Quickly', 'wan', 'waste', 'command', 'king-a', 'elderberries', 'give', 'tart', 'nice-a', 'dear', 'bridgekeeper', 'Skip', 'two-thirds', 'castanets', 'case', 'signifying', 'Therefore', 'HERBERT', 'carried', 'hidden', 'imprisoned', 'doctors', 'streak', 'how', 'diaphragm', 'already', 'utterly', 'Pure', 'suppose', 'learning', 'Riiight', 'pestilence', 'swallows', 'minutes', 'Mmm', 'Two', 'Ho', 'sank', 'lobbest', 'mooooooo', 'silence', 'held', 'Is', 'crone', 'penalty', 'protect', 'so-called', 'mud', 'dictating', 'starling', 'clang', 'discovered', 'could', 'having', 'travellers', 'pulp', 'repressing', 'ordinary', 'clllank', 'la', 'Thy', 'Hya', 'least', 'ENCHANTER', 'dragging', 'first', 'pram', 'Aaauggh', 'strange', 'underwear', 'witch', 'stress', 'DEAD', 'dead', 'so', 'rabbit', 'Ooh', 'indefatigable', 'time', 'settles', 'push', 'Dragon', 'apart', 'wind', 'i', 'teeth', 'Lucky', 'everyone', 'nearer', 'fell', 'return', 'commands', 'Stay', 'idiom', 'u', 'vouchsafed', 'Am', 'His', 'Sir', 'something', 'Swamp', 'Y', 'wherein', ')', 'knight', 'food', 'separate', 'sworn', 'leaps', 'quiet', 'accomplished', 'mile', 'sorry', 'room', 'raised', 'Supreme', 'Yapping', 'But', 'pay', 'Say', 'Could', 'ways', 'try', 'fourth', 'bed', 'scimitar', 'mangled', 'May', 'Ninepence', 'hamster', 'problem', 'Beast', 'Cornwall', 'ham', 'Quick', 'any', 'Autumn', 'go', 'earthquakes', 'ye', 'Grenade', 'adversary', 'King', '3', 'night', \"'is\", 'high-pitched', 'mine', 'laurels', 'Back', 'awaits', \"'First\", 'ounce', 'Five', 'thanks', 'power', 'court', 'splash', 'here', 'Too', 'fire', 'Like', 'feast', 'things', 'autocracy', 'their', 'You', 'African', 'bleed', 'wounded', 'wicked', 'point', 'Uhh', 'Pie', 'This', 'quest', 'Britain', 'va.', 'Hiyaah', 'passing', 'rescue', 'explain', 'suffice', 'until', 'exploiting', 'snore', 'need', 'Old', 'scholar', 'tit', 'creature', 'Does', 'expect', 'avenged', 'Torment', 'bathing', 'lying', 'Seek', 'society', '4', 'thwonk', 'summon', 'Explain', 'heard', 'commune', 'Thee', 'beacon', 'MAYNARD', 'special', 'Then', 'spanking', 'terrible', 'Assyria', 'Badon', 'better', 'cereals', 'Everything', 'call', 'shimmering', 'Firstly', 'mayhem', 'W', 'herring', 'upon', 'word', '16', 'preserving', 'Let', 'length', 'biscuits', 'hoo', 'rejoicing', 'BRIDGEKEEPER', 'near', 'baaaa', 'mystic', 'With', 'be', 'It', 'castle', 'fly', 'throwing', 'nasty', 'Of', 'pass', 'miserable', 'runes', 'Far', 'nice', 'eight', 'properly', 'carved', 'carve', 'Wood', 'cheesy', 'suggesting', 'kicked', 'attack', 'mooo', 'Exactly', 'sacred', 'aaugh', 'Ask', 'particularly', 'Tim', 'mean', 'Really', 'whispering', 'pig', 'tiny', 'CRASH', 'Battle', 'well', 'tropical', 'business', 'way', 'coconuts', 'bastards', 'Stand', 'less', 'the-not-quite-so-brave-as-Sir-Lancelot', 'done', 'BROTHER', 'carving', 'looks', 'pound', 'hall', 'God', 'towards', 'breadth', 'intermission', 'wooden', 'unclog', 'snuff', 'progress', 'mashed', 'course', 'get', 'internal', 'She', 'model', 'to', 'profane', 'CHARACTER', 'assault', 'Listen', 'carrying', 'howl', 'bold', 'outrageous', 'tail', 'vote', 'bits', 'donaeis', 'France', ';', 'dine', 'Bones', 'aunties', 'soft', 'alight', 'Three', 'lost', 'chu', 'somebody', 'long', 'would', 'government', 'o', 'treat', 'PRISONER', 'new', 'Well', 'work', 'somewhere', 'horse', 'Aggh', 'reads', 'ROGER', 'They', 'ca', 'Excuse', 'SOLDIER', 'chops', 'meeting', 'bugger-folk', 'into', 'forest', 'formed', 'hopeless', 'second', 'Huyah', 'yourself', 'old', 'should', 'off', \"'old\", 'most', 'Providence', 'English', 'Lead', 'taunt', 'Keep', 'swamp', 'ho', '5', 'bladders', 'Your', \"'Morning\", 'pig-dogs', 'guest', 'feet', 'Man', 'sad', 'CRAPPER', 'duty', 'Lady', 'magne', 'resumes', 'shivering', '[', 'Our', 'saved', 'Action', 'Right', 'pray', 'false', 'Oh', 'interested', 'Knight', 'purest', 'found', 'wart', 'lot', 'badger', 'leads', 'goes', 'differences', 'might', 'Gawain', 'fold', 'wood', 'trusty', '..', 'sire', 'gra', 'scratch', 'enough', 'Dennis', 'hundred-and-fifty', 'SENTRY', 'Yup', 'sex', 'buy', 'drink', 'Camelot', 'Hiyah', 'someone', 'always', 'alarm', 'therefore', 'depart', 'lovely', 'one', 'risk', 'from', 'Lord', 'want', 'logically', 'keepers', 'ALL', '22', 'suit', 'grip', 'Almighty', 'Black', 'requiem', 'sequin', 'search', 'Are', 'awfully', 'Auuuuuuuugh', 'favor', 'Excalibur', 'wield', 'Ulk', 'STUNNER', 'testicles', 'scrape', 'tough', 'dull', 'dungeon', 'worthy', 'bang', 'then', 'Un', 'finest', 'Here', 'outwit', 'anyway', 'two-level', 'twong', 'wrong', 'husk', 'Caerbannog', 'throat', 'Father', 'Yay', 'retreat', 'glass', 'Practice', 'killer', 'himself', 'laden', 'GUESTS', 'a', 'chance', 'derives', 'walking', 'dogma', 'Huh', 'Chicken', 'forward', 'Quoi', 'Shall', 'witness', 'for', 'seldom', 'curtains', ',', 'Cider', 'Since', 'strongest', 'migrate', 'stretched', 'next', 'ask', 'bi-weekly', 'affairs', 'history', 'GUARD', 'wedlock', 'influential', 'Clark', 'unsingable', 'temptress', 'become', 'now', 'sheep', 'charged', 'he', 'foot', 'time-a', 'Aauuuves', 'bid', 'Please', 'PATSY', 'Forgive', 'Bristol', 'hacked', 'ooh', 'Bloody', 'Between', 'um', 'Thpppppt', 'nearly', 'foul', 'dressing', 'noise', '21', 'parts', 'sigh', 'woosh', 'yel', 'humble', 'gentle', 'illustrious', 'proceed', 'warm', 'farcical', 'Yes', 'Thank', 'mistake', 'tonight', 'got', 'Anthrax', 'bunny', 'halves', 'Fine', 'completely', 'mumble', 'once', 'tell', 'eh', 'Hoo', 'resting', 'Aaagh', 'smashed', 'guards', 'At', 'ptoo', 'shows', 'Bravely', '.', 'clad', 'task', 'dorsal', 'hospitality', 'haw', 'path', 'his', 'but', 'keep', 'hospital', 'Doctor', 'bond', 'Now', 'Tell', 'bum', 'defeat', 'expensive', 'vital', 'opera', 'more', 'moment', 'science', 'Monsieur', 'liege', 'lads', 'back', 'daft', 'stab', 'kill', 'de', 'Packing', 'automatically', 'died', 'blood', 'identical', 'bicker', 'count', \"'d\", 'great', 'north', 'dying', 'Recently', 'ceremony', 'sir', 'being', 'Will', 'GIRLS', 'meant', 'required', 'without', 'bones', \"'s\", 'Woa', 'manner', 'Alright', 'like', 'clank', 'bad', 'Alice', 'Those', 'draw', 'Off', 'ni', 'Pendragon', 'told', 'Lancelot', 'pond', 'accent', 'hang', 'g', 'valiant', 'dancing', 'binding', 'Iiiiives', 'N', 'Aaaaaaaaah', 'Pull', 'bosom', 'us', 'build', 'ethereal', 'DIRECTOR', 'open', 'lies', 'chosen', 'thud', \"'anging\", 'wounding', 'has', 'finds', 'tired', 'take', 'turns', 'bird', 'stupid', 'Aauuggghhh', 'chest', 'pounds', 'beat', 'pissing', 'of', 'Ages', 'an', 'Would', '9', 'use', 'immediately', 'eats', 'legs', 'The', 'rocks', 'Did', 'surprise', 'day', 'sod', 'Throw', 'small', 'strength', 'knees-bent', 'tree', 'burn', 'trumpets', 'bother', 'Eee', 'Shrubber', 'fruit', 'Once', 'Anyway', 'times', 'women', 'inferior', 'daughter', 'air-speed', 'hee', 'in', 'enemies', 'Britons', \"'Ere\", 'gallantly', 'come', 'lie', 'lunged', 'plover', 'hmm', 'BORS', 'dirty', 'Tale', \"'em\", 'Dappy', 'There', 'forget', 'kind', 'Launcelot', 'leave', 'yeah', 'mother', 'Apples', 'empty', 'wiper', 'VILLAGER', 'approaching', 'chanting', '2', 'clear', 'swords', 'Rheged', 'south', 'True', 'frozen', 'lad', '24', 'anywhere', 'chorus', 'Farewell', 'autonomous', 'buggered', 'big', 'merger', 'single-handed', 'Halt', 'HEADS', 'Ha', 'Dingo', 'KNIGHTS', 'er', 'sword', 'VOICE', 'under', 'Ah', 'perpetuates', 'Sorry', 'started', 'grips', 'I', 'force', 'sign', 'delirious', 'rodent', 'than', 'giggle', 'twang', 'Have', 'language', 'WIFE', 'killed', 'valor', 'pack', 'country', 'invincible', 'eccentric', 'Most', 'carp', 'wo', 'scarper', 'radio', 'bangin', 'had', 'worse', 'year', 'slash', 'Unfortunately', 'please', 'all', 'ran', \"'it\", 'cave', 'Hand', 'Saint', 'purely', 'make', 'Galahad', 'Oooh', 'armor', 'not', 'easily', 'evil', 'object', 'sent', 'Thou', 'find', 'Thursday', \"'ni\", 'slightly', 'joyful', 'third', 'oui', 'Bridge', 'telling', 'clack', 'brush', 'nose', 'song', 'Twenty-one', 'particular', 'ratified', 'Not-appearing-in-this-film', 'Hyy', 'Meanwhile', 'out', 'committed', 'If', 'biggest', 'Bedwere', 'cross', 'havin', 'mangy', 'supreme', 'Mother', 'My', 'All', 'bet', 'OTHER', 'Looks', 'mer', 'line', 'Concorde', 'England', 'may', 'jam', 'sloths', 'haaa', 'ones', 'vicious', 'Blue', 'grenade', 'knows', \"'Erbert\", 'ungallant', 'far', 'lapin', 'LUCKY', 'handsome', 'luck', 'silly', 'lonely', 'worried', 'Anybody', 'decided', 'etc', 'kneecaps', 'Silence', 'worry', 'indeed', 'design', 'voluntarily', 'used', 'stuffed', 'SCENE', 'wishes', '11', 'CONCORDE', 'jokes', 'along', 'marrying', 'beyond', 'did', 'recover', 'effect', 'n', 'averting', 'Come', 'mayest', 'consulted', 'flight', 'sun', 'arrange', 'pointy', 'VILLAGERS', 'executive', 'around', 'Robin', 'fled', 'door', 'able', 'considerable', 'Lake', 'feathers', 'cut', 'Churches', 'never', \"n't\", 'sharp', 'necessary', 'stone', 'own', 'Splendid', 'enchanter', 'dappy', 'shalt', 'AMAZING', 'self-perpetuating', 'boom', 'Aaauugh', 'outdated', 'at', 'even', 'whoever', 'seems', 'heart', 'animator', 'miss', 'kings', 'grin', 'KNIGHT', 'defeator', 'nobody', 'masses', 'sure', 'bows', 'Herbert', 'bridge', 'arrows', 'mate', 'advancing', 'right', 'Antioch', 'twenty', 'needs', 'another', 'See', 'Shut', 'after', 'cadeau', 'approacheth', 'previous', 'gone', 'nibble', 'wave', 'vain', 'wants', 'baby', 'bring', 'Just', 'PRINCESS', 'whose', 'sort', 'Ni', 'awhile', 'Make', 'give-away', 'Ow', 'quick', 'land', 'k-niggets', 'coming', 'pansy', 'danger', 'sonny', 'nine', 'order', 'pimples', 'down', 'dangerous', 'certain', 'as', 'bit', 'European', 'prevent', 'French', 'unarmed', 'Nu', 'period', \"'uuggggggh\", 'PRINCE', 'Yeaaah', 'many', 'HEAD', 'tracts', 'working', 'kills', 'also', 'hat', 'Death', 'rope', 'see', 'reared', 'footwork', 'still', 'quarrel', 'courage', 'When', 'Haw', 'hills', 'tie', 'is', 'thonk', 'buggering', 'took', 'again', 'attend', 'Loimbard', 'handle', 'nor', 'do', 'Zoot', 'wise', 'Table', 'ai', 'earth', 'wide', 'GUARDS', 'Quite', 'Thpppt', 'seem', 'swallow', 'Christ', 's', 'OLD', 'police', 'rather', 'against', 'impeccable', 'Hah', 'vary', 'what', 'on', 'Winston', 'them', 'aquatic', 'folk', 'anarcho-syndicalist', 'nightfall', 'Over', 'accompanied', 'knock', 'Hee', 'cartoon', 'In', 'p', 'dress', 'gained', 'MIDGET', 'remain', 'BLACK', 'On', 'flights', 'brought', 'main', 'guard', 'Dramatically', 'BEDEVERE', 'Good', '?', 'employed', 'Prince', 'full', 'afraid', 'cop', 'INSPECTOR', 'frighten', 'hear', 'we', \"'re\", 'taking', 'Hold', 'yellow', 'yelling', 'turned', 'music', 'Frank', 'named', 'snap', 'types-a', 'it', 'smelt', 'Chickennn', 'Roger', '...', 'git', 'set', \"'forgive\", 'Shh', 'king', 'Uh', 'successful', 'think', 'l', 'types', \"'shrubberies\", 'rrrr', 'guests', 'fight', 'Thppt', 'thirty-seven', 'Iiiives', 'eat', 'Angnor', 'sixteen', 'naughty', 'Princess', 'cast', 'foe', 'apologise', 'Wait', 'your', 'Allo', 'Ay', 'PARTY', 'visually', 'quack', 'relics', 'Agh', 'tear', 'that', 'certainly', 'knocked', \"d'you\", 'Enchanter', 'arms', 'oh', 'orangutans', \"'cause\", 'drilllll', 'singing', 'officer', 'safety', 'One', 'Nothing', 'limbs', 'icy', 'sponge', 'thank', 'burst', 'knights', 'son', 'yes', 'armed', 'side', 'dub', 'taken', 'ninepence', 'suffered', 'No', 'martin', 'floats', 'beautiful', 'gouged', 'say', 'our', 'Gorge', 'Charge', 'cost', 'Rather', 'weapon', 'Mind', 'Fiends', \"'Here\", 'Bravest', 'LEFT', 'Beyond', 'pussy', 'given', 'wait', 'bleeder', 'Hiyya', 'change', 'soon', 'system', 'Running', 'Message', 'plain', 'happy', 'these', 'Jesus', \"'aaaah\", 'bitching', 'b', 'Piglet', 'Every', 'raped', 'mightiest', \"'Ecky-ecky-ecky-ecky-pikang-zoop-boing-goodem-zoo-owli-zhiv\", 'am', 'said', 'inside', 'brunettes', 'donkey-bottom', 'were', 'splat', \"'aaggggh\", '12', 'real', 'oooh', 'kick', 'quests', 'coconut', 'rewr', '15', 'bringing', 'continue', '(', 'window-dresser', 'made', 'GREEN', 'me', 'mad', 'four', 'same', '13', 'boys', 'kneeling', 'regulations', 'let', 'WITCH', 'That', 'strand', 'blessing', 'closest', 'number', 'How', 'HISTORIAN', 'perilous', 'up', 'servant', 'counting', 'scared', 'oo', 'auuuuuuuugh', 'Bring', 'every', 'Uugh', 'spam', 'banana-shaped', '7', 'CAMERAMAN', 'crash', 'late', 'sons', 'bowels', 'B', 'Where', 'retold', 'thing', 'uuggggggh', 'Ayy', 'basis', 'aside', 'pen', 'zoosh', 'table', 'large', 'Nine', 'GUEST', 'too', 'presence', 'names', '1', 'Mine', 'straight', 'laughing', 'passed', 'CUSTOMER', 'speak', 'known', 'honored', 'heh', 'ARMY', 'Bad', 'THE', \"'ll\", 'duck', 'everything', 'Bread', \"C'est\", 'such', 'Aagh', 'Erm', 'worked', 'since', 'anyone', 'minstrels', 'Hm', 'eyes', 'decision', 'amazes', 'Put', 'Anarcho-syndicalism', 'live', \"'S\", 'Prepare', 'Amen', 'good', 'newt', 'roar', 'Ahh', \"'Oooooooh\", 'Brave', 'bravely', 'dictatorship', 'Who', 'Stop', 'ready', 'strangers', 'the', 'makes', 'using', 'forty-three', 'Clear', 'holy', 'Order', 'triumphs', 'forth', 'wedding', '#', 'north-east', 'escape', 'aaaaaah', 'impersonate', 'Greetings', 'weight', 'glory', 'Camaaaaaargue', 'sovereign', 'medical', 'Look', 'stop', 'e', 'WINSTON', 'leg', \"'T\", 'flesh', 'later', 'smashing', 'spank', 'act', 'Thppppt', 'rhymes', 'undressing', 'domine', 'Oooohoohohooo', 'week', 'wound', 'went', 'run', 'beside', 'RANDOM', 'behaviour', 'quite', 'split', 'Maynard', 'man', \"'sorry\", 'CRONE', 'by', 'Steady', 'why', 'Guards', 'y', 'trouble', 'or', 'oral', 'person', 'understand', 'Supposing', 'whom', 'felt', 'heeh', 'spanked', 'actually', 'send', \"'Man\", 'For'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent! Tokenization is fundamental to NLP, and you'll end up using it a lot in text mining and information retrieval projects.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Word tokenization with NLTK\n",
    "\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. \n",
    "Feel free to check it out in the IPython Shell!\n",
    "\n",
    "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize \n",
    "to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the sent_tokenize and word_tokenize functions from nltk.tokenize.\n",
    "    Tokenize all the sentences in scene_one using the sent_tokenize() function.\n",
    "    Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.\n",
    "    Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().\n",
    "    Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent! Tokenization is fundamental to NLP, and you'll end up using it a lot in text mining and information retrieval projects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFantastic work! Now that you're familiar with the basics of tokenization and regular expressions, it's time to learn about more advanced tokenization.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "More regex with re.search()\n",
    "\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use re.search() to search for the first occurrence of the word \"coconuts\" in scene_one. Store the result in match.\n",
    "    Print the start and end indexes of match using its .start() and .end() methods, respectively.\n",
    "---\n",
    "\n",
    "    Write a regular expression called pattern1 to find anything in square brackets.\n",
    "    Use re.search() with the pattern to find the first text in scene_one in square brackets in the scene. Print the result.\n",
    "---\n",
    "\n",
    "    Create a pattern to match the script notation (e.g. Character:), assigning the result to pattern2. Remember that you will want to match any words or spaces that precede the : (such as the space within SOLDIER #1:).\n",
    "    Use re.match() with your new pattern to find and print the script notation in the fourth line. The tokenized sentences are available in your namespace as sentences.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import re\n",
    "\n",
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w.*]+:\"\n",
    "print(re.match(pattern2, sentences[3])) \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fantastic work! Now that you're familiar with the basics of tokenization and regular expressions, it's time to learn about more advanced tokenization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWell done!\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Choosing a tokenizer\n",
    "\n",
    "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "The string is available in your workspace as my_string, and the patterns have been pre-loaded as pattern1, pattern2, pattern3, and pattern4, respectively.\n",
    "\n",
    "Additionally, regexp_tokenize has been imported from nltk.tokenize. You can use regexp_tokenize(string, pattern) with my_string and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "pattern2 = r\"(\\\\w+|#\\\\d|\\\\?|!)\"\n",
    "print(regexp_tokenize(pattern2, my_string))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n",
      "['@datacamp', '#nlp', '#python']\n",
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! Isn't NLP fun?\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Regex with NLTK tokenization\n",
    "\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n",
    "\n",
    "Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    From nltk.tokenize, import regexp_tokenize and TweetTokenizer.\n",
    "---\n",
    "\n",
    "    A regex pattern to define hashtags called pattern1 has been defined for you. Call regexp_tokenize() with this hashtag pattern on the first tweet in tweets and assign the result to hashtags.\n",
    "    Print hashtags (this has already been done for you).\n",
    "---\n",
    "\n",
    "\n",
    "    Write a new pattern called pattern2 to match mentions and hashtags. A mention is something like @DataCamp.\n",
    "\n",
    "    Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets and assign the result to mentions_hashtags.\n",
    "        You can access the last element of a list using -1 as the index, for example, tweets[-1].\n",
    "\n",
    "    Print mentions_hashtags (this has been done for you).\n",
    "---\n",
    "\n",
    "    Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize each tweet into a new list called all_tokens.\n",
    "        To do this, use the .tokenize() method of tknzr, with t as your iterator variable.\n",
    "    Print all_tokens.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Isn't NLP fun?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWonderful!\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Non-ascii tokenization\n",
    "\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF').\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Tokenize all the words in german_text using word_tokenize(), and print the result.\n",
    "    Tokenize only the capital words in german_text.\n",
    "        First, write a pattern called capital_words to match only capital words. Make sure to check for the German Ãœ! To use this character in the exercise, copy and paste it from these instructions.\n",
    "        Then, tokenize it using regexp_tokenize(). \n",
    "    Tokenize only the emoji in german_text. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use regexp_tokenize() to tokenize the emoji.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Wonderful!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "holy_grail = scene_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done, and congratulations on finishing Chapter 1! See you in Chapter 2, where you'll begin learning about topic identification!\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Charting practice\n",
    "\n",
    "Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
    "\n",
    "Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.\n",
    "\n",
    "You have access to the entire script in the variable holy_grail. Go for it!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split the script holy_grail into lines using the newline ('\\n') character.\n",
    "    Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1. The pattern has been written for you.\n",
    "    Use a list comprehension to tokenize lines with regexp_tokenize(), keeping only words. Recall that the pattern for words is \"\\w+\".\n",
    "    Use a list comprehension to create a list of line lengths called line_num_words.\n",
    "        Use t_line as your iterator variable to iterate over tokenized_lines, and then len() function to compute line lengths.\n",
    "    Plot a histogram of line_num_words using plt.hist(). Don't forgot to use plt.show() as well to display the plot.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace prompts using re.sub()\n",
    "pattern = r\"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', line) for line in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [nltk.regexp_tokenize(line, pattern=\"\\w+\") for line in lines]\n",
    "\n",
    "# Create a list of line lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of line_num_words\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done, and congratulations on finishing Chapter 1! See you in Chapter 2, where you'll begin learning about topic identification!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
