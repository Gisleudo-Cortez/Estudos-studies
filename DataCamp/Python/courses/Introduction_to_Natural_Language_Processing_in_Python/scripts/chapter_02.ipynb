{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Introduction_to_Natural_Language_Processing_in_Python/datasets/'\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_data + 'Wikipedia articles/wiki_text_debugging.txt') as file:\n",
    "    article = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 69), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work!\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Building a Counter with bag-of-words\n",
    "\n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "word_tokenize has been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import Counter from collections.\n",
    "    Use word_tokenize() to split the article into tokens.\n",
    "    Use a list comprehension with t as the iterator variable to convert all the tokens into lowercase. The .lower() method converts text into lowercase.\n",
    "    Create a bag-of-words counter called bow_simple by using Counter() with lower_tokens as the argument.\n",
    "    Use the .most_common() method of bow_simple to print the 10 most common tokens.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nero/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nero/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 39), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work!\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Text preprocessing practice\n",
    "\n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "\n",
    "You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the WordNetLemmatizer class from nltk.stem.\n",
    "    Create a list alpha_only that contains only alphabetical characters. You can use the .isalpha() method to check for this.\n",
    "    Create another list called no_stops consisting of words from alpha_only that are not contained in english_stops.\n",
    "    Initialize a WordNetLemmatizer object called wordnet_lemmatizer and use its .lemmatize() method on the tokens in no_stops to create a new list called lemmatized.\n",
    "    Create a new Counter called bow with the lemmatized words.\n",
    "    Lastly, print the 10 most common tokens.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import  stopwords\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_1 = article # wiki_text_debugging.txt\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_bug.txt') as file:\n",
    "    article_2 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_computer.txt') as file:\n",
    "    article_3 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_crash.txt') as file:\n",
    "    article_4 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_debugger.txt') as file:\n",
    "    article_5 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_exception.txt') as file:\n",
    "    article_6 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_hopper.txt') as file:\n",
    "    article_7 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_language.txt') as file:\n",
    "    article_8 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_malware.txt') as file:\n",
    "    article_9 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_program.txt') as file:\n",
    "    article_10 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_reversing.txt') as file:\n",
    "    article_11 = file.read()\n",
    "    file.close()\n",
    "\n",
    "with open(path_data + 'Wikipedia articles/wiki_text_software.txt') as file:\n",
    "    article_12 = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = [article_1, article_2, article_3, article_4, article_5, article_6, article_7, article_8, article_9, article_10, article_11, article_12]\n",
    "articles = []\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for article in article_list:\n",
    "    tokens = word_tokenize(article.lower())\n",
    "    filtered_tokens = [token for token in tokens if re.match(r'^[a-zA-Z]+$', token) and token not in stop_words]\n",
    "    articles.append(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'report'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "[(0, 2), (23, 8), (24, 1), (25, 1), (40, 2), (54, 1), (55, 1), (58, 1), (74, 2), (76, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work!\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Creating and querying a corpus with gensim\n",
    "\n",
    "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
    "\n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import Dictionary from gensim.corpora.dictionary.\n",
    "    Initialize a gensim Dictionary with the tokens in articles.\n",
    "    Obtain the id for \"computer\" from dictionary. To do this, use its .token2id method which returns ids from text, and then chain .get() which returns tokens from ids. Pass in \"computer\" as an argument to .get().\n",
    "    Use a list comprehension in which you iterate over articles to create a gensim MmCorpus from dictionary.\n",
    "        In the output expression, use the .doc2bow() method on dictionary with article as the argument.\n",
    "    Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular words in the 5th article\n",
      "debugger 24\n",
      "debuggers 22\n",
      "debugging 18\n",
      "program 15\n",
      "software 9\n",
      "========================\n",
      "Most popular words in total\n",
      "computer 597\n",
      "software 451\n",
      "cite 322\n",
      "ref 259\n",
      "code 235\n",
      "program 226\n",
      "programming 217\n",
      "language 189\n",
      "may 189\n",
      "system 186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGood work!\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Gensim bag-of-words\n",
    "\n",
    "Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
    "\n",
    "You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.\n",
    "\n",
    "    defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.\n",
    "\n",
    "    itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists).\n",
    "\n",
    "The fifth document from corpus is stored in the variable doc, which has been sorted in descending order.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Using the first for loop, print the top five words of bow_doc using each word_id with the dictionary alongside word_count.\n",
    "        The word_id can be accessed using the .get() method of dictionary.\n",
    "\n",
    "    Create a defaultdict called total_word_count in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count).\n",
    "        Remember to specify int when creating the defaultdict, and inside the second for loop, increment each word_id of total_word_count by word_count.\n",
    "---\n",
    "\n",
    "    Create a sorted list from the defaultdict, using words across the entire corpus. To achieve this, use the .items() method on total_word_count inside sorted().\n",
    "    Similar to how you printed the top five words of bow_doc earlier, print the top five words of sorted_word_count as well as the number of occurrences of each word across all the documents.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "print(\"Most popular words in the 5th article\")\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "print(\"=\"*24 + '\\n'+'Most popular words in total')\n",
    "for word_id, word_count in sorted_word_count[:10]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.036658409333692514), (24, 0.023001037763350884), (25, 0.011284671576627325), (40, 0.02256934315325465), (54, 0.006023040412486634)]\n",
      "debuggers 0.4032425026706177\n",
      "debugger 0.3482889381100502\n",
      "kumar 0.260125579695942\n",
      "debug 0.18400830210680708\n",
      "reverse 0.1610072643434562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work!\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Tf-idf with Wikipedia\n",
    "\n",
    "Now it's your turn to determine new significant terms for your corpus by applying gensim's tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - dictionary, corpus, and doc. Will tf-idf make for more interesting results on the document level?\n",
    "\n",
    "TfidfModel has been imported for you from gensim.models.tfidfmodel\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Initialize a new TfidfModel called tfidf using corpus.\n",
    "    Use doc to calculate the weights. You can do this by passing [doc] to tfidf.\n",
    "    Print the first five term ids with weights.\n",
    "---\n",
    "\n",
    "    Sort the term ids and weights in a new list from highest to lowest weight. This has been done for you.\n",
    "    Using your pre-existing dictionary, print the top five weighted words (term_id) from sorted_tfidf_weights, along with their weighted score (weight).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
