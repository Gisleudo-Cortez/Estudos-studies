{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T13:23:07.488681Z",
     "iopub.status.busy": "2023-06-05T13:23:07.487177Z",
     "iopub.status.idle": "2023-06-05T13:23:08.462115Z",
     "shell.execute_reply": "2023-06-05T13:23:08.460286Z",
     "shell.execute_reply.started": "2023-06-05T13:23:07.488588Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Extreme_Gradient_Boosting_with_XGBoost/datasets/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T13:26:43.030550Z",
     "iopub.status.busy": "2023-06-05T13:26:43.030197Z",
     "iopub.status.idle": "2023-06-05T13:26:43.046244Z",
     "shell.execute_reply": "2023-06-05T13:26:43.045535Z",
     "shell.execute_reply.started": "2023-06-05T13:26:43.030525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "df = pd.read_csv(path_data + 'ames_housing_trimmed_processed.csv')\n",
    "\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T13:26:57.364395Z",
     "iopub.status.busy": "2023-06-05T13:26:57.363866Z",
     "iopub.status.idle": "2023-06-05T13:26:57.796745Z",
     "shell.execute_reply": "2023-06-05T13:26:57.795895Z",
     "shell.execute_reply.started": "2023-06-05T13:26:57.364345Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.299752\n",
      "1                   10  34774.194090\n",
      "2                   15  32895.099185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAwesome! As you can see, increasing the number of boosting rounds decreases the RMSE.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Tuning the number of boosting rounds\n",
    "\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a DMatrix called housing_dmatrix from X and y.\n",
    "    Create a parameter dictionary called params, passing in the appropriate \"objective\" (\"reg:linear\") and \"max_depth\" (set it to 3).\n",
    "    Iterate over num_rounds inside a for loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (curr_num_rounds) to xgb.cv() as the argument to num_boost_round.\n",
    "    Append the final boosting round RMSE for each cross-validated XGBoost model to the final_rmse_per_round list.\n",
    "    num_rounds and final_rmse_per_round have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data = X, label = y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! As you can see, increasing the number of boosting rounds decreases the RMSE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T13:30:53.590505Z",
     "iopub.status.busy": "2023-06-05T13:30:53.589837Z",
     "iopub.status.idle": "2023-06-05T13:30:54.036379Z",
     "shell.execute_reply": "2023-06-05T13:30:54.035860Z",
     "shell.execute_reply.started": "2023-06-05T13:30:53.590428Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.635216      403.633062   142640.653507     705.559723\n",
      "1     103057.033818       73.768079   104907.664683     111.117033\n",
      "2      75975.967655      253.727043    79262.056654     563.766693\n",
      "3      57420.530642      521.658273    61620.137859    1087.693428\n",
      "4      44552.956483      544.170426    50437.560906    1846.446643\n",
      "5      35763.948865      681.796675    43035.659539    2034.471115\n",
      "6      29861.464164      769.571418    38600.880800    2169.796804\n",
      "7      25994.675122      756.520639    36071.817710    2109.795408\n",
      "8      23306.836299      759.237848    34383.186387    1934.547433\n",
      "9      21459.770256      745.624640    33509.140338    1887.375358\n",
      "10     20148.721060      749.612186    32916.806725    1850.893437\n",
      "11     19215.382607      641.387200    32197.833474    1734.456654\n",
      "12     18627.388962      716.256240    31770.852340    1802.154296\n",
      "13     17960.695080      557.043324    31482.782172    1779.124406\n",
      "14     17559.736640      631.413137    31389.990252    1892.320326\n",
      "15     17205.713357      590.171774    31302.883291    1955.165882\n",
      "16     16876.571801      703.631953    31234.058914    1880.706205\n",
      "17     16597.662170      703.677363    31318.347820    1828.860754\n",
      "18     16330.460661      607.274258    31323.634893    1775.909992\n",
      "19     16005.972387      520.470815    31204.135450    1739.076237\n",
      "20     15814.300847      518.604822    31089.863868    1756.022175\n",
      "21     15493.405856      505.616461    31047.997697    1624.673447\n",
      "22     15270.734205      502.018639    31056.916210    1668.043691\n",
      "23     15086.381896      503.913078    31024.984403    1548.985086\n",
      "24     14917.608289      486.206137    30983.685376    1663.131135\n",
      "25     14709.589477      449.668262    30989.476981    1686.667218\n",
      "26     14457.286251      376.787759    30952.113767    1613.172390\n",
      "27     14185.567149      383.102597    31066.901381    1648.534545\n",
      "28     13934.066721      473.465580    31095.641882    1709.225578\n",
      "29     13749.644941      473.670743    31103.886799    1778.879849\n",
      "30     13549.836644      454.898742    30976.084872    1744.514518\n",
      "31     13413.484678      399.603422    30938.469354    1746.053330\n",
      "32     13275.915700      415.408595    30931.000055    1772.469405\n",
      "33     13085.878211      493.792795    30929.056846    1765.541040\n",
      "34     12947.181279      517.790033    30890.629160    1786.510472\n",
      "35     12846.027264      547.732747    30884.493051    1769.728787\n",
      "36     12702.378727      505.523140    30833.542124    1691.002007\n",
      "37     12532.244170      508.298300    30856.688154    1771.445485\n",
      "38     12384.055037      536.224929    30818.016568    1782.785175\n",
      "39     12198.443769      545.165604    30839.393263    1847.326671\n",
      "40     12054.583621      508.841802    30776.965294    1912.780332\n",
      "41     11897.036784      477.177932    30794.702627    1919.675130\n",
      "42     11756.221708      502.992363    30780.956160    1906.820178\n",
      "43     11618.846752      519.837483    30783.754746    1951.260120\n",
      "44     11484.080227      578.428500    30776.731276    1953.447810\n",
      "45     11356.552654      565.368946    30758.543732    1947.454939\n",
      "46     11193.557745      552.298986    30729.971937    1985.699239\n",
      "47     11071.315547      604.090125    30732.663173    1966.997252\n",
      "48     10950.778492      574.862853    30712.241251    1957.750615\n",
      "49     10824.865446      576.665678    30720.853939    1950.511037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work!\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Automated boosting round selection using early_stopping\n",
    "\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Perform 3-fold cross-validation with early stopping and \"rmse\" as your metric. Use 10 early stopping rounds and 50 boosting rounds. Specify a seed of 123 and make sure the output is a pandas DataFrame. Remember to specify the other parameters such as dtrain, params, and metrics.\n",
    "    \n",
    "    Print cv_results.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain = housing_dmatrix,params = params, seed=123, early_stopping_rounds = 10, num_boost_round = 50, as_pandas = True, nfold = 3, metrics = 'rmse')\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T16:37:51.109135Z",
     "iopub.status.busy": "2023-06-05T16:37:51.108910Z",
     "iopub.status.idle": "2023-06-05T16:37:51.856915Z",
     "shell.execute_reply": "2023-06-05T16:37:51.856421Z",
     "shell.execute_reply.started": "2023-06-05T16:37:51.109116Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.402543\n",
      "1  0.010  179932.183986\n",
      "2  0.100   79759.411808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work!\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Tuning eta\n",
    "\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a list called eta_vals to store the following \"eta\" values: 0.001, 0.01, and 0.1.\n",
    "    Iterate over your eta_vals list using a for loop.\n",
    "    In each iteration of the for loop, set the \"eta\" key of params to be equal to curr_val. Then, perform 3-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\n",
    "    Append the final round RMSE to the best_rmse list.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix, params = params, num_boost_round = 10,early_stopping_rounds = 5, nfold = 3, metrics = 'rmse', seed = 123, as_pandas = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T16:40:40.946348Z",
     "iopub.status.busy": "2023-06-05T16:40:40.946139Z",
     "iopub.status.idle": "2023-06-05T16:40:41.342188Z",
     "shell.execute_reply": "2023-06-05T16:40:41.341638Z",
     "shell.execute_reply.started": "2023-06-05T16:40:40.946330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.469464\n",
      "1          5  35596.599504\n",
      "2         10  36065.547345\n",
      "3         20  36739.576068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work!\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Tuning max_depth\n",
    "\n",
    "In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a list called max_depths to store the following \"max_depth\" values: 2, 5, 10, and 20.\n",
    "    Iterate over your max_depths list using a for loop.\n",
    "    Systematically vary \"max_depth\" in each iteration of the for loop and perform 2-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:squarederror\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2,5,10,20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix, params = params, nfold = 2, early_stopping_rounds=5, num_boost_round = 10, metrics = 'rmse', seed = 123, as_pandas = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T16:45:18.809785Z",
     "iopub.status.busy": "2023-06-05T16:45:18.809245Z",
     "iopub.status.idle": "2023-06-05T16:45:19.107552Z",
     "shell.execute_reply": "2023-06-05T16:45:19.106829Z",
     "shell.execute_reply.started": "2023-06-05T16:45:18.809740Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  50033.734626\n",
      "1               0.5  35656.185735\n",
      "2               0.8  36399.002280\n",
      "3               1.0  35836.044343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAwesome! There are several other individual parameters that you can tune, such as \"subsample\", which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Tuning colsample_bytree\n",
    "\n",
    "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a list called colsample_bytree_vals to store the values 0.1, 0.5, 0.8, and 1.\n",
    "    Systematically vary \"colsample_bytree\" and perform cross-validation, exactly as you did with max_depth and eta previously.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1,0.5,0.8,1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! There are several other individual parameters that you can tune, such as \"subsample\", which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T16:55:16.587319Z",
     "iopub.status.busy": "2023-06-05T16:55:16.586540Z",
     "iopub.status.idle": "2023-06-05T16:55:16.592182Z",
     "shell.execute_reply": "2023-06-05T16:55:16.591117Z",
     "shell.execute_reply.started": "2023-06-05T16:55:16.587278Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T16:55:18.402209Z",
     "iopub.status.busy": "2023-06-05T16:55:18.401575Z",
     "iopub.status.idle": "2023-06-05T16:55:20.302861Z",
     "shell.execute_reply": "2023-06-05T16:55:20.293592Z",
     "shell.execute_reply.started": "2023-06-05T16:55:18.402172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  29916.017850830365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExcellent work! Next up, RandomizedSearchCV\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Grid search with XGBoost\n",
    "\n",
    "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a parameter grid called gbm_param_grid that contains a list of \"colsample_bytree\" values (0.3, 0.7), a list with a single value for \"n_estimators\" (50), and a list of 2 \"max_depth\" (2, 5) values.\n",
    "    Instantiate an XGBRegressor object called gbm.\n",
    "    Create a GridSearchCV object called grid_mse, passing in: the parameter grid to param_grid, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n",
    "    Fit the GridSearchCV object to X and y.\n",
    "    Print the best parameter values and lowest RMSE, using the .best_params_ and .best_score_ attributes, respectively, of grid_mse.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(gbm, param_grid = gbm_param_grid, scoring = 'neg_mean_squared_error', cv = 4, verbose = 1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Next up, RandomizedSearchCV\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T16:59:53.741964Z",
     "iopub.status.busy": "2023-06-05T16:59:53.740481Z",
     "iopub.status.idle": "2023-06-05T16:59:56.486547Z",
     "shell.execute_reply": "2023-06-05T16:59:56.482039Z",
     "shell.execute_reply.started": "2023-06-05T16:59:53.741888Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found:  29998.4522530019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSuperb!\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Random search with XGBoost\n",
    "\n",
    "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a parameter grid called gbm_param_grid that contains a list with a single value for 'n_estimators' (25), and a list of 'max_depth' values between 2 and 11 for 'max_depth' - use range(2, 12) for this.\n",
    "    Create a RandomizedSearchCV object called randomized_mse, passing in: the parameter grid to param_distributions, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, 5 to n_iter, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n",
    "    Fit the RandomizedSearchCV object to X and y.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': np.arange(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(gbm, param_distributions = gbm_param_grid, scoring = 'neg_mean_squared_error', n_iter = 5, cv = 4, verbose = 1)\n",
    "\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Superb!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
