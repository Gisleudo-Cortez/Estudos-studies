{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Web_scraping_in_python/datasets/'\n",
    "\n",
    "from scrapy import Selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class(c):\n",
    "  newc = c()\n",
    "  meths = dir(newc)\n",
    "  if 'name' in meths:\n",
    "    print(\"Your spider class name is:\", newc.name)\n",
    "  if 'from_crawler' in meths:\n",
    "    print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "  else:\n",
    "    print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job setting up a class so that it inherits the tools from scrapy.Spider!\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Inheriting the Spider\n",
    "\n",
    "When learning about scrapy spiders, we saw that the main portion of the code for us to adjust is the class for the spider. To help build some familiarity of the class, you will complete a short piece of code to complete a toy-model of the spider class code. We've omitted the code that would actually run the spider, only including the pieces necessary to create the class.\n",
    "\n",
    "As mentioned in the lesson, a class is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will inherit methods from a different class. That's what we do in the spider class.\n",
    "\n",
    "We wrote the function inspect_class to look at the your class once you're done, if you'd like to test your solution!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Pass scrapy.Spider as an argument to the class YourSpider; this will make it so that YourSpider inherits the methods from scrapy.Spider.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job setting up a class so that it inherits the tools from scrapy.Spider!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  meths = dir( newc )\n",
    "  if 'start_requests' in meths:\n",
    "    print( \"The start_requests method yields the following urls:\" )\n",
    "    for u in newc.start_requests():\n",
    "      print(  \"\\t-\", u )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start_requests method yields the following urls:\n",
      "\t- https://www.datacamp.com\n",
      "\t- https://scrapy.org\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat! We'll discuss more about the start_requests method and how it uses yield in the next lesson.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Hurl the URLs\n",
    "\n",
    "In the next lesson we will talk about the start_requests method within the spider class. In this quick exercise, we ask you to change around a variable within the start_requests method which foreshadows some of what we will be learning in the next lesson. Basically, we want you to start becoming comfortable turning some of the wheels within a spider class; in this case, making a list of urls within the start_requests method.\n",
    "\n",
    "We've written a function inspect_class which will print out the list of elements you have in the urls variable within the start_requests method.\n",
    "\n",
    "Note: in the next several exercises, you will write code to complete your spider class, but the code does not yet include the pieces to actually run the spider; that will come at the end.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the blank within the start_requests method to assign the variable urls a list with the two strings: \"https://www.datacamp.com\" and\"https://scrapy.org\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\",\"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! We'll discuss more about the start_requests method and how it uses yield in the next lesson.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    newc.start_requests()\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling start_requests in YourSpider prints out: Hello World!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAwesome! Self referencing in classes can be a bit confusing, but you nailed it!! (And, don't worry, we don't need to worry too much more about this subject for this course)!!\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Self Referencing is Classy\n",
    "\n",
    "You probably have noticed that within the spider class, we always input the argument self in the start_requests and parse methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method parse within the start_requests method, we would need to write self.parse rather than just parse; what writing self does is tell the code: \"Look in the same class as start_requests for a method called parse to use.\"\n",
    "\n",
    "In this exercise you will get a chance to play with this \"self referencing\".\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the required scrapy object into the class YourSpider needed to create the scrapy spider.\n",
    "    Pass the string argument \"Hello World!\" to fill in the blank in the start_requests method to use the print_msg method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( 'Hello World!' )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! Self referencing in classes can be a bit confusing, but you nailed it!! (And, don't worry, we don't need to worry too much more about this subject for this course)!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    y = list( newc.start_requests() )\n",
    "    first_yield = y[0]\n",
    "    print( \"The url you would scrape is:\", first_yield.url )\n",
    "    cb = first_yield.callback\n",
    "    print( \"The name of the callback method you called is:\", cb.__name__ )\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The url you would scrape is: https://www.datacamp.com\n",
      "The name of the callback method you called is: parse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! You've been able to use the yielded scrapy.Request to direct to the correct URL and parsing method!\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Starting with Start Requests\n",
    "\n",
    "In the last lesson we learned about setting up the start_requests method within a scrapy spider. Here we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method. What we want is for you to start becomming familiar with the arguments you pass into the scrapy.Request call within start_requests.\n",
    "\n",
    "As before, we have created the function inspect_class to examine what you are yielding in start_requests.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the required scrapy object into the class YourSpider needed to create the scrapy spider.\n",
    "\n",
    "    Fill in the blank in the yielded scrapy.Request call within the start_requests method so that the URL this spider would start scraping is \"https://www.datacamp.com\" and would use the parse method (within the YourSpider class) as the method to parse the website.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = 'https://www.datacamp.com', callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You've been able to use the yielded scrapy.Request to direct to the correct URL and parsing method!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req = list( news.start_requests() )[0]\n",
    "    url = req.url\n",
    "    html = requests.get( url ).content\n",
    "    response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "    author_names = req.callback( response )\n",
    "    print( 'You have collected the author names:')\n",
    "    for a in author_names:\n",
    "      print('\\t-', a )\n",
    "  except:\n",
    "    print( 'Oh no! Something went wrong with the code. Keep trying!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "\n",
    "html = requests.get(url_short)\n",
    "\n",
    "body = html.text\n",
    "\n",
    "sel = Selector(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! Something went wrong with the code. Keep trying!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNicely done! Notice that you could have written the code for this exercise in the previous chapter, the only difference here is that the response variable appeared within your spider.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Pen Names\n",
    "\n",
    "In this exercise, we have set up a spider class which, when finished, will retrieve the author names from a shortened version of the DataCamp course directory. The URL for the shortened version is stored in the variable url_short. Your job will be to create the list of extracted author names in the parse method of the spider.\n",
    "\n",
    "Two things you should know:\n",
    "\n",
    "    You will be using the response object and the css method here.\n",
    "    The course author names are defined by the text within the paragraph p elements belonging to the class course-block__author-name\n",
    "\n",
    "You can inspect the spider using the function inspect_spider() that we built for you -- it will print out the author names you find!\n",
    "\n",
    "Note that this and the remaining exercises in this chapter may take some time to load.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the required arguments to the parse method so that it will work as required when called in the start_requests method.\n",
    "    Within the parse method, create a variable author_names, which is a list of strings created by extracting the text from the paragraph elements belonging to the class course-block__author-name.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = html.url , callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css(' p.course-block__author-name::text').extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nicely done! Notice that you could have written the code for this exercise in the previous chapter, the only difference here is that the response variable appeared within your spider.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! Something went wrong with the code. Keep trying!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThat was a long bit of code that you had to read through, but you did it! And you got the crawler, well, crawling!!!\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Crawler Time\n",
    "\n",
    "This will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the parse method; from there, it will follow those links to extract the course descriptions from each course page in the parse_descr method, and put these descriptions into the list course_descrs. Your job is to complete the code so that the spider runs as desired!\n",
    "\n",
    "We have created a function inspect_spider which will print out one of the course descriptions you scrape (if done correctly)!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the two blanks below (one in each of the parsing methods) with the appropriate entries so that the spider can move from the first parsing method to the second correctly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url = link, callback = self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That was a long bit of code that you had to read through, but you did it! And you got the crawler, well, crawling!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previewCourses( dc_dict, n = 3 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    for i,ct in enumerate(dc_dict[t]):\n",
    "      print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 08:02:44 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n",
      "2023-07-28 08:02:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.3 (main, Jun  5 2023, 09:32:32) [GCC 13.1.1 20230429], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.2, Platform Linux-6.4.5-zen1-1-zen-x86_64-with-glibc2.37\n",
      "2023-07-28 08:02:44 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-07-28 08:02:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2023-07-28 08:02:44 [scrapy.extensions.telnet] INFO: Telnet Password: 341e8f83d58d3625\n",
      "2023-07-28 08:02:44 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-07-28 08:02:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-07-28 08:02:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-07-28 08:02:44 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-07-28 08:02:44 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-07-28 08:02:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-07-28 08:02:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026\n",
      "2023-07-28 08:02:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-07-28 08:02:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 7020,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2464052,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 1.927175,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 7, 28, 11, 2, 46, 946744),\n",
      " 'httpcompression/response_bytes': 1634,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/404': 1,\n",
      " 'log_count/DEBUG': 13,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 122822656,\n",
      " 'memusage/startup': 122822656,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2023, 7, 28, 11, 2, 45, 19569)}\n",
      "2023-07-28 08:02:46 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A preview of DataCamp Courses:\n",
      "---------------------------------------\n",
      "\n",
      "TITLE: Intro to Python for Data Science\n",
      "\tChapter 1: Python Basics\n",
      "\tChapter 2: Functions and Packages\n",
      "\tChapter 3: Python Lists\n",
      "\tChapter 4: NumPy\n",
      "\tChapter 5: Python Basics\n",
      "\tChapter 6: Python Lists\n",
      "\tChapter 7: Functions and Packages\n",
      "\tChapter 8: NumPy\n",
      "\n",
      "TITLE: Introduction to R\n",
      "\tChapter 1: Intro to basics\n",
      "\tChapter 2: Vectors\n",
      "\tChapter 3: Matrices\n",
      "\tChapter 4: Factors\n",
      "\tChapter 5: Data frames\n",
      "\tChapter 6: Lists\n",
      "\n",
      "TITLE: Reporting with R Markdown\n",
      "\tChapter 1: Authoring R Markdown Reports\n",
      "\tChapter 2: Embedding Code\n",
      "\tChapter 3: Compiling Reports\n",
      "\tChapter 4: Configuring R Markdown (optional)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! While there was a lot of code for you to read through, you should be somewhat familiar now with the layout. Your job here was just to help fill in the dictionary items, but really we wanted to give you a chance to look through the actual code (of a fully running spider!!).\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Time to Run\n",
    "\n",
    "In the last lesson, we went through creating an entire web-crawler to access course information from each course in the DataCamp course directory. However, the lesson seemed to stop without a climax, because we didn't play with the code after finishing the parsing methods.\n",
    "\n",
    "The point of this exercise is to remedy that!\n",
    "\n",
    "The code we give you to look at in this and the next exercise is long, because its the entire spider that took us the lesson to create! However, don't be intimidated! The point of these two exercises is to give you a very easy task to complete, with the hope that you will look at and run the code for this spider. That way, even though it is long, you will have a grasp of it!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the one blank at the end of the parse_pages methods to assign the chapter titles to the dictionary whose key is the corresponding course title.\n",
    "\n",
    "NOTE: If you hit Run Code, you must Reset to Sample Code to successfully use Run Code again!!\n",
    "\"\"\"\n",
    "# Remove deprecation warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=scrapy.exceptions.ScrapyDeprecationWarning)\n",
    "# solution\n",
    "\n",
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "process.stop()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! While there was a lot of code for you to read through, you should be somewhat familiar now with the layout. Your job here was just to help fill in the dictionary items, but really we wanted to give you a chance to look through the actual code (of a fully running spider!!).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 08:02:47 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n",
      "2023-07-28 08:02:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.3 (main, Jun  5 2023, 09:32:32) [GCC 13.1.1 20230429], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.2, Platform Linux-6.4.5-zen1-1-zen-x86_64-with-glibc2.37\n",
      "2023-07-28 08:02:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-07-28 08:02:47 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2023-07-28 08:02:47 [scrapy.extensions.telnet] INFO: Telnet Password: 868a8e2cdfd6279b\n",
      "2023-07-28 08:02:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-07-28 08:02:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-07-28 08:02:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-07-28 08:02:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-07-28 08:02:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-07-28 08:02:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-07-28 08:02:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m process \u001b[39m=\u001b[39m CrawlerProcess()\n\u001b[1;32m     63\u001b[0m process\u001b[39m.\u001b[39mcrawl(DC_Description_Spider)\n\u001b[0;32m---> 64\u001b[0m process\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m     65\u001b[0m process\u001b[39m.\u001b[39mstop()\n\u001b[1;32m     67\u001b[0m \u001b[39m# Print a preview of courses\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/scrapy/crawler.py:390\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    388\u001b[0m tp\u001b[39m.\u001b[39madjustPoolsize(maxthreads\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mgetint(\u001b[39m\"\u001b[39m\u001b[39mREACTOR_THREADPOOL_MAXSIZE\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    389\u001b[0m reactor\u001b[39m.\u001b[39maddSystemEventTrigger(\u001b[39m\"\u001b[39m\u001b[39mbefore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mshutdown\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop)\n\u001b[0;32m--> 390\u001b[0m reactor\u001b[39m.\u001b[39;49mrun(installSignalHandlers\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/twisted/internet/base.py:1317\u001b[0m, in \u001b[0;36m_SignalReactorMixin.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, installSignalHandlers: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstartRunning(installSignalHandlers\u001b[39m=\u001b[39;49minstallSignalHandlers)\n\u001b[1;32m   1318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/twisted/internet/base.py:1299\u001b[0m, in \u001b[0;36m_SignalReactorMixin.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[39mExtend the base implementation in order to remember whether signal\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39mhandlers should be installed later.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[39m    installed during startup.\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_installSignalHandlers \u001b[39m=\u001b[39m installSignalHandlers\n\u001b[0;32m-> 1299\u001b[0m ReactorBase\u001b[39m.\u001b[39;49mstartRunning(cast(ReactorBase, \u001b[39mself\u001b[39;49m))\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/twisted/internet/base.py:843\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 843\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    844\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_started \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopped \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "DataCamp Descriptions\n",
    "\n",
    "Like the previous exercise, the code here is long since you are working with an entire web-crawling spider! But again, don't let the amount of code intimidate you, you have a handle on how spiders work now, and you are perfectly capable to complete the easy task for you here!\n",
    "\n",
    "As in the previous exercise, we have created a function previewCourses which lets you preview the output of the spider, but you can always just explore the dictionary dc_dict too after you run the code.\n",
    "\n",
    "In this exercise, you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph p element which belongs to the class course__description (two underlines).\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the one blank below in the parse_pages method with a CSS Locator string which directs to the text within the paragraph p element which belongs to the class course__description.\n",
    "\n",
    "NOTE: If you hit Run Code, you must Reset to Sample Code to successfully use Run Code again!!\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css(\"div.course-block\")\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "process.stop()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Amazing! As mentioned in the lessons, most of the code you will write for your spider happens in the parsing function(s). Even though you did not have to write much code, please noticed that the code contained in the parsing function(s) is what we've been working to build on throughout this course.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Capstone Crawler\n",
    "\n",
    "This exercise gives you a chance to show off what you've learned! In this exercise, you will write the parse function for a spider and then fill in a few blanks to finish off the spider. On the course directory page of DataCamp, each listed course has a title and a short course description. This spider will be used to scrape the course directory to extract the course titles and short course descriptions. You will not need to follow any links this time. Everything you need to know is:\n",
    "\n",
    "    The course titles are defined by the text within an h4 element whose class contains the string block__title (double underline).\n",
    "    The short course descriptions are defined by the text within a paragraph p element whose class contains the string block__description (double underline).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Assign to the variable crs_titles the extracted list of course titles on the DataCamp course directory page. You should use the contains call within your XPath, and your XPath string should point to the text of the selected objects.\n",
    "    Assign to the variable crs_descrs the extracted list of short course descriptions. You should use the contains call within your XPath. You should use the contains call within your XPath, and your XPath string should point to the text of the selected objects.\n",
    "\n",
    "(Since we want a list of extracted data, we will use the extract() call (rather than extract_first()). )\n",
    "---\n",
    "\n",
    "    Fill in the four blanks below with the necessary entries to complete your spider.\n",
    "\n",
    "Note: If you hit Run Code, you will need to Reset to Sample before hitting Run Code again.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = 'yourspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
    "      dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "process.stop()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You just finished your Spider! Congratulations! You had already written the parsing code, and here you just filled in the few moving parts surrounding the setup of the spider.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
