{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<script.py> output:\n",
      "    i’m not insane, my mother had me tested. ove to the back of the sent of the fall of my resent of the way of the find that i can have to the locken and i was a sense to the little of the contric of the start of the child be the look and the s\n",
      "    \n",
      "    \n",
      "    Your poem lies below: \n",
      "    \n",
      "    \n",
      "    May thy beauty forever remain,\n",
      "    without thy love, and me at self i wast now.\n",
      "    \n",
      "    \n",
      "    \n",
      "    shen make the wail, and then i hase no love,\n",
      "    be the was my sare and the world, or my heart thee thee,\n",
      "    rone the beauty my then have dore me song,\n",
      "    and from the will of a lover of self and deet,\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nReally nice, right? There are many possible applications to text generation, and by using characters as tokens you can create good models with less data, meaning that with tens of thousands of example sentences you can create a model that has interesting results. In this first model, you can see that the generated phrase does not make any sense. Don't worry, the trained model used very few data and was trained for just 10 epochs. By using more data (sentence examples) and training longer, you can achieve much better results.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Text generation examples\n",
    "\n",
    "In this exercise, you are going to experiment on two pre-trained models for text generation.\n",
    "\n",
    "The first model will generate one phrase based on the character Sheldon of The Big Bang Theory TV show, and the second model will generate a Shakespeare poems up to 400 characters.\n",
    "\n",
    "The models are loaded on the sheldon_model and poem_model variables. Also, two custom functions to help generate text are available: generate_sheldon_phrase() and generate_poem(). Both receive the pre-trained model and a context string as parameters.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use pre-defined function generate_sheldon_phrase() with parameters sheldon_model and sheldon_context and store the output in the sheldon_phrase variable.\n",
    "    Print the obtained phrase.\n",
    "    Store the given text into the poem_context variable.\n",
    "    Print the poem generated by applying the function generate_poem() with the poem_model and poem_context parameters.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\"\"\"# Context for Sheldon phrase\n",
    "sheldon_context = \"I’m not insane, my mother had me tested. \"\n",
    "\n",
    "# Generate one Sheldon phrase\n",
    "sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n",
    "\n",
    "# Print the phrase\n",
    "print(sheldon_phrase)\n",
    "\n",
    "# Context for poem\n",
    "poem_context = \"May thy beauty forever remain\"\n",
    "\n",
    "# Print the poem\n",
    "print(generate_poem(poem_model, poem_context))\"\"\"\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "print('''<script.py> output:\n",
    "    i’m not insane, my mother had me tested. ove to the back of the sent of the fall of my resent of the way of the find that i can have to the locken and i was a sense to the little of the contric of the start of the child be the look and the s\n",
    "    \n",
    "    \n",
    "    Your poem lies below: \n",
    "    \n",
    "    \n",
    "    May thy beauty forever remain,\n",
    "    without thy love, and me at self i wast now.\n",
    "    \n",
    "    \n",
    "    \n",
    "    shen make the wail, and then i hase no love,\n",
    "    be the was my sare and the world, or my heart thee thee,\n",
    "    rone the beauty my then have dore me song,\n",
    "    and from the will of a lover of self and deet,\n",
    "    ''')\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Really nice, right? There are many possible applications to text generation, and by using characters as tokens you can create good models with less data, meaning that with tens of thousands of example sentences you can create a model that has interesting results. In this first model, you can see that the generated phrase does not make any sense. Don't worry, the trained model used very few data and was trained for just 10 epochs. By using more data (sentence examples) and training longer, you can achieve much better results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_length = 8\n",
    "\n",
    "def encode_sequences(lines, tokenizer=pt_tokenizer, length=pt_length):\n",
    "    \"\"\"Use the tokenizer to encode the given texts\n",
    "    \n",
    "    Transform the given texts into an numpy array of index sequences padded to length `length`.\n",
    "\n",
    "    Args:\n",
    "        lines (list or numpy.array): The given texts.\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): The fitted tokenizer object. Defaults to the trained Portuguese tokenizer.\n",
    "        length (int): The length to pad the sequences. Defaults to the maximum length of the Portuguese sentences (8).\n",
    "\n",
    "    Returns:\n",
    "        The padded numpy.array of indices sequences.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_many(model, sentences):\n",
    "    \"\"\" Translate a list of sentences\n",
    "\n",
    "    Use the pre-trained model to loop over the sentences and translate one by one.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Sequential): The pre-trained NMT model.\n",
    "        sentences (list or numpy.array): The list of sentences to translate.\n",
    "    \n",
    "    Returns:\n",
    "        A list containing the translated sentences.\n",
    "    \n",
    "    \"\"\"\n",
    "    translated = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # translate encoded sentence text\n",
    "        translation = predict_one(model, sentence)\n",
    "        translated.append(translation)\n",
    "    \n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(model, source, ix_to_word=index_to_word):\n",
    "    source = source.reshape((1, source.shape[0]))\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = ix_to_word.get(i, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 452    0    0    0    0    0    0    0]\n",
      " [  23  224   68    0    0    0    0    0]\n",
      " [1374    0    0    0    0    0    0    0]\n",
      " [   1  302   31    9    0    0    0    0]\n",
      " [   2    6   31   63    0    0    0    0]\n",
      " [   2    6  155    0    0    0    0    0]\n",
      " [  35 2134    0    0    0    0    0    0]\n",
      " [   1 1525    0    0    0    0    0    0]\n",
      " [   5 1142    0    0    0    0    0    0]\n",
      " [ 578   44    0    0    0    0    0    0]]\n",
      "              Original     Translated\n",
      "0             obrigada      thank you\n",
      "1    vocês podem ficar   you may stay\n",
      "2               ataque         attack\n",
      "3  eu acredito em você  i believe you\n",
      "4     tom está em casa    tom is home\n",
      "5     tom está sozinho   tom is alone\n",
      "6        posso sentila  i can feel it\n",
      "7            eu dancei       i danced\n",
      "8     estou contratado         im wet\n",
      "9     experimenta isto       try this\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCool, isn't it? Most of the translated sentences are correct, but some are wrong like 'estou contratado' does not mean 'im drunk', instead it means 'im hired. By increasing the number of examples (only 5,000 were used on the pre-trained model), and training for more epochs you could achieve a better result.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "NMT example\n",
    "\n",
    "This exercise aims to build on the sneak peek you got of NMT at the beginning of the course. You will continue to translate Portuguese small phrases into English.\n",
    "\n",
    "Some sample sentences are available on the sentences variable and are printed on the console.\n",
    "\n",
    "Also, a pre-trained model is available on the model variable and you will use two custom functions to simplify some steps:\n",
    "\n",
    "    encode_sequences(): Change texts into sequence of numerical indexes and pad them.\n",
    "    translate_many(): Uses the pre-trained model to translate a list of sentences from Portuguese into English. Later you will code this function yourself.\n",
    "\n",
    "For more details on the functions, use help(). The package pandas is loaded as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use the encode_sequences() function to pre-process the texts and save the results in the X variable.\n",
    "    Translate the sentences using the translate_many() function by passing X as a parameter.\n",
    "    Create a pd.DataFrame() with the original and translated lists as columns.\n",
    "    Print the data frame.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\"\"\"# Transform text into sequence of indexes and pad\n",
    "X = encode_sequences(sentences)\n",
    "\n",
    "# Print the sequences of indexes\n",
    "print(X)\n",
    "\n",
    "# Translate the sentences\n",
    "translated = translate_many(model, X)\n",
    "\n",
    "# Create pandas DataFrame with original and translated\n",
    "df = pd.DataFrame({'Original': sentences, 'Translated': translated})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\"\"\"\n",
    "\n",
    "\n",
    "print(\"\"\"[[ 452    0    0    0    0    0    0    0]\n",
    " [  23  224   68    0    0    0    0    0]\n",
    " [1374    0    0    0    0    0    0    0]\n",
    " [   1  302   31    9    0    0    0    0]\n",
    " [   2    6   31   63    0    0    0    0]\n",
    " [   2    6  155    0    0    0    0    0]\n",
    " [  35 2134    0    0    0    0    0    0]\n",
    " [   1 1525    0    0    0    0    0    0]\n",
    " [   5 1142    0    0    0    0    0    0]\n",
    " [ 578   44    0    0    0    0    0    0]]\n",
    "              Original     Translated\n",
    "0             obrigada      thank you\n",
    "1    vocês podem ficar   you may stay\n",
    "2               ataque         attack\n",
    "3  eu acredito em você  i believe you\n",
    "4     tom está em casa    tom is home\n",
    "5     tom está sozinho   tom is alone\n",
    "6        posso sentila  i can feel it\n",
    "7            eu dancei       i danced\n",
    "8     estou contratado         im wet\n",
    "9     experimenta isto       try this\"\"\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Cool, isn't it? Most of the translated sentences are correct, but some are wrong like 'estou contratado' does not mean 'im drunk', instead it means 'im hired. By increasing the number of examples (only 5,000 were used on the pre-trained model), and training for more epochs you could achieve a better result.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Predict next character\n",
    "\n",
    "In this exercise, you will code the function to predict the next character given a trained model. You will use the past 20 chars to predict the next one. You will learn how to train the model in the next lesson, as this step is integral before model training.\n",
    "\n",
    "This is the initial step to create rules for generating sentences, paragraphs, short texts or other blocks of text as needed.\n",
    "\n",
    "The variables n_vocab, chars_window and the dictionary index_to_char are already loaded in the environment. Also, the functions below are already created for you:\n",
    "\n",
    "    initialize_X(): Transforms the text input into a sequence of index numbers with the correct shape.\n",
    "    predict_next_char(): Gets the next character using the .predict() method of the model class and the index_to_char dictionary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the function get_next_char() and add the parameters initial_text and chars_window without default values.\n",
    "    Use initialize_X() function and pass variable char_to_index to obtain a vector of zeros to be used for prediction.\n",
    "    Use the predict_next_char() function to obtain the prediction and store it in the next_char variable.\n",
    "    Print the predicted character by applying the defined function on the given initial_text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def get_next_char(model, initial_text, chars_window, char_to_index, index_to_char):\n",
    "  \t# Initialize the X vector with zeros\n",
    "    X = initialize_X(initial_text, chars_window, char_to_index)\n",
    "    \n",
    "    # Get next character using the model\n",
    "    next_char = predict_next_char(model, X, index_to_char)\n",
    "\t\n",
    "    return next_char\n",
    "\n",
    "# Define context sentence and print the generated text\n",
    "initial_text = \"I am not insane, \"\n",
    "print(\"Next character: {0}\".format(get_next_char(model, initial_text, 20, char_to_index, index_to_char)))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "This function can help you to obtain one next character using the model. It used the previous 20 chars to make the prediction, and depending on the application you have, you may use higher or lower values. Seeing just this predicted character doesn't make much sense, that is why you will see next how to generate a full sentence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Generate sentence with context\n",
    "\n",
    "In this exercise, you are going to experiment on a pre-trained model for text generation. The model is already loaded in the environment in the model variable, as well as the initialize_params() and get_next_token() functions.\n",
    "\n",
    "This later uses the pre-trained model to predict the next character and return three variables: the next character next_char, the updated sentence res and the the shifted text seq that will be used to predict the next one.\n",
    "\n",
    "You will define a function that receives a pre-trained model and a string that will be the start of the generated sentence as inputs. This is a good practice to generate text with context. The sentence limit of 100 characters is an example, you can use other limits (or even without limit) in your applications.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Pass the initial_text variable to the initialize_params() function.\n",
    "    Create conditions to stop the loop when the counter reaches 100 or a dot (r'.') is found.\n",
    "    Pass the initial values res, seq to the get_next_token() function to obtain the next char.\n",
    "    Print the example phrase generated by the defined function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def generate_phrase(model, initial_text):\n",
    "    # Initialize variables  \n",
    "    res, seq, counter, next_char = initialize_params(initial_text)\n",
    "    \n",
    "    # Loop until stop conditions are met\n",
    "    while counter < 100 and next_char != r'.':\n",
    "      \t# Get next char using the model and append to the sentence\n",
    "        next_char, res, seq = get_next_token(model, res, seq)\n",
    "        # Update the counter\n",
    "        counter = counter + 1\n",
    "    return res\n",
    "  \n",
    "# Create a phrase\n",
    "print(generate_phrase(model, \"I am not insane, \"))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Cool! You can see that the generated text belongs to the Big Bang Theory universe. That is because the model was trained using phrases by the character Sheldon. Depending on trained data, the model will generate different phrases. You can try with other standard examples such as Sheakespeare collection to generate poems, or maybe on other famous authors from the literature. You choose! Note that the model was trained with 100 epochs and used only 20 characters to predict the next one. By incresing the chars_window and training for more epochs, you can obtain better predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Temperature 0.2: spock and me and the big back of the little of the start of the same of the beat of the start.\n",
      "    Temperature 0.8: spock and me with his ravites of my number of to be movies that.\n",
      "    Temperature 1.0: spock and me in my body.\n",
      "    Temperature 3.0: spock and me zksum.\n",
      "    Temperature 10.0: spock and me .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou can see that when the temperature is high, the text start to make less sense. If the model was trained with a larger sample and for more epochs, you would get better phrases. Also, when temperature is low, the phrases seems more realistic.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Change the probability scale\n",
    "\n",
    "In this exercise, you will see the difference in the resulted sentence when using different values of temperature to scale the probability distribution.\n",
    "\n",
    "The function generate_phrase() is an adaptation of the function you created before and is already loaded in the environment. It receives the parameters model with the pre-trained model, initial_text with the context text and temperature that is the value to scale the softmax() function.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Store the list of temperatures to the temperatures variable.\n",
    "    Loop a variable temperature over the temperatures list.\n",
    "    Generate a phrase using the pre-loaded function generate_phrase().\n",
    "    Print the temperature and the generated sentence.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\"\"\"# Define the initial text\n",
    "initial_text = \"Spock and me \"\n",
    "\n",
    "# Define a vector with temperature values\n",
    "temperatures = [0.2, 0.8, 1.0, 3.0, 10.0]\n",
    "\n",
    "# Loop over temperatures and generate phrases\n",
    "for temperature in temperatures:\n",
    "\t# Generate a phrase\n",
    "\tphrase = generate_phrase(model, initial_text, temperature)\n",
    "    \n",
    "\t# Print the phrase\n",
    "\tprint('Temperature {0}: {1}'.format(temperature, phrase))\"\"\"\n",
    "\n",
    "print(\"\"\"    Temperature 0.2: spock and me and the big back of the little of the start of the same of the beat of the start.\n",
    "    Temperature 0.8: spock and me with his ravites of my number of to be movies that.\n",
    "    Temperature 1.0: spock and me in my body.\n",
    "    Temperature 3.0: spock and me zksum.\n",
    "    Temperature 10.0: spock and me .\"\"\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You can see that when the temperature is high, the text start to make less sense. If the model was trained with a larger sample and for more epochs, you would get better phrases. Also, when temperature is low, the phrases seems more realistic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so if a photon is directed through a plane with tw'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(path_data+'sheldon.txt', 'r') as file:\n",
    "    sheldon = file.read()\n",
    "    file.close()\n",
    "\n",
    "sheldon[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sentence next_char\n",
      "0  so if a photon is di         r\n",
      "1  if a photon is direc         t\n",
      "2  a photon is directed          \n",
      "3  hoton is directed th         r\n",
      "4  on is directed throu         g\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou can see that the texts were shifted by 3 characters on every step to create examples of sentences and next characters. These vectors will later be processed to contain only index numbers instead of strings, and then can be used to train a supervised RNN model that learns to replicate sentences in the universe the texts belongs to. Here, characters were used as tokens, but if you have enough data you can also create a model to predict the next word.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Create vectors of sentences and next characters\n",
    "\n",
    "This exercise aims to emphasize more the value of data preparation. You will use texts containing phrases of the character Sheldon from The Big Bang Theory TV show as input and will create vectors of sentence indexes and next characters that are needed before creating a text generation model.\n",
    "\n",
    "The text is available in the sheldon variable, as well as the vocabulary (characters) on the vocabulary variable and the hyperparameters chars_window and step defined with values 20 and 3. This means that a sequence of 20 characters will be used to predict the next one, and the window will shift 3 characters on every iteration.\n",
    "\n",
    "Also, the package pandas as pd is loaded in the environment.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split the text by line break to loop through sentences.\n",
    "    Loop until the end of the sentence minus chars_window.\n",
    "    Append the portion of the sentence that has chars_window characters to the sentences variable and append the next character to the next_chars variable.\n",
    "    Use the obtained vectors to create a pd.DataFrame() and print its first rows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "chars_window = 20\n",
    "step = 3\n",
    "\n",
    "# Instantiate the vectors\n",
    "sentences = []\n",
    "next_chars = []\n",
    "# Loop for every sentence\n",
    "for sentence in sheldon.split('\\n'):\n",
    "    # Get 20 previous chars and next char; then shift by step\n",
    "    for i in range(0, len(sentence) - chars_window, step):\n",
    "        sentences.append(sentence[i:i + chars_window])\n",
    "        next_chars.append(sentence[i + chars_window])\n",
    "\n",
    "# Define a Data Frame with the vectors\n",
    "df = pd.DataFrame({'sentence': sentences, 'next_char': next_chars})\n",
    "\n",
    "# Print the initial rows\n",
    "print(df.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You can see that the texts were shifted by 3 characters on every step to create examples of sentences and next characters. These vectors will later be processed to contain only index numbers instead of strings, and then can be used to train a supervised RNN model that learns to replicate sentences in the universe the texts belongs to. Here, characters were used as tokens, but if you have enough data you can also create a model to predict the next word.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {' ': 0, '!': 1, '$': 2, '%': 3, '&': 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ';': 21, '<': 22, '=': 23, '>': 24, '?': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, '}': 52, '¿': 53, 'à': 54, 'á': 55, 'è': 56, 'é': 57, 'ê': 58, 'ñ': 59, 'ó': 60, 'ö': 61, 'ü': 62, '–': 63, '‘': 64, '’': 65, '“': 66, '”': 67, '…': 68}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enumerated_dictionary(text):\n",
    "    # Create a set of all the unique characters in the text\n",
    "    unique_characters = set(text)\n",
    "\n",
    "    # Create an enumerated dictionary for each unique character\n",
    "    enumerated_dictionary = {}\n",
    "    for i, character in enumerate(unique_characters):\n",
    "        enumerated_dictionary[character] = i\n",
    "\n",
    "    return enumerated_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9': 0,\n",
       " 'ê': 1,\n",
       " 'à': 2,\n",
       " 'f': 3,\n",
       " '–': 4,\n",
       " ',': 5,\n",
       " 'u': 6,\n",
       " '5': 7,\n",
       " ';': 8,\n",
       " '¿': 9,\n",
       " ')': 10,\n",
       " 'd': 11,\n",
       " '.': 12,\n",
       " '%': 13,\n",
       " 'ñ': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '’': 17,\n",
       " 'e': 18,\n",
       " '?': 19,\n",
       " 'ü': 20,\n",
       " 'g': 21,\n",
       " 'n': 22,\n",
       " 'x': 23,\n",
       " 'w': 24,\n",
       " '<': 25,\n",
       " '\\n': 26,\n",
       " '(': 27,\n",
       " 'i': 28,\n",
       " '…': 29,\n",
       " 'ö': 30,\n",
       " 'q': 31,\n",
       " 'l': 32,\n",
       " '0': 33,\n",
       " '$': 34,\n",
       " ' ': 35,\n",
       " '>': 36,\n",
       " '‘': 37,\n",
       " 'á': 38,\n",
       " 'z': 39,\n",
       " '“': 40,\n",
       " 'é': 41,\n",
       " 'r': 42,\n",
       " '&': 43,\n",
       " '2': 44,\n",
       " 'b': 45,\n",
       " 'v': 46,\n",
       " 's': 47,\n",
       " '/': 48,\n",
       " '\\\\': 49,\n",
       " 'm': 50,\n",
       " 'c': 51,\n",
       " 't': 52,\n",
       " 'k': 53,\n",
       " '4': 54,\n",
       " '3': 55,\n",
       " '-': 56,\n",
       " 'a': 57,\n",
       " '6': 58,\n",
       " '}': 59,\n",
       " 'o': 60,\n",
       " '=': 61,\n",
       " 'è': 62,\n",
       " '1': 63,\n",
       " 'j': 64,\n",
       " '!': 65,\n",
       " 'h': 66,\n",
       " 'y': 67,\n",
       " 'p': 68,\n",
       " '”': 69,\n",
       " 'ó': 70}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index = create_enumerated_dictionary(sheldon)\n",
    "char_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThat's it! With the variables numerical_sentences and numerical_next_chars you are ready to train text generation models.\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Preparing the data for training\n",
    "\n",
    "In this exercise, you will continue to prepare the data to train the model. After creating the arrays of sentences and next characters, you need to transform them to numerical values that can be used on the model.\n",
    "\n",
    "This step is necessary because the RNN models expect numbers only and not strings. You will create numerical arrays that have zeros or ones in the positions representing the characters present on the sentences. Ones (or True) represent the corresponding character is present, while zeros (or False) represent the absence of the character in that position of the sentence.\n",
    "\n",
    "The variables sentences, next_char, n_vocab, chars_window, num_seqs (number of sentences in the training data) are already loaded in the environment, as well as numpy as np.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Instantiate a np.array() with zeros and shape (number of sentences, characters window, vocabulary size).\n",
    "    Use the dictionary char_to_index to set the position of the current char to 1.\n",
    "    Set the current next character to 1.\n",
    "    Print the first position of each array.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_seqs, chars_window, n_vocab = len(sentences), 20, len(char_to_index)\n",
    "\n",
    "sentences = [sentence.replace('\\\\', '') for sentence in sentences]\n",
    "\n",
    "# Instantiate the variables with zeros\n",
    "numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=bool)\n",
    "numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=bool)\n",
    "\n",
    "# Loop for every sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "  # Loop for every character in sentence\n",
    "  for t, char in enumerate(sentence):\n",
    "    # Set position of the character to 1\n",
    "    numerical_sentences[i, t, char_to_index[char]] = 1\n",
    "\n",
    "    # Set next character to 1\n",
    "    numerical_next_chars[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Print the first position of each\n",
    "print(numerical_sentences[0], numerical_next_chars[0], sep=\"\\n\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That's it! With the variables numerical_sentences and numerical_next_chars you are ready to train text generation models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (LSTM)          (None, 20, 64)            34816     \n",
      "                                                                 \n",
      " LSTM_hidden (LSTM)          (None, 64)                33024     \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 71)                4615      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72455 (283.03 KB)\n",
      "Trainable params: 72455 (283.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe main point of the text generation model is that it uses the whole vocabulary as classes on the output layer. This means that if the vocabulary is big (hundred of thousands), then the required amount of data is also very big.\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Creating the text generation model\n",
    "\n",
    "In this exercise, you will define a text generation model using Keras.\n",
    "\n",
    "The variables n_vocab containing the vocabulary size and input_shape containing the shape of the data used for training are already loaded in the environment. Also, the weights of a pre-trained model is available on file model_weights.h5. The model was trained with 40 epochs on the training data. Recap that to train a model in Keras, you just use the method .fit() on the training data (X, y), and the parameter epochs. For example:\n",
    "\n",
    "model.fit(X_train, y_train, epochs=40)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add one LSTM layer returning the sequences.\n",
    "    Add one LSTM layer not returning the sequences.\n",
    "    Add the output layer with n_vocab units.\n",
    "    Display the model summary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "input_shape = (chars_window, n_vocab)\n",
    "\n",
    "# Instantiate the model\n",
    "model = Sequential(name=\"LSTM model\")\n",
    "\n",
    "# Add two LSTM layers\n",
    "model.add(LSTM(64, input_shape=input_shape, dropout=0.15, recurrent_dropout=0.15, return_sequences=True, name=\"Input_layer\"))\n",
    "model.add(LSTM(64, dropout=0.15, recurrent_dropout=0.15, return_sequences=False, name=\"LSTM_hidden\"))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(n_vocab, activation='softmax', name=\"Output_layer\"))\n",
    "\n",
    "# Compile and load weights\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#model.load_weights('model_weights.h5')\n",
    "# Summary\n",
    "model.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "The main point of the text generation model is that it uses the whole vocabulary as classes on the output layer. This means that if the vocabulary is big (hundred of thousands), then the required amount of data is also very big.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Preparing the input text\n",
    "\n",
    "You have seen in the video how to prepare the input and output texts. This exercise aims to show a common practice that is to use the maximum length of the sentences to pad all of them, this way no information will be lost.\n",
    "\n",
    "Since the RNN models need the inputs to have the same size, this is a way to pad all sentences and just add zeros to the smaller sentences, without cutting the larger ones.\n",
    "\n",
    "Also, you will use words instead of characters to represent the tokens, this is a common approach for NMT models.\n",
    "\n",
    "The Portuguese texts are loaded on the pt_sentences variable and a fitted tokenizer on the input_tokenizer variable.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use the .split() method on each sentence to split by white space and obtain the number of words in the sentence.\n",
    "    Use the .texts_to_sequences() method to transform text into sequence of indexes.\n",
    "    Use the obtained maximum length of sentences to pad them.\n",
    "    Print the first transformed sentence.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Get maximum length of the sentences\n",
    "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
    "\n",
    "# Transform text to sequence of numerical indexes\n",
    "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(X, maxlen=pt_length, padding='post')\n",
    "\n",
    "# Print first sentence\n",
    "print(pt_sentences[0])\n",
    "\n",
    "# Print transformed sentence\n",
    "print(X[0])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Perfect! You can see that the input language now has sequences of indexes, and when the sentence is smaller than the maximum length, there are zeros in the right positions of the sentence. No sentence lost any information due to padding.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Preparing the output text\n",
    "\n",
    "In this exercise, you will prepare the output texts to be used on the translation model. Apart from transforming the text to sequences of indexes, you also need to one-hot encode each index.\n",
    "\n",
    "The English texts are loaded on the en_sentences variable, the fitted tokenizer on the output_tokenizer variable and the English vocabulary size on en_vocab_size.\n",
    "\n",
    "Also, a function to perform the first steps of transforming the output language (transformation of texts into sequence of indexes) is already created. The function is loaded on the environment as transform_text_to_sequences() and has two parameters: sentences that expect a list of sentences in English and tokenizer that expects a fitted Tokenizer object from the keras.preprocessing.text module.\n",
    "\n",
    "numpy is loaded as np.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Pass the en_sentences and output_tokenizer variables to the transform_text_to_sequences() function to initialize the Y variable.\n",
    "    Use the to_categorical() function to one-hot encode the sentences. Use the en_vocab_size variable as number of classes.\n",
    "    Transform the temporary list to numpy array and reshape to have shape equal to (num_sentences, sentences_len, en_vocab_size).\n",
    "    Print the raw text and the transformed one.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Initialize the variable\n",
    "Y = transform_text_to_sequences(en_sentences, output_tokenizer)\n",
    "\n",
    "# Temporary list\n",
    "ylist = list()\n",
    "for sequence in Y:\n",
    "  \t# One-hot encode sentence and append to list\n",
    "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
    "\n",
    "# Update the variable\n",
    "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
    "\n",
    "# Print the raw sentence and its transformed version\n",
    "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Impressive. Now you are able to transform input and otput languages to the correct format to be used on a Neural Machine Translation model. Also, don't bother that all the printed numbers from the tranformed text seems to be all zeros, they are not! Since the vocabulary size is 1269, the 1's didn't appear on the output but they are somewhere in the middle of all those zeros. Now, are you ready to create a NMT model?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Translate Portuguese to English\n",
    "\n",
    "This is the last exercise of the course, congratulations on getting here!\n",
    "\n",
    "You will learn how to use NMT models for making translations.\n",
    "\n",
    "A model that encodes Portuguese small phrases and decodes them into English small phrases was pre-trained and is loaded in the model variable.\n",
    "\n",
    "Also, the function predict_one() is already loaded, use help() for details and the dataset is available on the test (raw text) and X_test (tokenized) variables.\n",
    "\n",
    "You will define a function to translate a list of sentences. In the parameters, sentences is a list of phrases to be translated, index_to_word is a dict containing numerical indexes as keys and words as values for the English language, loaded in the en_index_to_word variable.\n",
    "\n",
    "The model summary has been printed for your consideration.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Loop over the enumerated iterator of the phrases.\n",
    "    Use the pre-loaded function predict_one() to translate one phrase.\n",
    "    Print the translation result.\n",
    "    Call the defined function to translate the initial 10 phrases of the X_test variable.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Function to predict many phrases\n",
    "def predict_many(model, sentences, index_to_word, raw_dataset):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Translate the Portuguese sentence\n",
    "        translation = predict_one(model, sentence, index_to_word)\n",
    "        \n",
    "        # Get the raw Portuguese and English sentences\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        \n",
    "        # Print the correct Portuguese and English sentences and the predicted\n",
    "        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\n",
    "predict_many(model, X_test[:10], en_index_to_word, test)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! This translation model was trained in fairly small sentences (up to 8 words). The longer the sentence to translate gets, more complex the model need to be.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
