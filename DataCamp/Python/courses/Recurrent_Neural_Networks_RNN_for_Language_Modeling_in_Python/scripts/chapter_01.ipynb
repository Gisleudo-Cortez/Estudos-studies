{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYes, the ANN model has ~2 million parameters, and the RNN model needs to train ~66 thousand.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Comparing the number of parameter of RNN and ANN\n",
    "\n",
    "In this exercise, you will compare the number of parameters of an artificial neural network (ANN) with the recurrent neural network (RNN) architectures. Here, the vocabulary size is equal to 10,000 for both models.\n",
    "\n",
    "The models have been defined for you with similar architectures of only one layer with 256 units (Dense or RNN) plus the output layer. They are stored on variables ann_model and rnn_model.\n",
    "\n",
    "Use the method .summary() to print the models' architecture and number of parameters and select the correct statement.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "The RNN model has fewer parameters than the ANN model.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Yes, the ANN model has ~2 million parameters, and the RNN model needs to train ~66 thousand.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  the it of yet br stress and must in at town wh...  positive  negative\n",
      "    1  the what have just be ever have 2 at is over d...  negative  positive\n",
      "    2  the was me of and in character and performance...  negative  positive\n",
      "    3  the as on mean unlike and movie pictures is pa...  negative  negative\n",
      "    4  the genuine was capture now and and and new to...  negative  negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou can see that some of the predictions were correct and some were not. The model used was very simple and its accuracy was not very high. You will learn later some tuning approaches to the sentiment classification model. Also, the process of pre-processing the text data and creating, training and testing models in Keras will be detailed later in the course. Finally, you created a decision rule to determine if the sentiment would be classified as positive or negative. In many applications, the value of 0.5 is used as decision boundary, but other values can also be used depending on what metric you want to optimize.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Sentiment analysis\n",
    "\n",
    "In the video exercise, you were exposed to the various applications of sequence to sequence models. In this exercise you will see how to use a pre-trained model for sentiment analysis.\n",
    "\n",
    "The model is pre-loaded in the environment on variable model. Also, the tokenized test set variables X_test and y_test and the pre-processed original text data sentences from IMDb are also available.You will learn how to pre-process the text data and how to create and train the model using Keras later in the course.\n",
    "\n",
    "You will use the pre-trained model to obtain predictions of sentiment. The model returns a number between zero and one representing the probability of the sentence to have a positive sentiment. So, you will create a decision rule to set the prediction to positive or negative.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use the .predict() method to make predictions on the test data.\n",
    "    Make the prediction equal to \"positive\" if its value is greater than 0.5 and \"negative\" otherwise and store the result in the pred_sentiment variable.\n",
    "    Create a pd.DataFrame containing the pre-processed text, the prediction obtained in the previous step and their true values contained in the y_test variable.\n",
    "    Print the first rows using the .head() method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\"\"\"# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the predicion for all the sentences\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x>0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame({'sentence': sentences, 'y_pred': pred_sentiment, 'y_true': y_test})\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())\n",
    "\"\"\"\n",
    "\n",
    "print('''    0  the it of yet br stress and must in at town wh...  positive  negative\n",
    "    1  the what have just be ever have 2 at is over d...  negative  positive\n",
    "    2  the was me of and in character and performance...  negative  positive\n",
    "    3  the as on mean unlike and movie pictures is pa...  negative  negative\n",
    "    4  the genuine was capture now and and and new to...  negative  negative''')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You can see that some of the predictions were correct and some were not. The model used was very simple and its accuracy was not very high. You will learn later some tuning approaches to the sentiment classification model. Also, the process of pre-processing the text data and creating, training and testing models in Keras will be detailed later in the course. Finally, you created a decision rule to determine if the sentiment would be classified as positive or negative. In many applications, the value of 0.5 is used as decision boundary, but other values can also be used depending on what metric you want to optimize.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCorrect! A language model computes the probability of the whole sentence, and the correct homophone would have higher probability than the wrong one.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Sequence to sequence models\n",
    "\n",
    "In the video exercise, you learned about four types of sequence to sequence models: many-to-one (classification) and many-to-many (text generation, neural machine translation and language models). In this exercise, you have to choose the correct type of model given the following problem description:\n",
    "\n",
    "You are helping your friend who is a specialist in speech recognition. Your friend built a model that can recognize different accents of English, but the model is failing to distinguish homophones - words with the same pronunciation but have different meaning such as \"sea\" vs \"see\" or \"write\" vs \"right\".\n",
    "\n",
    "You propose to use a model that will use the context around the words to identify the semantic meaning of the words. By learning the meaning of the words, the new model would avoid outputs like \"Did you sea that car?\" - it would identify that in this case, the correct word would be \"see\".\n",
    "\n",
    "What type of sequence-to-sequence model is appropriate?\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "    Many-to-many, because it is a classification model.\n",
    "\n",
    "    Many-to-one, because it is a classification model.\n",
    "\n",
    "    Many-to-many, this problem can be solved with a language model. (Answer)\n",
    "\n",
    "    Many-to-one, because it is a prediction problem.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Correct! A language model computes the probability of the whole sentence, and the correct homophone would have higher probability than the wrong one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye-bye\n"
     ]
    }
   ],
   "source": [
    "with open(path_data+'bigbang.txt', 'r', encoding='utf-8') as file:\n",
    "    sheldon_quotes = file.readlines()\n",
    "    file.close()\n",
    "sheldon_quotes = [line[len('Sheldon: '):].strip() for line in sheldon_quotes if line.startswith('Sheldon: ')]\n",
    "print(sheldon_quotes[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Are', 1: 'Bye-bye', 2: 'DNA', 3: 'Evidently.', 4: 'Fuddruckers.', 5: 'I', 6: 'IQ', 7: 'If', 8: 'Leonard,', 9: 'Let’s', 10: 'No,', 11: 'No.', 12: 'So', 13: 'There’s', 14: 'We', 15: 'What’s', 16: 'You', 17: 'a', 18: 'about', 19: 'after', 20: 'an', 21: 'and', 22: 'are', 23: 'area', 24: 'as', 25: 'at', 26: 'bank?', 27: 'basic', 28: 'before', 29: 'both', 30: 'broke', 31: 'but', 32: 'by', 33: 'can', 34: 'clavicle.', 35: 'committing', 36: 'curve.', 37: 'did', 38: 'differential', 39: 'directed', 40: 'do', 41: 'doesn’t', 42: 'don’t', 43: 'downloads,', 44: 'either', 45: 'experiments', 46: 'faster', 47: 'father', 48: 'for', 49: 'fraud.', 50: 'generate', 51: 'genetic', 52: 'go', 53: 'going', 54: 'gone', 55: 'good', 56: 'guarantee', 57: 'have', 58: 'he', 59: 'hear', 60: 'height', 61: 'her', 62: 'high', 63: 'his', 64: 'hits', 65: 'hopes', 66: 'hostesses', 67: 'however,', 68: 'idea', 69: 'if', 70: 'in', 71: 'integral', 72: 'interesting', 73: 'is', 74: 'it', 75: 'its', 76: 'it’s', 77: 'just', 78: 'know', 79: 'know,', 80: 'lasers.', 81: 'leave.', 82: 'leaving?', 83: 'left', 84: 'little', 85: 'mad', 86: 'millimetres,', 87: 'mix', 88: 'most', 89: 'my', 90: 'no', 91: 'not', 92: 'observed', 93: 'of', 94: 'off', 95: 'offspring,', 96: 'on', 97: 'or', 98: 'our', 99: 'out.', 100: 'people', 101: 'photon', 102: 'pin', 103: 'place.', 104: 'plane', 105: 'point,', 106: 'poor', 107: 'protocol', 108: 'result', 109: 'same', 110: 'series', 111: 'she', 112: 'should', 113: 'single', 114: 'sister', 115: 'slit', 116: 'slits', 117: 'slits.', 118: 'solve', 119: 'some', 120: 'sperm', 121: 'sperm,', 122: 'stairs?', 123: 'step', 124: 'still', 125: 'target,', 126: 'tee-shirt.', 127: 'that', 128: 'that.', 129: 'the', 130: 'there’s', 131: 'thing', 132: 'think', 133: 'this', 134: 'this.', 135: 'through', 136: 'to', 137: 'toddler', 138: 'trip.', 139: 'true,', 140: 'try', 141: 'twelve,', 142: 'two', 143: 'under', 144: 'unobserved', 145: 'up', 146: 'use', 147: 'walking', 148: 'want', 149: 'was', 150: 'what', 151: 'when', 152: 'who', 153: 'will', 154: 'will,', 155: 'winds', 156: 'with', 157: 'woman', 158: 'work', 159: 'wouldn’t.', 160: 'yearn', 161: 'you'}\n",
      "{'Are': 0, 'Bye-bye': 1, 'DNA': 2, 'Evidently.': 3, 'Fuddruckers.': 4, 'I': 5, 'IQ': 6, 'If': 7, 'Leonard,': 8, 'Let’s': 9, 'No,': 10, 'No.': 11, 'So': 12, 'There’s': 13, 'We': 14, 'What’s': 15, 'You': 16, 'a': 17, 'about': 18, 'after': 19, 'an': 20, 'and': 21, 'are': 22, 'area': 23, 'as': 24, 'at': 25, 'bank?': 26, 'basic': 27, 'before': 28, 'both': 29, 'broke': 30, 'but': 31, 'by': 32, 'can': 33, 'clavicle.': 34, 'committing': 35, 'curve.': 36, 'did': 37, 'differential': 38, 'directed': 39, 'do': 40, 'doesn’t': 41, 'don’t': 42, 'downloads,': 43, 'either': 44, 'experiments': 45, 'faster': 46, 'father': 47, 'for': 48, 'fraud.': 49, 'generate': 50, 'genetic': 51, 'go': 52, 'going': 53, 'gone': 54, 'good': 55, 'guarantee': 56, 'have': 57, 'he': 58, 'hear': 59, 'height': 60, 'her': 61, 'high': 62, 'his': 63, 'hits': 64, 'hopes': 65, 'hostesses': 66, 'however,': 67, 'idea': 68, 'if': 69, 'in': 70, 'integral': 71, 'interesting': 72, 'is': 73, 'it': 74, 'its': 75, 'it’s': 76, 'just': 77, 'know': 78, 'know,': 79, 'lasers.': 80, 'leave.': 81, 'leaving?': 82, 'left': 83, 'little': 84, 'mad': 85, 'millimetres,': 86, 'mix': 87, 'most': 88, 'my': 89, 'no': 90, 'not': 91, 'observed': 92, 'of': 93, 'off': 94, 'offspring,': 95, 'on': 96, 'or': 97, 'our': 98, 'out.': 99, 'people': 100, 'photon': 101, 'pin': 102, 'place.': 103, 'plane': 104, 'point,': 105, 'poor': 106, 'protocol': 107, 'result': 108, 'same': 109, 'series': 110, 'she': 111, 'should': 112, 'single': 113, 'sister': 114, 'slit': 115, 'slits': 116, 'slits.': 117, 'solve': 118, 'some': 119, 'sperm': 120, 'sperm,': 121, 'stairs?': 122, 'step': 123, 'still': 124, 'target,': 125, 'tee-shirt.': 126, 'that': 127, 'that.': 128, 'the': 129, 'there’s': 130, 'thing': 131, 'think': 132, 'this': 133, 'this.': 134, 'through': 135, 'to': 136, 'toddler': 137, 'trip.': 138, 'true,': 139, 'try': 140, 'twelve,': 141, 'two': 142, 'under': 143, 'unobserved': 144, 'up': 145, 'use': 146, 'walking': 147, 'want': 148, 'was': 149, 'what': 150, 'when': 151, 'who': 152, 'will': 153, 'will,': 154, 'winds': 155, 'with': 156, 'woman': 157, 'work': 158, 'wouldn’t.': 159, 'yearn': 160, 'you': 161}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCool, This is the first step in building language models! You extracted the vocabulary unique_words of the raw texts and created dictionaries to go from words to numerical indexes and vice versa. Let's continue!\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Getting used to text data\n",
    "\n",
    "In this exercise, you will play with text data by analyzing quotes from Sheldon Cooper in The Big Bang Theory TV show. This will give you a chance to analyze sentences to obtain insights on what it's like to deal with real-world text data.\n",
    "\n",
    "You will use dictionary comprehensions to create dictionaries that map words to indexes and vice versa. The use of dictionaries instead of, for example, a pandas.DataFrame is because they are more intuitive and don't add unnecessary extra complexity.\n",
    "\n",
    "The data is available in sheldon_quotes with the first two sentences already printed for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    join the sentences into one variable and then extract all the words and store this list in all_words.\n",
    "    Remove the duplicated words by applying list(set()) on the list of words and store them in unique_words.\n",
    "    Create a dictionary with indexes as keys and words as values using dictionary comprehensions.\n",
    "    Create a dictionary with words as keys and indexes as values using dictionary comprehensions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes[:17]).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i,wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Cool, This is the first step in building language models! You extracted the vocabulary unique_words of the raw texts and created dictionaries to go from words to numerical indexes and vice versa. Let's continue!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(sentences, next_chars, n=10):\n",
    "    \"\"\"Function to print examples of (data,label)\n",
    "\n",
    "    This function loops over the sentences and prints the pair of data and label, \n",
    "    corresponding to the sentence and next char. \n",
    "    This way, the student can check how the data was transformed.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): the prepared data\n",
    "        next_chars (string): the label containing the next char of the sentences\n",
    "        n (int): the number of examples to print\n",
    "\n",
    "    Returns:\n",
    "        nothing\n",
    "\n",
    "    \"\"\"\n",
    "    result = 'Sentence\\tNext char\\n'\n",
    "    n_i = 1\n",
    "    for sent, char in zip(sentences, next_chars):\n",
    "        if n_i >= n:\n",
    "            break\n",
    "        result = result + sent + '\\t' + char + '\\n'\n",
    "        n_i += 1\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're afraid of insects and women, Ladybugs must render you catatonic.Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.For example, I cry because others are stupid, and that makes me sad.I'm not insane, my mother had me tested.Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.Amy's birthday present will be my genitals.(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!Thankfully all the things my girlfriend used to do can be taken care of with my right hand.I would have been here sooner but the bus kept stopping for other people to get on it.Oh gravity, thou art a heartless bitch.I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.Well, today we tried masturbating for money.I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.What computer do you have? And please don't say a white one.She calls me moon-pie because I'm nummy-nummy and she could just eat me up.Ah, memory impairment; the free prize at the bottom of every vodka bottle.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"You're afraid of insects and women, Ladybugs must render you catatonic.Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.For example, I cry because others are stupid, and that makes me sad.I'm not insane, my mother had me tested.Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.Amy's birthday present will be my genitals.(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!Thankfully all the things my girlfriend used to do can be taken care of with my right hand.I would have been here sooner but the bus kept stopping for other people to get on it.Oh gravity, thou art a heartless bitch.I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.Well, today we tried masturbating for money.I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.What computer do you have? And please don't say a white one.She calls me moon-pie because I'm nummy-nummy and she could just eat me up.Ah, memory impairment; the free prize at the bottom of every vodka bottle.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\tNext char\n",
      "You're afr\ta\n",
      "u're afrai\td\n",
      "re afraid \to\n",
      " afraid of\t \n",
      "fraid of i\tn\n",
      "aid of ins\te\n",
      "d of insec\tt\n",
      "of insects\t \n",
      " insects a\tn\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWith this you are ready to use the sentences and next character to train a supervised learning model! Don't mind that the printed sentences look strange, since you used characters instead of words and defined a sentence with a fixed length, the texts can be broken in the middle of a word. Note that the process of creating the sentences and next chars is the same when using words instead of characters, the only change being the values present on the lists (words instead of characters). Now, before going straight to training machine learning models, let's see what to do when you have a new text data not pre-processed yet.\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Preparing text data for model input\n",
    "\n",
    "Previously, you learned how to create dictionaries of indexes to words and vice versa. In this exercise, you will split the text by characters and continue to prepare the data for supervised learning.\n",
    "\n",
    "Splitting the texts into characters may seem strange, but it is often done for text generation. Also, the process to prepare the data is the same, the only change is how to split the texts.\n",
    "\n",
    "You will create the training data containing a list of fixed-length texts and their labels, which are the corresponding next characters.\n",
    "\n",
    "You will continue to use the dataset containing quotes from Sheldon (The Big Bang Theory), available in the sheldon_quotes variable.\n",
    "\n",
    "The print_examples() function print the pairs so you can see how the data was transformed. Use help() for details.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define step equal to 2 and chars_window equal to 10.\n",
    "    Append the next sentence to the variable sentences.\n",
    "    Append the correct position of the text sheldon to the variable next_chars.\n",
    "    Use the print_examples() function to print 10 sentences and next characters.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(text) - chars_window, step):\n",
    "    sentences.append(text[i:i + chars_window])\n",
    "    next_chars.append(text[i + chars_window])\n",
    "\n",
    "# Print 10 pairs\n",
    "print_examples(sentences, next_chars, 10)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "With this you are ready to use the sentences and next character to train a supervised learning model! Don't mind that the printed sentences look strange, since you used characters instead of words and defined a sentence with a fixed length, the texts can be broken in the middle of a word. Note that the process of creating the sentences and next chars is the same when using words instead of characters, the only change being the values present on the lists (words instead of characters). Now, before going straight to training machine learning models, let's see what to do when you have a new text data not pre-processed yet.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text=['A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away',\n",
    " 'To the brave crew and passengers of the Kobayshi Maru sucks to be you',\n",
    " 'Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies',\n",
    " 'They are merely scars not mortal wounds and you must use them to propel you forward',\n",
    " 'You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 44, 0, 0, 24, 74, 0, 136, 0, 0, 74, 0, 21, 0, 74, 97, 58, 0, 63, 0, 96, 74, 21, 0, 136, 0, 0]\n",
      "Are Are either Are Are as it Are to Are Are it Are and Are it or he Are his Are on it and Are to Are Are\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nYou can see that some of the words were not found on the dictionary and have index = 0. By using the token '<UKN/>' in the training phase, you can easily use the model on unseen data without getting errors. This is also done when limiting the size of the vocabulary, say to 5,000 most frequent words, and setting the others as '<UKN/>'.\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Transforming new text\n",
    "\n",
    "In this exercise, you will transform a new text into sequences of numerical indexes on the dictionaries created before.\n",
    "\n",
    "This is useful when you already have a trained model and want to apply it on a new dataset. The preprocessing steps done on the training data should also be applied to the new text, so the model can make predictions/classifications.\n",
    "\n",
    "Here, you will also use a special token '<UKN/>' to represent words that are not in the vocabulary. Typically, these special tokens are the first indexes of the dictionaries, the position 0.\n",
    "\n",
    "The variables word_to_index, index_to_word and vocabulary are already loaded in the environment. Also, the variable with the new text is also loaded as new_text. The new text has been printed for you to have a look.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Loop through the list new_text containing the sentences.\n",
    "    Set to 0 the index in case the word is not found in the dictionary.\n",
    "    Append the sentence with indexes to the variable new_text_split.\n",
    "    Convert the indexes back to text using the dictionary index_to_word.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You can see that some of the words were not found on the dictionary and have index = 0. By using the token '<UKN/>' in the training phase, you can easily use the model on unseen data without getting errors. This is also done when limiting the size of the vocabulary, say to 5,000 most frequent words, and setting the others as '<UKN/>'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " LSTM (LSTM)                 (None, 128)               71168     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71297 (278.50 KB)\n",
      "Trainable params: 71297 (278.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"modelclass_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, None, 10)]        0         \n",
      "                                                                 \n",
      " LSTM (LSTM)                 (None, 128)               71168     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71297 (278.50 KB)\n",
      "Trainable params: 71297 (278.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! You can see that the keras.models.Sequential is very easy to use to add layers in sequence. On the other hand, the keras.models.Model class is very flexible and is usually the choice when scientists need deep customization in their solution. Also, you saw how one layer is connected to another layer in both cases, be by adding them in sequence using the method add, or by creating a layer and calling the desired (previous) layer like a function, in the Model class API, every layer is callable on a tensor and always return a tensor.\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Keras models\n",
    "\n",
    "In this exercise you'll practice using two classes from the keras.models module. You will create one model using the two classes Sequential and Model.\n",
    "\n",
    "The Sequential class is easier since the layers are assumed to be in order, while the Model class is more flexible and allows multiple inputs, multiple outputs and shared layers (shared weights).\n",
    "\n",
    "The Model class needs to explicitly declare the input layer, while in the Sequential class, this is done with the input_shape parameter.\n",
    "\n",
    "The objects and modules Sequential, Model, Dense, Input, LSTM and np (numpy) are already loaded on the environment.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Instantiate theSequential model with name sequential_model.\n",
    "    Add a LSTM and a Dense layers, and print the summary.\n",
    "---\n",
    "    Create an Input layer, add LSTM and Dense layers and store in main_output.\n",
    "    Instantiate the model and print its summary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Instantiate the class\n",
    "model = Sequential(name='sequential_model')\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! You can see that the keras.models.Sequential is very easy to use to add layers in sequence. On the other hand, the keras.models.Model class is very flexible and is usually the choice when scientists need deep customization in their solution. Also, you saw how one layer is connected to another layer in both cases, be by adding them in sequence using the method add, or by creating a layer and calling the desired (previous) layer like a function, in the Model class API, every layer is callable on a tensor and always return a tensor.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "texts = np.array(['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
    "       'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?'],\n",
    "      dtype='<U419')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sample texts: (54, 78)\n",
      "Now the texts have fixed length: 60. Let's see the first one: \n",
      "[ 0  0  0  0  0  0 24  4  1 25 13 26  5  1 14  3 27  6 28  2  7 29 30 13\n",
      " 15  2  8 16 17  5 18  6  4  9 31  2  8 32  4  9 15 33  9 34 35 14 36 37\n",
      "  2 38 39 40  2  8 16 41 42  5 18  6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThese functions are very useful to prepare raw texts to be inputted on RNN models. Next you are going to put everything together to classify sentiment on movie reviews!\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Keras preprocessing\n",
    "\n",
    "The second most important module of Keras is keras.preprocessing. You will see how to use the most important modules and functions to prepare raw data to the correct input shape. Keras provides functionalities that substitute the dictionary approach you learned before.\n",
    "\n",
    "You will use the module keras.preprocessing.text.Tokenizer to create a dictionary of words using the method .fit_on_texts() and change the texts into numerical ids representing the index of each word on the dictionary using the method .texts_to_sequences().\n",
    "\n",
    "Then, use the function .pad_sequences() from keras.preprocessing.sequence to make all the sequences have the same size (necessary for the model) by adding zeros on the small texts and cutting the big ones.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import Tokenizer and pad_sequences from relevant modules.\n",
    "    Fit the tokenizer object on the sample data stored in texts.\n",
    "    Transform the texts into sequences of numerical indexes using the method .texts_to_sequences().\n",
    "    Fix the size of the texts by padding them.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import relevant classes/functions\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "These functions are very useful to prepare raw texts to be inputted on RNN models. Next you are going to put everything together to classify sentiment on movie reviews!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Your first RNN model\n",
    "\n",
    "In this exercise you will put in practice the Keras modules to build your first RNN model and use it to classify sentiment on movie reviews.\n",
    "\n",
    "This first model has one recurrent layer with the vanilla RNN cell: SimpleRNN, and the output layer with two possible values: 0 representing negative sentiment and 1 representing positive sentiment.\n",
    "\n",
    "You will use the IMDB dataset contained in keras.datasets. A model was already trained and its weights stored in the file model_weights.h5. You will build the model's architecture and use the pre-loaded variables x_test and y_test to check the its performance.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add the SimpleRNN cell with 128 units.\n",
    "    Add a Dense layer with one unit for sentiment classification.\n",
    "    Use the proper loss function for binary classification.\n",
    "    Evaluate the model on the pre-trained validation set: (x_test, y_test).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Method '.evaluate()' shows the loss and accuracy\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice! You just created your first RNN model! But wait.... the accuracy is very low!! In the upcoming chapters you will learn how to tune the model to achieve more than 95% accuracy on the same problem.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
