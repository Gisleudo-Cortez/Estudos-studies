{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.genfromtxt(path_data+'X_train.csv', delimiter=',', skip_header=1)\n",
    "y_train = np.genfromtxt(path_data+'y_train.csv', delimiter=',', skip_header=1)\n",
    "X_test = np.genfromtxt(path_data+'X_test.csv', delimiter=',', skip_header=1)\n",
    "y_test = np.genfromtxt(path_data+'y_test.csv', delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: nan, Test: nan\n",
      "Train: 142.074, Test: 137.123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe Exploding gradient problem can happen when using RNN models. Luckily, this can be addressed with simple techniques such as gradient clipping. Notice how after applying this technique, the outputs are no longer NaN, meaning that the gradients didn't 'explode' during Step 2.\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Exploding gradient problem\n",
    "\n",
    "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
    "\n",
    "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
    "\n",
    "The data is already loaded on the environment as X_train, X_test, y_train and y_test.\n",
    "\n",
    "You will use a Stochastic Gradient Descent (SGD) optimizer and Mean Squared Error (MSE) as the loss function.\n",
    "\n",
    "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the clipvalue parameter to solve the problem.\n",
    "\n",
    "The Stochastic Gradient Descent in Keras is loaded as SGD.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use SGD() as optimizer and (X_test, y_test) as validation data.\n",
    "    Evaluate train performance and print all the MSE values.\n",
    "---\n",
    "\n",
    "    Set the SGD() parameter clipvalue equal to 3.0.\n",
    "    Compute the MSE values and store them on train_mse and test_mse variables.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01, momentum=0.9))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01, momentum=0.9, clipvalue=3.0))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "The Exploding gradient problem can happen when using RNN models. Luckily, this can be addressed with simple techniques such as gradient clipping. Notice how after applying this technique, the outputs are no longer NaN, meaning that the gradients didn't 'explode' during Step 2.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Vanishing gradient problem\n",
    "\n",
    "The other possible gradient problem is when the gradients vanish, or go to zero. This is a much harder problem to solve because it is not as easy to detect. If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?\n",
    "\n",
    "This problem occurs more often in RNN models when long memory is required, meaning having long sentences.\n",
    "\n",
    "In this exercise you will observe the problem on the IMDB data, with longer sentences selected. The data is loaded in X and y variables, as well as classes Sequential, SimpleRNN, Dense and matplotlib.pyplot as plt. The model was pre-trained with 100 epochs and its weights are stored on the file model_weights.h5.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add a SimpleRNN layer to the model.\n",
    "    Load the pre-trained weights on the model using the method .load_weights().\n",
    "    Add the accuracy of the training data available on the attribute 'acc' to the plot.\n",
    "    Display the plot using the method .show().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You can observe that at some point the accuracy stopped to improve, which can happen because of the vanishing gradient problem. This kind of problem is harder to detect than the exploding gradient problem and will demand deeper analysis by the data scientist. Researchers found a model architecture way to solve this problem, which you will study later in this course. Instead of using SimpleRNN cells, you can use the more complex ones such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) cells.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "GRU cells are better than simpleRNN\n",
    "\n",
    "In this exercise you will re-run the same model as the first chapter of the course to compare the accuracy of the model by simpling changing the SimpleRNN cell to a GRU cell.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with a SimpleRNN cell. In order to compare the models, a test set (x_test, y_test) is already loaded in the environment, as well as the old model SimpleRNN_model.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the GRU cell.\n",
    "    Print the models' summaries.\n",
    "    Print the accuracy of each model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the modules\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "# Print the old and new model summaries\n",
    "SimpleRNN_model.summary()\n",
    "gru_model.summary()\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n",
    "print(\"GRU model's accuracy:\\t{0}\".format(acc_GRU))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Cool! Just changing the layer you already improved the model!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Stacking RNN layers\n",
    "\n",
    "Deep RNN models can have tens to hundreds of layers in order to achieve state-of-the-art results.\n",
    "\n",
    "In this exercise, you will get a glimpse of how to create deep RNN models by stacking layers of LSTM cells one after the other.\n",
    "\n",
    "To do this, you will set the return_sequences argument to True on the firsts two LSTM layers and to False on the last LSTM layer.\n",
    "\n",
    "To create models with even more layers, you can keep adding them one after the other or create a function that uses the .add() method inside a loop to add many layers with few lines of code.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the LSTM layer.\n",
    "    Return the sequences in the first two layers and don't return the sequences in the last LSTM layer.\n",
    "    Load the pre-trained weights.\n",
    "    Print the loss and accuracy obtained.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the LSTM layer\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('lstm_stack_model_weights.h5')\n",
    "\n",
    "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! Stacking more layers also improve the accuracy of the model when comparing to the baseline 'simple_RNN' model! In the next lesson you will learn what else you can do to improve the model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Number of parameters comparison\n",
    "\n",
    "You saw that the one-hot representation is not a good representation of words because it is very sparse. Using the Embedding layer creates a dense representation of the vectors, but also demands a lot of parameters to be learned.\n",
    "\n",
    "In this exercise you will compare the number of parameters of two models using embeddings and one-hot encoding to see the difference.\n",
    "\n",
    "The model model_onehot is already loaded in the environment, as well as the Sequential, Dense and GRU from keras. Finally, the parameters vocabulary_size=80000 and sentence_len=200 are also loaded.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the Embedding layer from keras.layers.\n",
    "    On the embedding layer, use vocabulary size plus one as input dimension and sentence size as input length.\n",
    "    Compile the model.\n",
    "    Print the summary of the model with embedding.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the embedding layer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, input_length=sentence_len, trainable=True))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the one-hot model\n",
    "model_onehot.summary()\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You can see the immense difference in the number of parameters when using the embedding layer! Don't worry, in the next exercise you will learn how make transfer learning to avoid having to train this layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(filename):\n",
    "    with ZipFile(filename) as myzip:\n",
    "        with myzip.open('tl_glove_200k.pickle') as myfile:\n",
    "            glove = pickle.load(myfile)\n",
    "        \n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Transfer learning\n",
    "\n",
    "You saw that when training an embedding layer, you need to learn a lot of parameters.\n",
    "\n",
    "In this exercise, you will see that when using transfer learning it is possible to use the pre-trained weights and don't update them, meaning that all the parameters of the embedding layer will be fixed, and the model will only need to learn the parameters from the other layers.\n",
    "\n",
    "The function load_glove is already loaded on the environment and retrieves the glove matrix as a numpy.ndarray vector. It uses the function covered on the lesson's slides to retrieve the glove vectors with 200 embedding dimensions for the vocabulary present in this exercise.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use the pre-defined function to load the glove vectors.\n",
    "    Use the initializer Constant on the pre-trained vectors.\n",
    "    Add the output layer as a Dense with one unit.\n",
    "    Print the summary and check the trainable parameters.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Load the glove pre-trained vectors\n",
    "glove_matrix = load_glove('glove_200d.zip')\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
    "                    embeddings_initializer=Constant(glove_matrix), \n",
    "                    input_length=sentence_len, trainable=False))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "As you can see, the total parameters is very big, but the number of parameteres that will be trained is much smaller. The trained vectors already has values for the words, but is equal to a vector of zeros for new words not present in the pre-trained vectors. This can lead to problems if the task at hand is very specific.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Embeddings improves performance\n",
    "\n",
    "Does the embedding layer improves the accuracy of the model? Let's check it out in the same IMDB data.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with simpleRNN cell. In order to compare the models, a test set (X_test, y_test) is available in the environment, as well as the old model simpleRNN_model. The old model's accuracy is loaded in the variable acc_SimpleRNN.\n",
    "\n",
    "All required modules and functions as loaded in the environment: Sequential() from keras.models, Embedding and Dense from keras.layers and SimpleRNN from keras.layers.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add the embedding layer to the model.\n",
    "    Compute the model's accuracy and store on the variable acc_embeddings.\n",
    "    Print the accuracy of the old and new models.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create the model with embedding\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=max_vocabulary, output_dim=wordvec_dim, input_length=max_len))\n",
    "model.add(SimpleRNN(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('embedding_model_weights.h5')\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_embeddings = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(acc_simpleRNN, acc_embeddings))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "You can see that the embedding layer greatly improves the accuracy of the model!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Better sentiment classification\n",
    "\n",
    "In this exercise, you go back to the sentiment classification problem seen in Chapter 1.\n",
    "\n",
    "You are going to add more complexity to the model and improve its accuracy. You will use an Embedding layer to train word vectors on the training set and two LSTM layers to keep track of longer texts. Also, you will add an extra Dense layer before the output.\n",
    "\n",
    "This is no longer a simple model, and the training can take some time. For this reason, a pre-trained model is available by loading its weights with the method .load_weights() from the keras.models.Sequential class. The model was trained with 10 epochs and its weights are available on the file model_weights.h5.\n",
    "\n",
    "The following modules are loaded on the environment: Sequential, Embedding, LSTM, Dropout, Dense.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add an Embedding layer as the first layer of the model.\n",
    "    Add a second LSTM layer with 64 units returning the sequences.\n",
    "    Add an extra Dense layer with 16 units.\n",
    "    Evaluate the model to print the accuracy on the training set.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Print the obtained loss and accuracy\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Superb! You just increased the accuracy of your sentiment classification task from poorly 50% to more than 80%, fantastic!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Using the CNN layer\n",
    "\n",
    "In this exercise, you will use a pre-trained model that makes use of the Conv1D and MaxPooling1D layers from the keras.layers.convolutional module, and achieves even better accuracy on the classification task.\n",
    "\n",
    "This architecture achieved good results in language modeling tasks such as classification, and is added here as an extra exercise to see it in action and have some intuitions.\n",
    "\n",
    "Because this layer is not in the scope of the course, you will focus on how to use the layers together with the RNN layers you already learned.\n",
    "\n",
    "Please follow the instructions to see the results.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print the model's architecture.\n",
    "    Load the pre-trained weights.\n",
    "    Evaluate the model on the test data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Print the model summary\n",
    "model_cnn.summary()\n",
    "\n",
    "# Load pre-trained weights\n",
    "model_cnn.load_weights('model_weights.h5')\n",
    "\n",
    "# Evaluate the model to get the loss and accuracy values\n",
    "loss, acc = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(loss, acc))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations, you achieved very high accuracy on the sentiment classification task! Remark that on the training data the model achieved more than 98% accuracy, and because the accuracy was not in the same level on the test data, you can guess that it had some level of overfitting. It may be because the dataset was not big enough to train the model and some patterns present on the test data weren't present on the train set. Finally, the model can be further extended to have additional layers to achieve even better results, but will also demand more data and computer power.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
