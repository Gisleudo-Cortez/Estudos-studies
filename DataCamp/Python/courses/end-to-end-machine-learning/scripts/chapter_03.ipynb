{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasons for tests\n",
    "\n",
    "Testing is a pivotal aspect of the machine learning lifecycle. Just as developers rigorously test software applications to identify bugs and ensure they function as intended, data scientists and machine learning practitioners must thoroughly test machine learning models. Robust testing ensures that your models are accurate, reliable, and free from unintended biases. Which of the following statements about testing is not correct?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Testing is an important part of pre-deployment and ensures the model's predictions are working as expected.\n",
    "    \n",
    "    \n",
    "    With unittest.TestCase(), you can create a new test case by subclassing TestCase; test methods added should always start with the word \"test\".\n",
    "    \n",
    "    \n",
    "    Automated tests are run during the deployment phase and not at other points in the machine learning development lifecycle. {Answer}\n",
    "    \n",
    "    \n",
    "    The purpose of a test case in the unittest library might be to verify that the endpoint is correctly returning a prediction when valid input data is given.\n",
    "\n",
    "**Tests don't have to be confined to deployment. In general, it is helpful to write tests throughout the machine learning development lifecycle to ensure all components and phases are working as intended.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Writing unit tests\n",
    "\n",
    "In the previous video on inference testing, you learned about the importance of writing test cases for your trained and evaluated model using the Python unittest library. In this exercise, you will put your new skills to the test by writing a test case for the model to check that it is producing binary outputs as expected. Your trained model is imported, as well as the testing portion of the dataset X_test.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "     Define a test case class called TestModelInference that inherits from unittest.TestCase .\n",
    "---\n",
    "    Complete the setUp function by assigning X_test as a testcase class attribute.\n",
    "---\n",
    "    Define a test called test_prediction_output_values().\n",
    "---\n",
    "    Complete the test case by calling model.predict() on X_test; the test then checks that the output values are either 1 or 0.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "# Create a class called TestModelInference\n",
    "class TestModelInference(unittest.TestCase):\n",
    "\tdef setUp(self):\n",
    "\t\tself.model = model\n",
    "\n",
    "\t\t# set X_test as a class attribute\n",
    "\t\tself.X_test = X_test\n",
    "\n",
    "\t# define a test for prediction output values\n",
    "\tdef test_prediction_output_values(self):\n",
    "\t\tprint(\"Running test_prediction_output_values test case\")\n",
    "\n",
    "\t\t# Get model predictions\n",
    "\t\ty_pred = self.model.predict(self.X_test)\n",
    "\t\tunique_values = np.unique(y_pred)\n",
    "\t\tfor value in unique_values:\n",
    "\t\t\tself.assertIn(value, [0, 1])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Insightful inference testing! You have written a unittest testcase for checking that your model's output aligns with expectations; that is, it is a binary classification for the presence or absence of heart disease. Sanity checks such as this are important for all phases of deployment and can increase overall confidence in predictions for more stable development. Keep up the good work!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature stores vs model registries\n",
    "\n",
    "Feature stores and model registries are essential components of the machine learning lifecycle. In order to ensure organized, reproducible, and sharable results, you need to keep a comprehensive record of the various resources you have created over time, keeping careful track of how they evolve and change and how this impacts the overall development process. \n",
    "\n",
    "![Answer](/home/nero/Documents/Estudos/DataCamp/Python/courses/end-to-end-machine-learning/ch_03-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import Field, Entity\n",
    "from feast.types import Int32, Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFearless feature creation! Now that you have formally defined a number of features and their data types, the next step will be to load them into a feature store. This ensures ease of use and reproducibility of results.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Defining features for a feature store\n",
    "\n",
    "Before creating a feature store, you need to ensure that features are formally defined, in order to ensure the feature store knows the relationships, type, and structure of the features to be loaded. In this exercise, you will formally define a number of features in preparation for the creation of a feature store. Field is imported for you from feast.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "    Define the cp, thalach, ca, thal features using Feast's Field class.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define entity and selected features\n",
    "patient = Entity(name=\"patient\", join_keys=[\"patient_id\"])\n",
    "cp = Field(name='cp', dtype=Float32)\n",
    "thalach = Field(name='thalach', dtype=Int32)\n",
    "ca = Field(name='ca', dtype=Int32)\n",
    "thal = Field(name='thal', dtype=Int32)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fearless feature creation! Now that you have formally defined a number of features and their data types, the next step will be to load them into a feature store. This ensures ease of use and reproducibility of results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-24 13:26:30.410757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-24 13:26:30.410757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-24 13:26:30.410757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-24 13:26:30.410757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138</td>\n",
       "      <td>294.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-24 13:26:30.410757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient  age  sex   cp  ...  ca  thal  target                  timestamp\n",
       "0        0   52    1  0.0  ...   2     3       0 2024-04-24 13:26:30.410757\n",
       "1        1   53    1  0.0  ...   0     3       0 2024-04-24 13:26:30.410757\n",
       "2        2   70    1  0.0  ...   0     3       0 2024-04-24 13:26:30.410757\n",
       "3        3   61    1  0.0  ...   1     3       0 2024-04-24 13:26:30.410757\n",
       "4        4   62    0  0.0  ...   3     2       0 2024-04-24 13:26:30.410757\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from feast import FeatureStore, ValueType, FileSource, FeatureView\n",
    "\n",
    "heart_disease_df = pd.read_csv(path_data+'heart_disease_cleaned_3.csv', parse_dates=['timestamp']).drop(columns=['Unnamed: 0'])\n",
    "heart_disease_df.rename(columns={'index':'patient'}, inplace=True)\n",
    "heart_disease_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/feast/feature_view.py:417: UserWarning: There are some mismatches in your feature view's registered entities. Please check if you have applied your entities correctly.Entities: ['patient'] vs Entity Columns: []\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fantastic feature-storing! You have defined the relationship between a number of features in your heart disease dataset which have been pre-selected by your feature-selection algorithm. Now that the features are saved in a feature store, you have a consistent, reproducible registry to work with for future phases of the machine learning lifecycle.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Feature store using Feast\n",
    "\n",
    "In order to ensure effective development throughout the machine learning lifecycle, it is important to maintain detailed and comprehensive records of resources. Feature stores and model registries are examples of helpful resource records in the pre-modelling and modelling phases. In this exercise, you will implement a feature store using Feast. The predefined patient, Entity, as well as the cp, thalach, ca, and thal features have been loaded for you. ValueType, FeatureStore, and FileSource are all imported from feast. heart_disease_df is also imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "    Define a data source of your heart_disease_df.\n",
    "---\n",
    "    Create a Feature View object using the features defined - make sure to pass features in the right order!\n",
    "---\n",
    "    Create a Feature Store and apply the features you have defined.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "heart_disease_df.to_parquet(path_data+\"heart_disease.parquet\")\n",
    "\n",
    "# Point File Source to the saved file\n",
    "data_source = FileSource(\n",
    "    path=path_data+\"heart_disease.parquet\",\n",
    "    event_timestamp_column=\"timestamp\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "# Create a Feature View of the features\n",
    "heart_disease_fv = FeatureView(\n",
    "    name=\"heart_disease\",\n",
    "    entities=[patient],\n",
    "    schema=[cp, thalach, ca, thal],\n",
    "    source=data_source,\n",
    ")\n",
    "\n",
    "# Create a store of the data and apply the features\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "store.apply([patient, heart_disease_fv])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"Fantastic feature-storing! You have defined the relationship between a number of features in your heart disease dataset which have been pre-selected by your feature-selection algorithm. Now that the features are saved in a feature store, you have a consistent, reproducible registry to work with for future phases of the machine learning lifecycle.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containerization steps\n",
    "\n",
    "In this video, you have covered the topic of containerizing a machine learning model for deployment using Docker. You have discussed how Docker allows us to package your model and its dependencies into a standalone unit, which can be executed easily in different environments. You have also learned about Dockerfiles, the text documents containing commands that build an image, and the sequence of steps to follow in order to containerize a machine learning model.\n",
    "\n",
    "![Answer](/home/nero/Documents/Estudos/DataCamp/Python/courses/end-to-end-machine-learning/ch_03-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containerization using Docker\n",
    "\n",
    "Containers and containerization are some of the most widely used technologies and practices in the software and machine learning world. As the de-facto framework for container-based deployment, Docker is a must-learn for any ML practitioner. As such, let's test your working knowledge of Docker with a few questions.\n",
    "\n",
    "    Select the correct answers.\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.{Answer}\n",
    "\n",
    "\n",
    "After specifying the base image, Docker is instructed to copy necessary files like the Python script and a requirements.txt file into the Docker image.{Answer}\n",
    "\n",
    "\n",
    "The tagging of a Docker image is done before building the Docker image.\n",
    "\n",
    "\n",
    "The Dockerfile should specify the port to run on, define any environment variables, and the command that should run when a container is launched.{Answer}\n",
    "\n",
    "\n",
    "It is recommended to include sensitive data in your Docker images for quick and easy access, as Docker images are securely packaged to run independently on various platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using Docker\n",
    "\n",
    "You have seen how you can use a containerization service like Docker to bundle and run applications. This can be used for a variety of purposes, including ensuring a consistent environment for development and deployment, simplifying dependencies management, and providing a portable solution that can run almost anywhere. This is especially important in healthcare projects with non-technical stakeholders who want to be able to deploy and run models easily and at speed.\n",
    "\n",
    "![Answer](/home/nero/Documents/Estudos/DataCamp/Python/courses/end-to-end-machine-learning/ch_03-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD principles\n",
    "\n",
    "Which of the following statements best represent the principles of Continuous Integration and Continuous Deployment (CI/CD) as applied to deploying machine learning models on AWS Elastic Beanstalk?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "In CI/CD, Continuous Integration involves sending models to production, while Continuous Deployment incorporating newly trained machine learning models into a central repository.\n",
    "\n",
    "\n",
    "In CI/CD, Continuous Integration means incorporating code changes into a central repository, while Continuous Deployment means sending updates or changes in the codebase to production.{Answer}\n",
    "\n",
    "\n",
    "Continuous Integration and Continuous Deployment are often used together.{Answer}\n",
    "\n",
    "\n",
    "AWS Elastic Beanstalk does not support the principles of CI/CD for deploying machine learning models.\n",
    "\n",
    "\n",
    "Kubernetes is an AWS-specific tool for CI/CD management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a model using AWS EB\n",
    "\n",
    "Deploying your machine learning model is a crucial step in the end-to-end machine learning process. It enables you to serve the model to end users and other applications, providing valuable insights and predictions based on the healthcare data you have processed and the model you have trained. Deploying your model to a cloud service like Elastic Beanstalk ensures that it is accessible, scalable, and reliable, allowing you to focus on improving the model's performance and integrating it with other systems.\n",
    "\n",
    "In this exercise, you will put into practice the concepts learned in Chapter 3. Your objective is to deploy your containerized machine learning model using Amazon Web Services' (AWS) Elastic Beanstalk. You will be provided with commands that need to be arranged in the correct order to deploy and view your model.\n",
    "\n",
    "![Answer](/home/nero/Documents/Estudos/DataCamp/Python/courses/end-to-end-machine-learning/ch_03-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment: bringing it all together\n",
    "\n",
    "Congratulations on making it this far! You have reached the end of the deployment section in our course, and should have a well-rounded, high-level overview of the general steps involved in serving your model to external stakeholders. Remember, the deployment process is multifaceted and could look different for your particular use-case - this is just a high level overview based on our heart-disease study!\n",
    "\n",
    "![Answer](/home/nero/Documents/Estudos/DataCamp/Python/courses/end-to-end-machine-learning/ch_03-05.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
