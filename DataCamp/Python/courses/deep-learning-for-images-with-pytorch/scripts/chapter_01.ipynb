{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The number of classes\n",
    "\n",
    "To determine whether to use a binary or a multi-class model for a classification problem, you need to know the number of classes in the data. The Torch dataset is available in your workspace as train_dataset.\n",
    "\n",
    "Considering the number of classes in train_dataset, which image classification task would be appropriate?\n",
    "\n",
    "```python\n",
    "train_dataset.classes\n",
    "['cat', 'dog']\n",
    "```\n",
    "\n",
    "\n",
    "### Possible answers\n",
    "    \n",
    "    Binary (2 classes) {Answer}\n",
    "    \n",
    "    Multi-Class (3 classes)\n",
    "    \n",
    "    Multi-Class (5 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGreat work! Notice that the fully connected (fc) layer has an input size of 16x32x32. This is due to the original image input size being 64x64. After passing through the max pooling layer, the spatial dimensions are reduced in half, resulting in 32x32.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Binary classification model\n",
    "\n",
    "As a deep learning practitioner, one of your main tasks is training models for image classification. You often encounter binary classification, where you need to distinguish between two classes. To streamline your workflow and ensure reusability, you have decided to create a template for a binary image classification CNN model, which can be applied to future projects.\n",
    "\n",
    "The package torch and torch.nn as nn have been imported. All image sizes are 64x64 pixels.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Create a convolutional layer with 3 channels, 16 output channels, kernel size of 3, stride of 1, and padding of 1.\n",
    "\n",
    "    Create a fully connected layer with an input size of 16x32x32 and a number of classes equal to 1; include only the values in the provided order (16*32*32, 1).\n",
    "\n",
    "    Create a sigmoid activation function.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "class BinaryImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryImageClassifier, self).__init__()\n",
    "        \n",
    "        # Create a convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Create a fully connected layer\n",
    "        self.fc = nn.Linear(16*32*32, 1)\n",
    "        \n",
    "        # Create an activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Notice that the fully connected (fc) layer has an input size of 16x32x32. This is due to the original image input size being 64x64. After passing through the max pooling layer, the spatial dimensions are reduced in half, resulting in 32x32.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! You now have the foundations for both binary image and multi-class image classfication tasks.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Multi-class classification model\n",
    "\n",
    "With a template for a binary classification model in place, you can now build on it to design a multi-class classification model. The model should handle different numbers of classes via a parameter, allowing you to tailor the model to a specific multi-class classification task in the future.\n",
    "\n",
    "The packages torch and torch.nn as nn have been imported. All image sizes are 64x64 pixels.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Define the __init__ method including self and num_classes as parameters.\n",
    "\n",
    "    Create a fully connected layer with the input size of 16*32*32 and the number of classes num_classes as output.\n",
    "\n",
    "    Create an activation function softmax with dim=1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "class MultiClassImageClassifier(nn.Module):\n",
    "  \n",
    "    # Define the init method\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiClassImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Create a fully connected layer\n",
    "        self.fc = nn.Linear(16*32*32, num_classes)\n",
    "        \n",
    "        # Create an activation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! You now have the foundations for both binary image and multi-class image classfication tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB, grayscale, or alpha?\n",
    "\n",
    "Your team obtained a new image dataset for training. Before designing a model, you want to check if the images are RGB, grayscale, or with a transparency alpha channel. Assuming all the images in the dataset are the same, you only need to check one sample image.\n",
    "\n",
    "The torchvision.transforms.functional module has been imported as F. The sample image to check has been loaded as image.\n",
    "\n",
    "What kind of images does the dataset consist of?\n",
    "\n",
    "```pyhton\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "image = Image.open('image.png')\n",
    "F.get_image_num_channels(image)\n",
    "3\n",
    "```\n",
    "\n",
    "### Possible answers\n",
    "    \n",
    "    Grayscale\n",
    "    \n",
    "    RGB {Answer}\n",
    "    \n",
    "    Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Create a convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:  CNNModel(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Extended model:  CNNModel(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! Knowing the model architecture is a great way to learn about the model complexity.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Adding a new convolutional layer\n",
    "\n",
    "Your project lead provided you with a new CNN model. Let's take a look at the model's architecture and append a new convolutional layer to it.\n",
    "\n",
    "The model is available as CNNModel. The packages torch and torch.nn as nn have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Instantiate a model from the CNNModel class and access the convolutional layers.\n",
    "\n",
    "    Create a new convolutional layer with in_channels equal to existing layer's out_channels, out_channels set to 32, and stride and padding both set to 1, and assign it to conv2.\n",
    "\n",
    "    Append the new layer to the model, calling it \"conv2\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a model\n",
    "model = CNNModel()\n",
    "print(\"Original model: \", model)\n",
    "\n",
    "# Create a new convolutional layer\n",
    "conv2 = nn.Conv2d(16,32, stride=1, padding=1, kernel_size=3)\n",
    "\n",
    "# Append the new layer to the model\n",
    "model.add_module('conv2', conv2)\n",
    "print(\"Extended model: \", model)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Knowing the model architecture is a great way to learn about the model complexity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations! With this knowledge, you can now efficiently stack multiple layers to build complex CNN architectures for various image processing tasks.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Creating a sequential block\n",
    "\n",
    "You decided to redesign your binary CNN model template by creating a block of convolutional layers. This will help you stack multiple layers sequentially. With this improved model, you will be able to easily design various CNN architectures.\n",
    "\n",
    "torch and torch.nn as nn have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    In the __init__() method, define a block of convolutional layers and assign it to self.conv_block.\n",
    "\n",
    "    In the forward() pass, pass the inputs through the convolutional block you defined.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "class BinaryImageClassification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BinaryImageClassification, self).__init__()\n",
    "    # Create a convolutional block\n",
    "    self.conv_block = nn.Sequential(\n",
    "      nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # Pass inputs through the convolutional block\n",
    "    x = self.conv_block(x)\n",
    "    return x\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! With this knowledge, you can now efficiently stack multiple layers to build complex CNN architectures for various image processing tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Save and load a model\n",
    "\n",
    "A manufacturing company wants to classify their projects based on images and determine the appropriate shipping packaging. Having trained a highly accurate model in PyTorch, you now plan to save the model and its pre-trained weights for future use and to share it with your team, making sure they can seamlessly load it.\n",
    "\n",
    "torch and torch.nn as nn have been imported. The pre-trained model object is available in your workspace as model, and its architecture as ManufacturingCNN.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Save the pre-trained model as ModelCNN.pth remembering to save the weights, not only the architecture.\n",
    "\n",
    "    Create a model instance called loaded_model from the class ManufacturingCNN().\n",
    "\n",
    "    Load ModelCNN.pth weights to loaded_model by passing the weights to .load_state_dict().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'ModelCNN.pth')\n",
    "\n",
    "# Create a new model\n",
    "loaded_model = ManufacturingCNN()\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model.load_state_dict(torch.load('ModelCNN.pth'))\n",
    "print(loaded_model)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Saving and later re-using models make your workflow more efficient.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Loading a pre-trained model\n",
    "\n",
    "You are building an application to label images from the social media. This task requires high accuracy and speed. You are going to use a pre-trained ResNet18 model to infer image classes.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import resnet18 and ResNet18_Weights from torchvision.models.\n",
    "\n",
    "    Instantiate the model using resnet18(), setting the weights parameter to weights.\n",
    "\n",
    "    Set model to the evaluation mode.\n",
    "\n",
    "    Initialize the input transforms and assign them to transform.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import resnet18 model\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Initialize model with default weights\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the transforms\n",
    "transform = weights.transforms()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Using DEFAULT weights is very convenient as it will provide you with the latest version of weights.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Image classification with ResNet\n",
    "\n",
    "You have created the model from the pre-trained ResNet18. Now, it is time to test it on an example image.\n",
    "\n",
    "You are going to apply preprocessing transforms to an image and classify it. You will need to use the softmax() layer followed by the argmax(), since ResNet18 has been trained on a multi-class dataset.\n",
    "\n",
    "You have selected the following image to use for prediction testing: A cup of espresso\n",
    "\n",
    "The preprocessing transform is saved as transform. The PIL image is uploaded as img.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Apply the preprocessing transforms to the image and reshape it using .unsqueeze(0) to add the batch dimension.\n",
    "\n",
    "    Pass the image through the model, reshape the output using .squeeze(0) to remove the batch dimension, and add a softmax() layer.\n",
    "\n",
    "    Apply argmax() to select the highest-probability class.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Apply preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Apply model with softmax layer\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "\n",
    "# Apply argmax\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(category_name)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! With the ResNet pre-trained model you are able to classify images without training and without any additional labeled data!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
