{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object recognition\n",
    "\n",
    "Which of the following statements about object recognition and bounding boxes is true?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Bounding boxes are represented by the x and y coordinates of each of the four box corners, so eight numbers altogether.\n",
    "    \n",
    "    \n",
    "    Object recognition's goal is only to identify the spatial location of an object within the image.\n",
    "    \n",
    "    \n",
    "    Bounding boxes are both the way to annotate the training data for object recognition tasks as well as the outputs of the models. {Answer}\n",
    "\n",
    "**Correct! Box coordinates constitute the ground truth labels as well as model predictions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "\n",
    "image = PIL.Image.open('/home/nero/Documents/Estudos/DataCamp/Python/courses/deep-learning-for-images-with-pytorch/Espresso.jpeg')\n",
    "\n",
    "bbox = [10, 10, 200, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0039,  ..., 0.0980, 0.0941, 0.0902],\n",
      "         [0.0000, 0.0000, 0.0039,  ..., 0.1020, 0.0980, 0.0902],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1059, 0.1020, 0.0902],\n",
      "         ...,\n",
      "         [0.5137, 0.5255, 0.5255,  ..., 0.9098, 0.9098, 0.9059],\n",
      "         [0.5294, 0.5294, 0.5294,  ..., 0.8863, 0.8902, 0.9020],\n",
      "         [0.5294, 0.5373, 0.5333,  ..., 0.8863, 0.8902, 0.9020]],\n",
      "\n",
      "        [[0.0039, 0.0039, 0.0078,  ..., 0.1176, 0.1137, 0.1098],\n",
      "         [0.0039, 0.0039, 0.0078,  ..., 0.1216, 0.1176, 0.1098],\n",
      "         [0.0039, 0.0039, 0.0039,  ..., 0.1255, 0.1216, 0.1098],\n",
      "         ...,\n",
      "         [0.4706, 0.4824, 0.4824,  ..., 0.7216, 0.7176, 0.7137],\n",
      "         [0.4745, 0.4863, 0.4863,  ..., 0.6980, 0.7020, 0.7098],\n",
      "         [0.4745, 0.4824, 0.4863,  ..., 0.6980, 0.7020, 0.7098]],\n",
      "\n",
      "        [[0.0196, 0.0196, 0.0235,  ..., 0.1294, 0.1255, 0.1216],\n",
      "         [0.0196, 0.0196, 0.0235,  ..., 0.1333, 0.1294, 0.1216],\n",
      "         [0.0196, 0.0196, 0.0196,  ..., 0.1373, 0.1333, 0.1216],\n",
      "         ...,\n",
      "         [0.4588, 0.4706, 0.4745,  ..., 0.6980, 0.7020, 0.6941],\n",
      "         [0.4627, 0.4706, 0.4706,  ..., 0.6824, 0.6863, 0.6941],\n",
      "         [0.4627, 0.4706, 0.4667,  ..., 0.6824, 0.6863, 0.6941]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! Usually, we use transforms.ToTensor() float transformation for images to scale them in the range [0,1]. However, the bounding box requires an unscaled image type with the range [0, 255], so we use transforms.PILToTensor().\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Image tensors\n",
    "\n",
    "A coffee company has an object detection project where they need to annotate objects of interest, in this case, espresso shots. You have created a list with the bounding box coordinates for an espresso shot image. Now, you need to convert the image and the coordinates into tensors.\n",
    "\n",
    "torch and torchvision have been imported. torchvision.transforms is imported as transforms. The image has been loaded as image using Image.open() from PIL library. The bounding box coordinates are stored in the variable bbox.\n",
    "\n",
    "espresso\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Convert the bbox into tensors using torch.tensor().\n",
    "    Reshape bbox_tensor by adding a batch dimension using unsqueeze(0).\n",
    "    Create a transform to resize image to (224) and transform to an unscaled image tensor.\n",
    "    Apply transform to image.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Convert bbox into tensors\n",
    "bbox_tensor = torch.tensor(bbox)\n",
    "\n",
    "# Add a new batch dimension\n",
    "bbox_tensor = bbox_tensor.unsqueeze(0)\n",
    "\n",
    "# Resize the image and transform to tensor\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize(244),\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply transform to image\n",
    "image_tensor = transform(image)\n",
    "print(image_tensor)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Usually, we use transforms.ToTensor() float transformation for images to scale them in the range [0,1]. However, the bounding box requires an unscaled image type with the range [0, 255], so we use transforms.PILToTensor().\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_min, x_max, y_max, y_min = 70, 190, 180, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! The bounding box seems to fit well! In the next video, we will take a look at a more qunatitative approach to evaluating the bounding box accuracy.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Drawing a bounding box\n",
    "\n",
    "Having prepared the image and bounding box tensors, you can now draw the box on top of the image and visually inspect its accuracy.\n",
    "\n",
    "torch, torchvision,torchvision.transforms have been imported. Image has been already transformed to tensors as image_tensor. The coordinates have been assigned to the variables: x_min, y_min, x_max, y_max.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Import draw_bounding_boxes from torchvision.utils.\n",
    "\n",
    "    Define the bounding box bbox as list consisting of x_min, y_min, x_max, and y_max.\n",
    "\n",
    "    Pass image_tensor and bbox_tensor to draw_bounding_boxes to draw the box on top of the image and assign the output to img_bbox.\n",
    "\n",
    "    Convert the img_bbox tensor to image.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import draw_bounding_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "# Define the bounding box coordinates\n",
    "bbox = [x_min, y_min, x_max, y_max]\n",
    "bbox_tensor = torch.tensor(bbox).unsqueeze(0)\n",
    "\n",
    "# Implement draw_bounding_boxes\n",
    "image_tensor = image_tensor.to(torch.uint8)\n",
    "img_bbox = draw_bounding_boxes(image_tensor, bbox_tensor, width=3, colors=\"red\")\n",
    "\n",
    "# Tranform tensors to image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "plt.imshow(transform(img_bbox))\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! The bounding box seems to fit well! In the next video, we will take a look at a more qunatitative approach to evaluating the bounding box accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Calculate IoU\n",
    "\n",
    "You have been asked to calculate the Intersection over Union (IoU) metric between each of the three predicted bounding boxes (box_a,box_b,box_c) and the ground truth box bbox.\n",
    "\n",
    "Which predicted box is the closest to the ground truth?\n",
    "\n",
    "box_iou has been imported from torchvision.ops. box_a, box_b, box_c, bbox has been already converted to torch.int type tensors with an additional batch dimension.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Possible answers\n",
    "    \n",
    "    box_a\n",
    "    \n",
    "    box_b\n",
    "    \n",
    "    box_c\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "boxes = [box_a, box_b, box_c]\n",
    "for box in boxes:\n",
    "    prinbt(box_iou(box, bbox))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! The best box scores the IoU > 0.5, indicating it's a good match!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Bounding boxes prediction\n",
    "\n",
    "You have trained an object recognition model. Now, you want to use to generate bounding boxes and classifications for a test image.\n",
    "\n",
    "The model is available as model and it's already in the evaluation mode. The test_image is also available, and torch and torch.nn as nn have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Get the model's output for the test_image and assign it to output.\n",
    "\n",
    "    Extract the predicted bounding boxes from the output and assign them to boxes.\n",
    "\n",
    "    Extract the predicted scores from the output and assign them to scores.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Get model's prediction\n",
    "with torch.no_grad():\n",
    "    output = model(test_image)\n",
    "\n",
    "# Extract boxes from the output\n",
    "boxes = output[0]['boxes']\n",
    "\n",
    "# Extract scores from the output\n",
    "scores = output[0]['scores']\n",
    "\n",
    "print(boxes, scores)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Predicted box coordinates and their confidence scores are important values for understanding how the model is performing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Calculate NMS\n",
    "\n",
    "Having extracted the predicted bounding boxes and scores from your object recognition model, your next task is to ensure that only the most accurate and non-overlapping predicted bounding boxes are retained by using the non-max suppression technique.\n",
    "\n",
    "boxes and scores you created in the previous exercise are available in your workspace and torch and torchvision have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import nms from torchvision.ops.\n",
    "\n",
    "    Set the IoU threshold to be equal 0.5.\n",
    "\n",
    "    Apply non-max suppression passing boxes, confidence_scores, and iou_threshold to the relevant function.\n",
    "\n",
    "    Use the output indices to filter predicted boxes.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import nms\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# Set the IoU threshold\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Apply non-max suppression\n",
    "box_indices = nms(boxes, scores, iou_threshold)\n",
    "\n",
    "# Filter boxes\n",
    "filtered_boxes = boxes[box_indices]\n",
    "\n",
    "print(\"Filtered Boxes:\", filtered_boxes)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You filtered boxes with the highest IoU overlap and highest probability scores. You can make the threshold more strict by decreasing it: feel free to verify that with iou_threshold = 0.2, you will only get a single, best bounding box!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Pre-trained model backbone\n",
    "\n",
    "It's time to build an R-CNN architecture! You will use the vgg16 pre-trained model's backbone for feature extraction. You also remember to store the output shape of the backbone which will serve as the input shape for the subsequent blocks: the classifier and the box regressor.\n",
    "\n",
    "torch, torchvision, torch.nn as nn have been imported. The model has been imported as vgg16 with the weights stored in VGG16_Weights.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Load the pre-trained VGG16 weights.\n",
    "\n",
    "    Extract in_features from the classifier's first layer using .children() as a sequential block and store it as input_dim.\n",
    "\n",
    "    Create a backbone as a sequential block using features and .children().\n",
    "\n",
    "    Print the backbone model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Load pretrained weights\n",
    "vgg_model = vgg16(weights=VGG16_Weights)\n",
    "\n",
    "# Extract the input dimension\n",
    "input_dim = nn.Sequential(*list(vgg_model.classifier.children()))[0].in_features\n",
    "\n",
    "# Create a backbone with convolutional layers\n",
    "backbone = nn.Sequential(*list(vgg_model.features.children()))\n",
    "\n",
    "# Print the backbone model\n",
    "print(backbone)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! The pre-trained backbone is essential for extracting features from the input image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Classifier block\n",
    "\n",
    "Your next task is to create a classifier block that will replace the original VGG16 classifier. You decide to use a block with two fully connected layers with a ReLU activation in between.\n",
    "\n",
    "The vgg_model and input_dim you defined in the last exercise are available in your workspace, and torch and torchvision.models have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a variable num_classes with the number of classes assuming you're dealing with detecting cats and dogs only.\n",
    "\n",
    "    Create a sequential block using nn.Sequential.\n",
    "\n",
    "    Create a linear layer with in_features set to input_dim.\n",
    "\n",
    "    Add the output features to the classifier's last layer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a variable with the number of classes\n",
    "num_classes = 2\n",
    "    \n",
    "# Create a sequential block\n",
    "classifier = nn.Sequential(\n",
    "\t# Create a linear layer with input features\n",
    "\tnn.Linear(input_dim, 512),\n",
    "\tnn.ReLU(),\n",
    "\t# Add the output dimension to the classifier\n",
    "\tnn.Linear(512, num_classes),\n",
    ")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Your classifier block is ready.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Box regressor block\n",
    "\n",
    "Your final task is to create a regressor block to predict bounding box coordinates. You decide to have a block with 2 fully connected layers with a ReLU activation in between, similar to the classifier you defined earlier.\n",
    "\n",
    "Your vgg_model and input_dim are still available and torch and torchvision.models have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a variable num_coordinates with the number of boundary box coordinates to predict.\n",
    "\n",
    "    Define the appropriate input dimension for the first linear layer and set the output dimension to 32.\n",
    "\n",
    "    Define the appropriate output dimension in the regressor's last layer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define the number of coordinates\n",
    "num_coordinates = 4\n",
    "\n",
    "bb = nn.Sequential(  \n",
    "\t# Add input and output dimensions\n",
    "\tnn.Linear(input_dim, 32),\n",
    "\tnn.ReLU(),\n",
    "\t# Add the output for the last regression layer\n",
    "\tnn.Linear(32, num_coordinates),\n",
    ")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! You have mastered all building blocks of the R-CNN architecture for object recognition!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Anchor generator\n",
    "\n",
    "Your team is developing object detection models based on the Faster R-CNN architecture and using pre-trained backbones. Your task is to create anchor boxes to serve as reference bounding boxes for proposing potential object regions.\n",
    "\n",
    "You will create 9 standard anchors (3 box sizes and 3 aspect ratios).\n",
    "\n",
    "torch has been import for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import AnchorGenerator from torchvision.models.detection.rpn.\n",
    "    \n",
    "    Configure anchor sizes with 3 values: ((32, 64, 128),).\n",
    "    \n",
    "    Configure aspect ratio with 3 values `((0.5, 1.0, 2.0),).\n",
    "    \n",
    "    Instantiate AnchorGenerator with anchor_sizes and aspect_ratios.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Configure anchor size\n",
    "anchor_sizes = ((32,64,128),)\n",
    "\n",
    "# Configure aspect ratio\n",
    "aspect_ratios = ((0.5,1.0,2.0),)\n",
    "\n",
    "# Instantiate AnchorGenerator\n",
    "rpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! The AnchorGenerator can generate anchor boxes for multiple sizes and for different scales in the image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Faster R-CNN model\n",
    "\n",
    "Your next task is to build a Faster R-CNN model that can detect objects of different sizes in an image. For this task, you will be using a handy class MultiScaleRoIAlign() from torchvision.ops.\n",
    "\n",
    "FasterRCNN class has been imported from torchvision.models.detection. Your anchor_generator from the last exercise is available in your workspace and torch, torch.nn as nn, and torchvision have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import MultiScaleRoIAlign from torchvision.ops.\n",
    "    \n",
    "    Instantiate the RoI pooler using MultiScaleRoIAlign with featmap_names set \n",
    "    to [\"0\"], output_size to 7, and sampling_ratio to 2.\n",
    "    \n",
    "    Create the Faster R-CNN model passing it the backbone, num_class for a binary classification, anchor_generator, and roi_pooler.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import MultiScaleRoIAlign\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "# Instantiate RoI pooler\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "\tfeatmap_names=['0'],\n",
    "\toutput_size=7,\n",
    "\tsampling_ratio=2,\n",
    ")\n",
    "\n",
    "mobilenet = torchvision.models.mobilenet_v2(weights=\"DEFAULT\")\n",
    "backbone = nn.Sequential(*list(mobilenet.features.children()))\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# Create Faster R-CNN model\n",
    "model = FasterRCNN(\n",
    "\tbackbone=backbone,\n",
    "\tnum_classes=2,\n",
    "\tanchor_generator=anchor_generator,\n",
    "\tbox_roi_pool=roi_pooler,\n",
    ")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You can now build a Faster R-CNN with the Region Proposal Network!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Define losses for RPN and R-CNN\n",
    "\n",
    "You are planning to train an object detection model that utilizes both the RPN and R-CNN components. To be able to train it, you will need to define the loss function for each component.\n",
    "\n",
    "You remember that the RPN component classifies whether a region contains an object and predicts the bounding box coordinates for the proposed regions.The R-CNN component classifies the object into one of multiple classes while also predicting the final bounding box coordinates.\n",
    "\n",
    "torch, torch.nn as nn have been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the RPN classification loss function and assign it to rpn_cls_criterion.\n",
    "\n",
    "    Define the RPN regression loss function and assign it to rpn_reg_criterion.\n",
    "\n",
    "    Define the R-CNN classification loss function and assign it to rcnn_cls_criterion.\n",
    "\n",
    "    Define the R-CNN regression loss function using and assign it to rcnn_reg_criterion.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Implement the RPN classification loss function\n",
    "rpn_cls_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Implement the RPN regression loss function\n",
    "rpn_reg_criterion = nn.MSELoss()\n",
    "\n",
    "# Implement the R-CNN classification Loss function\n",
    "rcnn_cls_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Implement the R-CNN regression loss function\n",
    "rcnn_reg_criterion = nn.MSELoss()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Using the correct loss functions is crucial for training machine learning models effectively and it ensures that the model updates its parameters correctly.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
