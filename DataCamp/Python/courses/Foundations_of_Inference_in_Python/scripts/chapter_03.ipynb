{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market</th>\n",
       "      <th>funding_total_usd</th>\n",
       "      <th>status</th>\n",
       "      <th>country_code</th>\n",
       "      <th>funding_rounds</th>\n",
       "      <th>seed</th>\n",
       "      <th>venture</th>\n",
       "      <th>equity_crowdfunding</th>\n",
       "      <th>private_equity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Games</td>\n",
       "      <td>4000000</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Software</td>\n",
       "      <td>7000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advertising</td>\n",
       "      <td>4912393</td>\n",
       "      <td>closed</td>\n",
       "      <td>ARG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Curated Web</td>\n",
       "      <td>2000000</td>\n",
       "      <td>operating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Games</td>\n",
       "      <td>41250</td>\n",
       "      <td>operating</td>\n",
       "      <td>HKG</td>\n",
       "      <td>1</td>\n",
       "      <td>41250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        market  funding_total_usd     status country_code  funding_rounds  \\\n",
       "0        Games            4000000  operating          USA               2   \n",
       "1     Software            7000000        NaN          USA               1   \n",
       "2  Advertising            4912393     closed          ARG               1   \n",
       "3  Curated Web            2000000  operating          NaN               1   \n",
       "4        Games              41250  operating          HKG               1   \n",
       "\n",
       "    seed  venture  equity_crowdfunding  private_equity  \n",
       "0      0  4000000                    0               0  \n",
       "1      0  7000000                    0               0  \n",
       "2      0        0                    0               0  \n",
       "3      0  2000000                    0               0  \n",
       "4  41250        0                    0               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "investments_df = pd.read_csv(path_data+'investments_VC.csv')\n",
    "investments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07719192881235956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nIf you printed out d in the console you'll see it's only about 0.08. That's a surprisingly low value! That tells us that moving to a second round of funding does not in itself have a large effect on the amount of money raised. This is likely due to how large the standard deviations are, which means that the means are unreliable estimates.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Effect size for means\n",
    "\n",
    "Many venture capital-backed companies receive more than one round of funding. In general, the second round is bigger than the first. Just how much of an effect does the round number have on the average funding amount? You can use Cohen's d to quantify this.\n",
    "\n",
    "Recall that, to calculate Cohen's d, you need to first calculate the pooled standard deviation. That is given by the equation\n",
    "\n",
    "s = √((n1 - 1)s1² + (n2 - 1)s2²)/(n1 + n2 - 2)\n",
    "\n",
    "Cohen's d is then given by:\n",
    "\n",
    "(X_1.mean - X_2.mean)/s\n",
    "\n",
    "A DataFrame of venture capital investments (investments_df) has been loaded for you, as have the packages pandas as pd, NumPy as np and stats from SciPy. The column funding_total_usd shows the total funding received in that round.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Filter investments_df to select funding_rounds 1 and 2 separately.\n",
    "    Calculate the standard deviation and sample size of each round.\n",
    "    Calculate the pooled standard deviation between the two rounds.\n",
    "    Calculate Cohen's d using the terms you just calculated.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Select all investments from rounds 1 and 2 separately\n",
    "round1_df = investments_df[investments_df['funding_rounds'] == 1]\n",
    "round2_df = investments_df[investments_df['funding_rounds'] == 2]\n",
    "\n",
    "# Calculate the standard deviation of each round and the number of companies in each round\n",
    "round1_sd = round1_df['funding_total_usd'].std()\n",
    "round2_sd = round2_df['funding_total_usd'].std()\n",
    "round1_n = round1_df.shape[0]\n",
    "round2_n = round2_df.shape[0]\n",
    "\n",
    "# Calculate the pooled standard deviation between the two rounds\n",
    "pooled_sd = np.sqrt(((round1_n - 1) * round1_sd ** 2 + (round2_n - 1) *round2_sd ** 2) / (round1_n + round2_n - 2))\n",
    "\n",
    "# Calculate Cohen's d\n",
    "d = (round1_df['funding_total_usd'].mean() - round2_df['funding_total_usd'].mean()) / pooled_sd\n",
    "\n",
    "print(d)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "If you printed out d in the console you'll see it's only about 0.08. That's a surprisingly low value! That tells us that moving to a second round of funding does not in itself have a large effect on the amount of money raised. This is likely due to how large the standard deviations are, which means that the means are unreliable estimates.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_BTC</th>\n",
       "      <th>High_BTC</th>\n",
       "      <th>Low_BTC</th>\n",
       "      <th>Close_BTC</th>\n",
       "      <th>Close_SP500</th>\n",
       "      <th>Open_SP500</th>\n",
       "      <th>High_SP500</th>\n",
       "      <th>Low_SP500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>3212.780029</td>\n",
       "      <td>3397.679932</td>\n",
       "      <td>3180.889893</td>\n",
       "      <td>3378.939941</td>\n",
       "      <td>2480.91</td>\n",
       "      <td>2477.14</td>\n",
       "      <td>2480.95</td>\n",
       "      <td>2475.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-08</td>\n",
       "      <td>3370.219971</td>\n",
       "      <td>3484.850098</td>\n",
       "      <td>3345.830078</td>\n",
       "      <td>3419.939941</td>\n",
       "      <td>2474.92</td>\n",
       "      <td>2478.35</td>\n",
       "      <td>2490.87</td>\n",
       "      <td>2470.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>3420.399902</td>\n",
       "      <td>3422.760010</td>\n",
       "      <td>3247.669922</td>\n",
       "      <td>3342.469971</td>\n",
       "      <td>2474.02</td>\n",
       "      <td>2465.35</td>\n",
       "      <td>2474.41</td>\n",
       "      <td>2462.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>3341.840088</td>\n",
       "      <td>3453.449951</td>\n",
       "      <td>3319.469971</td>\n",
       "      <td>3381.280029</td>\n",
       "      <td>2438.21</td>\n",
       "      <td>2465.38</td>\n",
       "      <td>2465.38</td>\n",
       "      <td>2437.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-11</td>\n",
       "      <td>3373.820068</td>\n",
       "      <td>3679.719971</td>\n",
       "      <td>3372.120117</td>\n",
       "      <td>3650.620117</td>\n",
       "      <td>2441.32</td>\n",
       "      <td>2441.04</td>\n",
       "      <td>2448.09</td>\n",
       "      <td>2437.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Open_BTC     High_BTC      Low_BTC    Close_BTC  \\\n",
       "0  2017-08-07  3212.780029  3397.679932  3180.889893  3378.939941   \n",
       "1  2017-08-08  3370.219971  3484.850098  3345.830078  3419.939941   \n",
       "2  2017-08-09  3420.399902  3422.760010  3247.669922  3342.469971   \n",
       "3  2017-08-10  3341.840088  3453.449951  3319.469971  3381.280029   \n",
       "4  2017-08-11  3373.820068  3679.719971  3372.120117  3650.620117   \n",
       "\n",
       "   Close_SP500  Open_SP500  High_SP500  Low_SP500  \n",
       "0      2480.91     2477.14     2480.95    2475.88  \n",
       "1      2474.92     2478.35     2490.87    2470.32  \n",
       "2      2474.02     2465.35     2474.41    2462.08  \n",
       "3      2438.21     2465.38     2465.38    2437.75  \n",
       "4      2441.32     2441.04     2448.09    2437.85  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_sp_df = pd.read_csv(path_data+'btc_sp.csv')\n",
    "btc_sp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 between volatility of the assets: 0.03152987723111431\n",
      "R^2 between closing price and volatility of BTC: 0.01252065913517721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhile examining things like line graphs and scatter plots can help you gain an intuitive understanding of the data, rigorous statistical tests are a necessity. They take beyond 'I feel like...' and towards 'The data shows that...'. These tools are key in principled decision making.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Effect size for correlations\n",
    "\n",
    "The volatility of an asset is roughly defined by how much its price changes. In this exercise you'll measure volatility on a per-day basis, defined as the (high price - low price) / closing price.\n",
    "\n",
    "What factors explain the volatility of Bitcoin? Is the volatility of the S&P500 closely related to this? Does volatility increase or decrease as prices rise? In other words, what is the effect size of the correlation between these different factors? You'll compute both of these effect size in this exercise.\n",
    "\n",
    "A DataFrame of S&P 500 and Bitcoin prices (btc_sp_df) has been loaded for you, as have the packages pandas as pd, NumPy as np, Matplotlib as plt, and stats from SciPy.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Compute the volatility of BTC.\n",
    "Repeat for the S&P 500.\n",
    "Compute R² between the volatility of each asset.\n",
    "Compute R² between the volatility and closing price of BTC.\n",
    "---\n",
    "Question\n",
    "\n",
    "Neither the volatility of the S&P 500 nor the closing price of BTC has a large correlation with the volatility of BTC. However, you still learn something from the correlations! You can see that the volatility of the S&P 500 explains about 3% of the variation of volatility of BTC, while the closing price of BTC explains about 1%. Therefore, price swings in BTC aren't simply related to price swings in the S&P 500, nor in the price of BTC being especially high/low. These sorts of quantitative assessments are important to conduct.\n",
    "\n",
    "Next, you'll turn to inference. Which of the following statements are supported by the effect size calculations you just ran?\n",
    "[The volatility in the S&P 500 has the greater effect on the volatility of Bitcoin.]\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute the volatility of Bitcoin\n",
    "btc_sp_df['Volatility_BTC'] = (btc_sp_df['High_BTC'] - btc_sp_df['Low_BTC']) / btc_sp_df['Close_BTC']\n",
    "\n",
    "# Compute the volatility of the S&P500\n",
    "btc_sp_df['Volatility_SP500'] = (btc_sp_df['High_SP500'] - btc_sp_df['Low_SP500']) / btc_sp_df['Close_SP500']\n",
    "\n",
    "# Compute and print R^2 between the volatility of BTC and SP500\n",
    "r_volatility, p_value_volatility = stats.pearsonr(btc_sp_df['Volatility_BTC'], btc_sp_df['Volatility_SP500'])\n",
    "print('R^2 between volatility of the assets:', r_volatility**2)\n",
    "\n",
    "# Compute and print R^2 between the volatility of BTC and the closing price of BTC\n",
    "r_closing, p_value_closing = stats.pearsonr(btc_sp_df['Volatility_BTC'], btc_sp_df['Close_BTC'])\n",
    "print('R^2 between closing price and volatility of BTC:', r_closing**2)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "While examining things like line graphs and scatter plots can help you gain an intuitive understanding of the data, rigorous statistical tests are a necessity. They take beyond 'I feel like...' and towards 'The data shows that...'. These tools are key in principled decision making.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black or African American</th>\n",
       "      <th>Hispanic or Latino</th>\n",
       "      <th>White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Administrative Specialist</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>99</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fire Specialist</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Firefighter</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>127</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MuniProg, Paraprofessional</td>\n",
       "      <td>37</td>\n",
       "      <td>142</td>\n",
       "      <td>227</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Police Corporal/Detective</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>77</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title  Asian  Black or African American  \\\n",
       "0   Administrative Specialist      5                         34   \n",
       "1             Fire Specialist      8                          9   \n",
       "2                 Firefighter      5                         37   \n",
       "3  MuniProg, Paraprofessional     37                        142   \n",
       "4   Police Corporal/Detective      5                         31   \n",
       "\n",
       "   Hispanic or Latino  White  \n",
       "0                  99     78  \n",
       "1                  36    149  \n",
       "2                 127    361  \n",
       "3                 227    493  \n",
       "4                  77    263  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees_df = pd.read_csv(path_data+'employees_df.csv')\n",
    "employees_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cramer's V: 0.11173756740290954 \n",
      "Degrees of freedom: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nUnlike R-squared, Cramer's V doesn't have an easy interpretation. But by knowing the degrees of freedom you can get a general feel for how large or small an effect one categorical variable has on another. Here you see that job title and ethnicity are certainly related, but not directly linked. This might lead you to search for what other factors are influencing the job a person gets.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Effect size for categorical variables\n",
    "\n",
    "You saw in the City of Austin employee data that job titles have an unequal distribution of genders. But does the same thing hold for ethnicities? And to what extent does ethnicity relate to the job title chosen? In this exercise you'll dig in and answer that question.\n",
    "\n",
    "A DataFrame of comparing job titles and ethnicities (employees_df) has been loaded for you, as have the packages pandas as pd, NumPy as np, and stats from SciPy.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Compute the chi-squared statistic from the contingency table employees_df.\n",
    "    Compute the degrees of freedom for Cramer's V.\n",
    "    Compute the total number of people in the contingency table.\n",
    "    Compute Cramer's V using the equation from the video.\n",
    "---\n",
    "Question\n",
    "\n",
    "While a chi-squared test can test for association between two categorical variables, it doesn't directly answer the question of the degree of association. By computing an effect size like Cramer's V, you can directly measure the effect that one variable has on the other.\n",
    "\n",
    "Now, you'll turn to inference. What can you conclude from the value of Cramer's V shown in the console?\n",
    "[Ethnicity has a moderate effect on the job title a person holds.]\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute the chi-squared statistic\n",
    "chi2, p, d, expected = stats.chi2_contingency(employees_df.iloc[:,1:])\n",
    "\n",
    "# Compute the DOF using the number of rows and columns\n",
    "dof = min(employees_df.shape[0] - 1, employees_df.shape[1] - 1)\n",
    "\n",
    "# Compute the total number of people\n",
    "n = np.sum(employees_df.iloc[:,1:].values)\n",
    "\n",
    "# Compute Cramer's V\n",
    "v = np.sqrt((chi2 / n) / dof)\n",
    "\n",
    "print(\"Cramer's V:\", v, \"\\nDegrees of freedom:\", dof)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Unlike R-squared, Cramer's V doesn't have an easy interpretation. But by knowing the degrees of freedom you can get a general feel for how large or small an effect one categorical variable has on another. Here you see that job title and ethnicity are certainly related, but not directly linked. This might lead you to search for what other factors are influencing the job a person gets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Annual Salary</th>\n",
       "      <th>Years of Employment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police Officer</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>72681.44</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Police Officer</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>83210.40</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Police Officer</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>83210.40</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Police Officer</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>72681.44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Police Officer</td>\n",
       "      <td>M</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>95270.24</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Title Gender           Ethnicity  Annual Salary  \\\n",
       "0  Police Officer      M               White       72681.44   \n",
       "1  Police Officer      M               White       83210.40   \n",
       "2  Police Officer      M               White       83210.40   \n",
       "3  Police Officer      M               White       72681.44   \n",
       "4  Police Officer      M  Hispanic or Latino       95270.24   \n",
       "\n",
       "   Years of Employment  \n",
       "0                    6  \n",
       "1                   15  \n",
       "2                   13  \n",
       "3                    5  \n",
       "4                   20  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "police_salaries_df = pd.read_csv(path_data+'police_salaries_df.csv')\n",
    "police_salaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNotice how about 50 out of 1000, or 5%, of the results were significant. This is not a coincidence! Your cutoff for significance was 5%, meaning about 5% of the time a random correlation will cross this threshold. This clearly demonstrates the problem with repeatedly running experiments and assuming that low p-values indicate something meaningful.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Multiple comparisons problem\n",
    "\n",
    "The multiple comparisons problem arises when a researcher repeatedly checks different variables/samples against one another for significance. Just by random chance we expect to find an occasional result of statistical significance.\n",
    "\n",
    "In this exercise you'll work with data from salaries for employees at the City of Austin, TX. You will compare their salaries against randomly generated data. You will see how often this random data is \"significant\" in explaining the salaries of employees. Clearly any such \"significance\" would be spurious, as random numbers aren't very helpful in explaining anything!\n",
    "\n",
    "A DataFrame of police officers salaries (police_salaries_df) has been loaded for you, as have the packages pandas as pd, NumPy as np, Matplotlib as plt, and stats from SciPy.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Store the number of people in the dataset in n_rows (each row is a person), and initialize the number of significant results, n_significant, to zero.\n",
    "    Write a for loop which runs 1000 times and generates n_rows random numbers.\n",
    "    Compute Pearson's R and the associated p-value between these randomly generated numbers and the police officer salaries.\n",
    "    If the p-value is significant at 5%, add one to n_significant using the += operator.\n",
    "\n",
    "\"\"\"\n",
    "p_values = []\n",
    "# solution\n",
    "\n",
    "# Compute number of rows and initialize n_significant\n",
    "n_rows = police_salaries_df.shape[0]\n",
    "n_significant = 0\n",
    "\n",
    "# For loop which generates n_rows random numbers 1000 times\n",
    "for i in range(1000):\n",
    "  random_nums = np.random.uniform(size=n_rows)\n",
    "  # Compute correlation between random_nums and police salaries\n",
    "  r, p_value = stats.pearsonr(police_salaries_df['Annual Salary'], random_nums)\n",
    "  # If the p-value is significant at 5%, increment n_significant\n",
    "  p_values.append(p_value)\n",
    "  if p_value < 0.05:\n",
    "    n_significant += 1\n",
    "    \n",
    "print(n_significant)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Notice how about 50 out of 1000, or 5%, of the results were significant. This is not a coincidence! Your cutoff for significance was 5%, meaning about 5% of the time a random correlation will cross this threshold. This clearly demonstrates the problem with repeatedly running experiments and assuming that low p-values indicate something meaningful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nBeyond knowing about statistical tools, it's important to know exactly when to apply them. By understanding that the role of the Bonferonni correction is to minimize the chance of spurious correlations being deemed statistically significant, you know exactly when and where to use it!\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Bonferonni-Holm correction\n",
    "\n",
    "You've seen that comparing many different datasets, even randomly generated ones, can result in \"statistically significant relationships\" that are anything but! One way around this is to apply a correction to the alpha of your confidence level. In this exercise you'll explore why you should apply this correction and how to do so.\n",
    "\n",
    "The 1000 p-values you calculated in the previous exercise have been loaded for you in a NumPy array p_values, as has the package NumPy as np.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Compute the Bonferonni-corrected value of alpha = 5%.\n",
    "    Print out how many of the p-values were less than this corrected cutoff.\n",
    "---\n",
    "Question\n",
    "\n",
    "What a drastic change from the last exercise! Previously you had quite a few spurious correlations being detected. Now you likely had none, or very close! This means that the bar for what is significant has been raised drastically. This helps reduce the chance of spurious correlations being marked at significant.\n",
    "\n",
    "When you used the uncorrected alpha, about 50 of the 1000 experiments (or 5% of them) were \"significant\". When using the Bonferonni-Holm correction, none (or close to none) of them were significant. Which of the following options explains why this happened?\n",
    "[The Bonferonni-Holm correction reduces the probability of rejecting the null proportional to the number of experiments performed.]\n",
    "\n",
    "---\n",
    "\n",
    "Question\n",
    "\n",
    "Which of the following explains when you should use the Bonferonni-Holm correction when making inference?\n",
    "[Any time you wish to make many different comparisons to perform inference.]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute the Bonferonni-corrected alpha\n",
    "bonf_alpha = 0.05/1000\n",
    "p_values = np.array(p_values)\n",
    "# Check how many p-values were significant at this level\n",
    "print(sum(p_values < bonf_alpha))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Beyond knowing about statistical tools, it's important to know exactly when to apply them. By understanding that the role of the Bonferonni correction is to minimize the chance of spurious correlations being deemed statistically significant, you know exactly when and where to use it!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is power anyway?\n",
    "\n",
    "Some of the wording around power can be quite confusing. For example, the power of a two-sample t-test is:\n",
    "\n",
    "*The probability of rejecting the null in favor of the alternative, assuming there is indeed a difference in population means between the two groups.*\n",
    "\n",
    "That's quite a mouthful!\n",
    "\n",
    "Imagine you are analyzing a marketing campaign and want to infer if it increases customer spending. You conduct a two-sample t-test (customers exposed to the campaign versus customers not exposed) to compare the mean dollar amounts spent for each group.\n",
    "\n",
    "![Categories](/home/nero/Documents/Estudos/DataCamp/Python/courses/Foundations_of_Inference_in_Python/ch03.png)\n",
    "\n",
    "\n",
    "**Being able to translate between statistical language and concrete examples is an incredibly important skill. Notice how sample and population means can be re-expressed as groups of customers and all possible customers, tests can be re-expressed as looking for significant differences in samples, and power can be expressed as the chance of detection.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power for experimental design\n",
    "\n",
    "Imagine that you collected a sample of 100 people for your study and spend time and money executing it. After your study was done, you realized that the power of your test was only 10%. In other words, even if there was a difference between your groups, there is only a 10% chance your test would detect it, given the data you supplied. What a waste of effort!\n",
    "\n",
    "Therefore, the best practice is to estimate power before collecting data and running an experiment. In this exercise you'll organize the steps followed in this process. The items below are steps in the process of experimental design using power analysis.\n",
    "\n",
    "![Solution](/home/nero/Documents/Estudos/DataCamp/Python/courses/Foundations_of_Inference_in_Python/ch03_02.png)\n",
    "\n",
    "**Notice how following this process lays out a blueprint for conducting your analysis. By following a principled approach you are able to collect a sample that is likely to help you test your hypothesis in a principled way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10123429934046722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "games = investments_df[investments_df['market'] == 'Games']\n",
    "ads = investments_df[investments_df['market'] == 'Advertising']\n",
    "\n",
    "\n",
    "# Calculate the standard deviation of each round and the number of companies in each round\n",
    "games_sd = games['funding_total_usd'].std()\n",
    "ads_sd = ads['funding_total_usd'].std()\n",
    "games_n = games.shape[0]\n",
    "ads_n = ads.shape[0]\n",
    "\n",
    "# Calculate the pooled standard deviation between the two rounds\n",
    "pooled_sd = np.sqrt(((games_n - 1) * games_sd ** 2 + (ads_n - 1) * ads_sd ** 2) / (games_n + ads_n - 2))\n",
    "\n",
    "# Calculate Cohen's d\n",
    "d = (games['funding_total_usd'].mean() - ads['funding_total_usd'].mean()) / pooled_sd\n",
    "\n",
    "display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1532.6875406057334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nPower can be a tricky topic, great job mastering it! You've been able to use solve_power() to identify that you need nearly 14,000 participants in each group! While much of the techniques you've learned so far focus on simply running a test and hoping for the best, power analysis allows you to understand just how likely your test is to be successful.\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Computing power and sample sizes\n",
    "\n",
    "You want to compare average funding per investment round for games companies versus advertising companies. You begin by computing the power of a two sample t-test.\n",
    "\n",
    "Three things have been loaded for you:\n",
    "\n",
    "    ads_acquired_avg_funding - (pandas Series) The average funding of each acquired advertising company\n",
    "    games_acquired_avg_funding - (pandas Series) Same, but for game companies\n",
    "    ads_games_cohensd - (Number) Cohen's d (standardized effect size) for advertising versus games company's\n",
    "\n",
    "The function needed to compute power has been imported from statsmodels.stats.power.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Compute the ratio of games companies (games_n) to advertising companies (ads_n).\n",
    "    Compute power with an effect size ads_games_cohensd, alpha of 5%, number of advertising companies ads_n as nobs1, and the games_ads_ratio above.\n",
    "---\n",
    "\n",
    "    Solve for the sample size nobs1 needed to achieve a power of 80% with an alpha of 5% and an effect size of ads_games_cohensd by setting nobs1 equal to None in solve_power().\n",
    "    Print nobs1, the number of observations in one group (e.g. ads).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "# Compute the ratio of games to advertising companies\n",
    "games_ads_ratio = games_n / ads_n\n",
    "\n",
    "TTestIndPower().power(effect_size=d, \n",
    "                      nobs1=ads_n,\n",
    "                      alpha=0.05,\n",
    "                      ratio=games_ads_ratio)\n",
    "\n",
    "# Solve for the sample size needed to achieve a power of 80%\n",
    "nobs1 = TTestIndPower().solve_power(effect_size=d, nobs1=None, alpha=0.05, power=0.8)\n",
    "\n",
    "# Print the number of participants needed in one group\n",
    "print(nobs1)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Power can be a tricky topic, great job mastering it! You've been able to use solve_power() to identify that you need nearly 14,000 participants in each group! While much of the techniques you've learned so far focus on simply running a test and hoping for the best, power analysis allows you to understand just how likely your test is to be successful.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
