{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Introduction_to_TensorFlow_in_Python/datasets/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import Variable, keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                12560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 08:32:16.191414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-21 08:32:16.193117: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent work! Notice that we've defined a model, but we haven't compiled it. The compilation step in keras allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the .summary() method allows us to view the model's architecture\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "The sequential model in Keras\n",
    "\n",
    "In chapter 3, we used components of the keras API in tensorflow to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the keras sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the .summary() method to print the model's architecture, including the shape and number of parameters associated with each layer.\n",
    "\n",
    "Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that keras has been imported from tensorflow for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a keras sequential model named model.\n",
    "    Set the first layer to be Dense() and to have 16 nodes and a relu activation.\n",
    "    Define the second layer to be Dense() and to have 8 nodes and a relu activation.\n",
    "    Set the output layer to have 4 nodes and use a softmax activation function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Notice that we've defined a model, but we haven't compiled it. The compilation step in keras allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the .summary() method allows us to view the model's architecture\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 16)                12560     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,628\n",
      "Trainable params: 12,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat work! You've now defined and compiled a neural network using the keras sequential model. Notice that printing the .summary() method shows the layer type, output shape, and number of parameters of each layer.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Compiling a sequential model\n",
    "\n",
    "In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the adam optimizer and the categorical_crossentropy loss. You will also use a method in keras to summarize your model's architecture. Note that keras has been imported from tensorflow for you and a sequential keras model has been defined as model.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    In the first dense layer, set the number of nodes to 16, the activation to sigmoid, and the input_shape to (784,).\n",
    "    Apply dropout at a rate of 25% to the first layer's output.\n",
    "    Set the output layer to be dense, have 4 nodes, and use a softmax activation function.\n",
    "    Compile the model using an adam optimizer and categorical_crossentropy loss function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation = 'sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! You've now defined and compiled a neural network using the keras sequential model. Notice that printing the .summary() method shows the layer type, output shape, and number of parameters of each layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_inputs = keras.Input(shape=(784,))\n",
    "m2_inputs = keras.Input(shape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 784)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 784)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 12)           9420        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 12)           9420        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4)            52          ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4)            52          ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4)            0           ['dense_10[0][0]',               \n",
      "                                                                  'dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,944\n",
      "Trainable params: 18,944\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice work! Notice that the .summary() method yields a new column: connected to. This column tells you how layers connect to each other within the network. We can see that dense_2, for instance, is connected to the input_2 layer. We can also see that the add layer, which merged the two models, connected to both dense_1 and dense_3.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Defining a multiple input model\n",
    "\n",
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model's architecture.\n",
    "\n",
    "Note that keras has been imported from tensorflow for you. Additionally, the input layers of the first and second models have been defined as m1_inputs and m2_inputs, respectively. Note that the two models have the same architecture, but one of them uses a sigmoid activation in the first layer and the other uses a relu.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Pass model 1's input layer to its first layer and model 1's first layer to its second layer.\n",
    "    Pass model 2's input layer to its first layer and model 2's first layer to its second layer.\n",
    "    Use the add() operation to combine the second layers of model 1 and model 2.\n",
    "    Complete the functional model definition.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! Notice that the .summary() method yields a new column: connected to. This column tells you how layers connect to each other within the network. We can see that dense_2, for instance, is connected to the input_2 layer. We can also see that the add layer, which merged the two models, connected to both dense_1 and dense_3.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 25) and (None, 28, 28, 4) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39mSGD\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39m# Complete the fitting operation\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m \u001b[39m#----------------------------------#\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Conclusion\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mGreat work! You probably noticed that your only measure of performance improvement was the value of the loss function in the training sample, which is not particularly informative. You will improve on this in the next exercise.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileh01rqjup.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 25) and (None, 28, 28, 4) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Training with Keras\n",
    "\n",
    "In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.\n",
    "\n",
    "Note that keras has been imported from tensorflow for you. Additionally, the features are available as sign_language_features and the targets are available as sign_language_labels.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a sequential model named model.\n",
    "    Set the output layer to be dense, have 4 nodes, and use a softmax activation function.\n",
    "    Compile the model with the SGD optimizer and categorical_crossentropy loss.\n",
    "    Complete the fitting operation and set the number of epochs to 5.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! You probably noticed that your only measure of performance improvement was the value of the loss function in the training sample, which is not particularly informative. You will improve on this in the next exercise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Metrics and validation with Keras\n",
    "\n",
    "We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation.\n",
    "\n",
    "Note that keras has been imported for you from tensorflow.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Set the first dense layer to have 32 nodes, use a sigmoid activation function, and have an input shape of (784,).\n",
    "    Use the root mean square propagation optimizer, a categorical crossentropy loss, and the accuracy metric.\n",
    "    Set the number of epochs to 10 and use 10% of the dataset for validation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.10)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! With the keras API, you only needed 14 lines of code to define, compile, train, and validate a model. You may have noticed that your model performed quite well. In just 10 epochs, we achieved a classification accuracy of over 90% in the validation sample!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Overfitting detection\n",
    "\n",
    "In this exercise, we'll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.\n",
    "\n",
    "You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting.\n",
    "\n",
    "Note that keras has been imported from tensorflow.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a sequential model in keras named model.\n",
    "    Add a first dense layer with 1024 nodes, a relu activation, and an input shape of (784,).\n",
    "    Set the learning rate to 0.001.\n",
    "    Set the fit() operation to iterate over the full sample 50 times and use 50% of the sample for validation purposes.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(1024, activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Finish the model compilation\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Complete the model fit operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=0.5)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! You may have noticed that the validation loss, val_loss, was substantially higher than the training loss, loss. Furthermore, if val_loss started to increase before the training process was terminated, then we may have overfitted. When this happens, you will want to try decreasing the number of epochs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Evaluating models\n",
    "\n",
    "Two models have been trained and are available: large_model, which has many parameters; and small_model, which has fewer parameters. Both models have been trained using train_features and train_labels, which are available to you. A separate test set, which consists of test_features and test_labels, is also available.\n",
    "\n",
    "Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating large_model and small_model on both the train and test sets. For each model, you can do this by applying the .evaluate(x, y) method to compute the loss for features x and labels y. You will then compare the four losses generated.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Evaluate the small model using the train data.\n",
    "    Evaluate the small model using the test data.\n",
    "    Evaluate the large model using the train data.\n",
    "    Evaluate the large model using the test data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Evaluate the small model using the train data\n",
    "small_train = small_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the small model using the test data\n",
    "small_test = small_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Evaluate the large model using the train data\n",
    "large_train = large_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the large model using the test data\n",
    "large_test = large_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Print losses\n",
    "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
    "print('Large - Train: {}, Test: {}'.format(large_train, large_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Notice that the gap between the test and train set losses is high for large_model, suggesting that overfitting may be an issue. Furthermore, both test and train set performance is better for large_model. This suggests that we may want to use large_model, but reduce the number of training epochs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(path_data + 'kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent work! In the next exercise, we'll use the feature columns and data input function to define and train an estimator.\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Preparing to train with Estimators\n",
    "\n",
    "For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the estimator API.\n",
    "\n",
    "Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade estimator. Note that feature_column has been imported for you from tensorflow. Additionally, numpy has been imported as np, and the Kings County housing dataset is available as a pandas DataFrame: housing.   \n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Complete the feature column for bedrooms and add another numeric feature column for bathrooms. Use bedrooms and bathrooms as the keys.\n",
    "    Create a list of the feature columns, feature_list, in the order in which they were defined.\n",
    "    Set labels to be equal to the price column in housing.\n",
    "    Complete the bedrooms entry of the features dictionary and add another entry for bathrooms.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from tensorflow import  feature_column\n",
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = feature_column.numeric_column('bathrooms')\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing['price'])\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "\treturn features, labels\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! In the next exercise, we'll use the feature columns and data input function to define and train an estimator.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpt__2meex\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpt__2meex', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'inspect' has no attribute 'getargspec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Define the model and set the number of steps\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mDNNRegressor(feature_columns\u001b[39m=\u001b[39mfeature_list, hidden_units\u001b[39m=\u001b[39m[\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m model\u001b[39m.\u001b[39;49mtrain(input_fn, steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m \u001b[39m#----------------------------------#\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m# Define the model and set the number of steps\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mLinearRegressor(feature_columns\u001b[39m=\u001b[39mfeature_list)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:360\u001b[0m, in \u001b[0;36mEstimator.train\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    357\u001b[0m hooks\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_train_steps_to_hooks(steps, max_steps))\n\u001b[1;32m    359\u001b[0m saving_listeners \u001b[39m=\u001b[39m _check_listeners_type(saving_listeners)\n\u001b[0;32m--> 360\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_model(input_fn, hooks, saving_listeners)\n\u001b[1;32m    361\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mLoss for final step: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, loss)\n\u001b[1;32m    362\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:1186\u001b[0m, in \u001b[0;36mEstimator._train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1184\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_model_distributed(input_fn, hooks, saving_listeners)\n\u001b[1;32m   1185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1186\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_model_default(input_fn, hooks, saving_listeners)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:1214\u001b[0m, in \u001b[0;36mEstimator._train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1211\u001b[0m features, labels, input_hooks \u001b[39m=\u001b[39m (\n\u001b[1;32m   1212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_features_and_labels_from_input_fn(input_fn, ModeKeys\u001b[39m.\u001b[39mTRAIN))\n\u001b[1;32m   1213\u001b[0m worker_hooks\u001b[39m.\u001b[39mextend(input_hooks)\n\u001b[0;32m-> 1214\u001b[0m estimator_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_model_fn(features, labels, ModeKeys\u001b[39m.\u001b[39;49mTRAIN,\n\u001b[1;32m   1215\u001b[0m                                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n\u001b[1;32m   1216\u001b[0m global_step_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mget_global_step(g)\n\u001b[1;32m   1217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1218\u001b[0m                                        hooks, global_step_tensor,\n\u001b[1;32m   1219\u001b[0m                                        saving_listeners)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:1174\u001b[0m, in \u001b[0;36mEstimator._call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1171\u001b[0m   kwargs[\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m config\n\u001b[1;32m   1173\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mCalling model_fn.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1174\u001b[0m model_fn_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_fn(features\u001b[39m=\u001b[39;49mfeatures, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1175\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mDone calling model_fn.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model_fn_results, model_fn_lib\u001b[39m.\u001b[39mEstimatorSpec):\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py:1159\u001b[0m, in \u001b[0;36mDNNRegressorV2.__init__.<locals>._model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_model_fn\u001b[39m(features, labels, mode, config):\n\u001b[1;32m   1158\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Call the defined shared dnn_model_fn_v2.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m   \u001b[39mreturn\u001b[39;00m dnn_model_fn_v2(\n\u001b[1;32m   1160\u001b[0m       features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1161\u001b[0m       labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1162\u001b[0m       mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1163\u001b[0m       head\u001b[39m=\u001b[39;49mhead,\n\u001b[1;32m   1164\u001b[0m       hidden_units\u001b[39m=\u001b[39;49mhidden_units,\n\u001b[1;32m   1165\u001b[0m       feature_columns\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(feature_columns \u001b[39mor\u001b[39;49;00m []),\n\u001b[1;32m   1166\u001b[0m       optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m   1167\u001b[0m       activation_fn\u001b[39m=\u001b[39;49mactivation_fn,\n\u001b[1;32m   1168\u001b[0m       dropout\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m   1169\u001b[0m       config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m   1170\u001b[0m       batch_norm\u001b[39m=\u001b[39;49mbatch_norm)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py:570\u001b[0m, in \u001b[0;36mdnn_model_fn_v2\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[39m# In TRAIN mode, create optimizer and assign global_step variable to\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m# optimizer.iterations to make global_step increased correctly, as Hooks\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m# relies on global step as step counter.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m ModeKeys\u001b[39m.\u001b[39mTRAIN:\n\u001b[0;32m--> 570\u001b[0m   optimizer \u001b[39m=\u001b[39m optimizers\u001b[39m.\u001b[39;49mget_optimizer_instance_v2(optimizer)\n\u001b[1;32m    571\u001b[0m   optimizer\u001b[39m.\u001b[39miterations \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mget_or_create_global_step()\n\u001b[1;32m    573\u001b[0m \u001b[39m# Create EstimatorSpec.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/optimizers.py:127\u001b[0m, in \u001b[0;36mget_optimizer_instance_v2\u001b[0;34m(opt, learning_rate)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m opt \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miterkeys(_OPTIMIZER_CLS_NAMES_V2):\n\u001b[1;32m    126\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m learning_rate:\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mif\u001b[39;00m _optimizer_has_default_learning_rate(_OPTIMIZER_CLS_NAMES_V2[opt]):\n\u001b[1;32m    128\u001b[0m       \u001b[39mreturn\u001b[39;00m _OPTIMIZER_CLS_NAMES_V2[opt]()\n\u001b[1;32m    129\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/optimizers.py:90\u001b[0m, in \u001b[0;36m_optimizer_has_default_learning_rate\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_optimizer_has_default_learning_rate\u001b[39m(opt):\n\u001b[0;32m---> 90\u001b[0m   signature \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetargspec(opt\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m)\n\u001b[1;32m     91\u001b[0m   default_name_to_value \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39margs[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], signature\u001b[39m.\u001b[39mdefaults))\n\u001b[1;32m     92\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m default_name_to_value\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'inspect' has no attribute 'getargspec'"
     ]
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Defining Estimators\n",
    "\n",
    "In the previous exercise, you defined a list of feature columns, feature_list, and a data input function, input_fn(). In this exercise, you will build on that work by defining an estimator that makes use of input data.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Use a deep neural network regressor with 2 nodes in both the first and second hidden layers and 1 training step.\n",
    "\n",
    "Modify the code to use a LinearRegressor(), remove the hidden_units, and set the number of steps to 2.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "from tensorflow import estimator\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Note that you have other premade estimator options, such as BoostedTreesRegressor(), and can also create your own custom estimators.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
