{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0 - Name: NVIDIA GeForce MX110, Memory Capacity: 2002.9375 MB\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/courses/Intermediate_Deep_Learning_with_PyTorch/datasets/'\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and properties of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i} - Name: {gpu_properties.name}, \"\n",
    "              f\"Memory Capacity: {gpu_properties.total_memory / (1024 ** 2)} MB\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "# Set the default device to GPU (\"cuda\") if available, otherwise use CPU (\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGreat job! You can now use create_sequences() to create a set of training or testing examples for the model, where each example consists of an input of seq_length consecutive data points, and the single target, the following data point. You'll put this into action in the next exercise!\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Generating sequences\n",
    "\n",
    "To be able to train neural networks on sequential data, you need to pre-process it first. You'll chunk the data into inputs-target pairs, where the inputs are some number of consecutive data points and the target is the next data point.\n",
    "\n",
    "Your task is to define a function to do this called create_sequences(). As inputs, it will receive data stored in a DataFrame, df and seq_length, the length of the inputs. As outputs, it should return two NumPy arrays, one with input sequences and the other one with the corresponding targets.\n",
    "\n",
    "As a reminder, here is how the DataFrame df looks like:\n",
    "\n",
    "                 timestamp  consumption\n",
    "0      2011-01-01 00:15:00    -0.704319\n",
    "...                    ...          ...\n",
    "140255 2015-01-01 00:00:00    -0.095751\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Iterate over the range of the number of data points minus the length of an input sequence.\n",
    "    Define the inputs x as the slice of df from the ith row to the i + seq_lengthth row and the column at index 1.\n",
    "    Define the target y as the slice of df at row index i + seq_length and the column at index 1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(df)-seq_length):\n",
    "      \t# Define inputs\n",
    "        x = df.iloc[i:(i+seq_length), 1]\n",
    "        # Define target\n",
    "        y = df.iloc[i+seq_length, 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! You can now use create_sequences() to create a set of training or testing examples for the model, where each example consists of an input of seq_length consecutive data points, and the single target, the following data point. You'll put this into action in the next exercise!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(path_data+'electricity_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105119, 96) (105119,)\n",
      "105119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nPerfect! As you can see from the printed output, we have 105119 training examples, each consisting of 96 inputs and 1 target value. The TensorDataset you have just built behaves the same way as other Torch Datasets you have used before, such us our custom WaterDataset or the ImageFolder dataset; you can pass it to a DataLoader in the same way. With the sequential data ready, let's take a look at model architectures suitable for processing sequential data!\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Sequential Dataset\n",
    "\n",
    "Good job building the create_sequences() function! It's time to use it to create a training dataset for your model.\n",
    "\n",
    "Just like tabular and image data, sequential data is easiest passed to a model through a torch Dataset and DataLoader. To build a sequential Dataset, you will call create_sequences() to get the NumPy arrays with inputs and targets, and inspect their shape. Next, you will pass them to a TensorDataset to create a proper torch Dataset, and inspect its length.\n",
    "\n",
    "Your implementation of create_sequences() and a DataFrame with the training data called train_data are available.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Call create_sequences(), passing it the training DataFrame and a sequence length of 24*4, assigning the result to X_train, y_train.\n",
    "    Define dataset_train by calling TensorDataset and passing it two arguments, the inputs and the targets created by create_sequences(), both converted from NumPy arrays to tensors of floats.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Use create_sequences to create inputs and targets\n",
    "X_train, y_train = create_sequences(train_data, 24*4)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float(),\n",
    ")\n",
    "print(len(dataset_train))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Perfect! As you can see from the printed output, we have 105119 training examples, each consisting of 96 inputs and 1 target value. The TensorDataset you have just built behaves the same way as other Torch Datasets you have used before, such us our custom WaterDataset or the ImageFolder dataset; you can pass it to a DataLoader in the same way. With the sequential data ready, let's take a look at model architectures suitable for processing sequential data!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFantastic! That's a pretty neat RNN! Before you move on to training and evaluating the model on our electricity consumption data, let's take a look at how we can make the simple RNN layer slightly more powerful.\\n\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Building a forecasting RNN\n",
    "\n",
    "It's time to build your first recurrent network! It will be a sequence-to-vector model consisting of an RNN layer with two layers and a hidden_size of 32. After the RNN layer, a simple linear layer will map the outputs to a single value to be predicted.\n",
    "\n",
    "The following imports have already be done for you:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the RNN layer passing it the correct values for input_size, hidden_size, num_layers, and batch_first, and assign it to self.rnn.\n",
    "---\n",
    "\n",
    "    Initialize the first hidden state h0 as a tensor of zeros of the appropriate shape.\n",
    "---\n",
    "\n",
    "    Pass the input x and the first hidden state h0 through recurrent layer.\n",
    "---\n",
    "\n",
    "    Pass recurrent layer's last output through the linear layer\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# solution\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize first hidden state with zeros\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass x and h0 through recurrent layer\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # Pass recurrent layer's last output through linear layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fantastic! That's a pretty neat RNN! Before you move on to training and evaluating the model on our electricity consumption data, let's take a look at how we can make the simple RNN layer slightly more powerful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGood job! With only a few updates to the simple RNN's code, you have build a much powerful recurrent model! Let's see if you can do similar adaptations to create a GRU network in the next exercise!\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "LSTM network\n",
    "\n",
    "As you already know, plain RNN cells are not used that much in practice. A more frequently used alternative that ensures a much better handling of long sequences are Long Short-Term Memory cells, or LSTMs. In this exercise, you will be build an LSTM network yourself!\n",
    "\n",
    "The most important implementation difference from the RNN network you have built previously comes from the fact that LSTMs have two rather than one hidden states. This means you will need to initialize this additional hidden state and pass it to the LSTM cell.\n",
    "\n",
    "torch and torch.nn have already been imported for you, so start coding!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    In the .__init__() method, define an LSTM layer and assign it to self.lstm.\n",
    "    In the forward() method, initialize the first long-term memory hidden state c0 with zeros.\n",
    "    In the forward() method, pass all three inputs to the LSTM layer: the current time step's inputs, and a tuple containing the two hidden states.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! With only a few updates to the simple RNN's code, you have build a much powerful recurrent model! Let's see if you can do similar adaptations to create a GRU network in the next exercise!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPerfectly done! As you can see, PyTorch makes it very convenient to build different recurrent network flavors without requiring many changes in the code. Now that you know how to build LSTM and GRU models, let's take a look at training and evaluating them!\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "GRU network\n",
    "\n",
    "Next to LSTMs, another popular recurrent neural network variant is the Gated Recurrent Unit, or GRU. It's appeal is in its simplicity: GRU cells require less computation than LSTM cells while often matching them in performance.\n",
    "\n",
    "The code you are provided with is the RNN model definition that you coded previously. Your task is to adapt it such that it produces a GRU network instead. torch and torch.nn as nn have already been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Update the RNN model definition in order to obtain a GRU network.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Perfectly done! As you can see, PyTorch makes it very convenient to build different recurrent network flavors without requiring many changes in the code. Now that you know how to build LSTM and GRU models, let's take a look at training and evaluating them!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105215, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6575.9375"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "105215/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "  dataset_train, shuffle=True, batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 96, 1]' is invalid for input of size 1440",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m seqs, labels \u001b[39min\u001b[39;00m dataloader_train:\n\u001b[1;32m     37\u001b[0m         \u001b[39m# Reshape model inputs\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         seqs \u001b[39m=\u001b[39m seqs\u001b[39m.\u001b[39;49mview(\u001b[39m16\u001b[39;49m, \u001b[39m96\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m         \u001b[39m#print(seqs.shape)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[39m# Get model outputs\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         outputs \u001b[39m=\u001b[39m net(seqs)\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 96, 1]' is invalid for input of size 1440"
     ]
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "RNN training loop\n",
    "\n",
    "It's time to train the electricity consumption forecasting model!\n",
    "\n",
    "You will use the LSTM network you have defined previously which is available to you as Net, as is the dataloader_train you built before. You will also need to use torch.nn which has already been imported as nn.\n",
    "\n",
    "In this exercise, you will train the model for only three epochs to make sure the training progresses as expected. Let's get to it!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Set up the Mean Squared Error loss and assign it to criterion.\n",
    "    Reshape seqs to (batch size, sequence length, num features), which in our case is (16, 96, 1), and re-assign the result to seqs.\n",
    "    Pass seqs to the model to get its outputs.\n",
    "    Based on previously computed quantities, calculate the loss, assigning it to loss.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "net = Net()\n",
    "# Set up MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "  net.parameters(), lr=0.0001\n",
    ")\n",
    "\n",
    "for epoch in range(1):\n",
    "    for seqs, labels in dataloader_train:\n",
    "        # Reshape model inputs\n",
    "        seqs = seqs.view(16, 96, 1)\n",
    "        #print(seqs.shape)\n",
    "        # Get model outputs\n",
    "        outputs = net(seqs).squeeze()\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nicely done! You should be able to see the loss decreasing over the first three training epochs, which means that the model is learning! Naturally, to have a well-trained models, more epochs are needed. Let's evaluate the model next!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Evaluating forecasting models\n",
    "\n",
    "It's evaluation time! The same LSTM network that you have trained in the previous exercise has been trained for you for a few more epochs and is available as net.\n",
    "\n",
    "Your task is to evaluate it on a test dataset using the Mean Squared Error metric (torchmetrics has already been imported for you). Let's see how well the model is doing!\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the Mean Squared Error metrics and assign it to mse.\n",
    "    Pass the input sequence to net, and squeeze the result before you assign it to outputs.\n",
    "    Compute the final value of the test metric assigning it to test_mse.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define MSE metric\n",
    "mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in dataloader_test:\n",
    "        seqs = seqs.view(32, 96, 1)\n",
    "        # Pass seqs to net and squeeze the result\n",
    "        outputs = net(seqs).squeeze()\n",
    "        mse(outputs, labels)\n",
    "\n",
    "# Compute final metric value\n",
    "test_mse = mse.compute()\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations, you've come a long way! You can now prepare sequantial data for model injection, build RNN, LSTM and GRU architectures for time series forecasting, as well as train and evaluate recurrent models. In the final Chapter of the course, we will build multi-input and multi-output models—see you there!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
