{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place test modules at the correct location\n",
    "\n",
    "A data science project without visualization is like pizza without cheese, right? But this has been fixed by creating a package called visualization under the top level application directory src.\n",
    "\n",
    "    src/                                    # All application code lives here\n",
    "    |-- visualization/                      # Package for visualization\n",
    "        |-- __init__.py\n",
    "        |-- plots.py                        # Module for plotting\n",
    "\n",
    "In the package, there is a Python module plots.py, which contain functions related to plotting. These functions should be tested in a test module test_plots.py.\n",
    "\n",
    "According to pytest guidelines, where should you place this test module within the project structure?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    src/visualization/test_plots.py.\n",
    "    \n",
    "    \n",
    "    src/visualization/tests/test_plots.py.\n",
    "    \n",
    "    \n",
    "    tests/test_plots.py.\n",
    "    \n",
    "    \n",
    "    tests/visualization/test_plots.py. {Answer}\n",
    "\n",
    "**Placing it in this location gives us two advantages: easier navigation within the tests folder and the possibility of having identically named test modules distinguished by the parent mirror package.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Create a test class\n",
    "\n",
    "Test classes are containers inside test modules. They help separate tests for different functions within the test module, and serve as a structuring tool in the pytest framework.\n",
    "\n",
    "Test classes are written in CamelCase e.g. TestMyFunction as opposed to tests, which are written using underscores e.g. test_something().\n",
    "\n",
    "You met the function split_into_training_and_testing_sets() in Chapter 2, and wrote some tests for it. One of these tests was called test_on_one_row() and it checked if the function raises a ValueError when passed a NumPy array with only one row.\n",
    "\n",
    "In this exercise you are going to create a test class for this function. This test class will hold the test test_on_one_row().\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Declare the test class for the function split_into_training_and_testing_sets(), making sure to give it a name that follows the standard naming convention.\n",
    "\n",
    "    Fill in the mandatory argument in the test test_on_one_row().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "One command to run them all\n",
    "\n",
    "One of your colleagues pushed some changes to the functions row_to_list(), convert_to_int(), get_data_as_numpy_array() and split_into_training_and_testing_sets(). That means that you have to run all the tests again to figure out if something got broken as a result.\n",
    "\n",
    "The current working directory in the IPython console is the tests directory, which contains all the tests in the same layout as described in the video. You can, at any time, run the tests in the IPython console using the appropriate command.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Question\n",
    "\n",
    "In the IPython console, what is the correct command for running all tests contained in the tests folder?\n",
    "Possible answers\n",
    "\n",
    "    !pytest {Answer}\n",
    "    \n",
    "    !pytest -x\n",
    "    \n",
    "    !pytest tests\n",
    "    \n",
    "    pytest\n",
    "---\n",
    "Question\n",
    "\n",
    "When you run all tests with the command !pytest, how many of them pass and how may fail?\n",
    "Possible answers\n",
    "\n",
    "    Passing: 10, Failing: 6\n",
    "    \n",
    "    Passing: 15, Failing: 1 {Answer}\n",
    "    \n",
    "    Passing 16, Failing: 0\n",
    "---\n",
    "Question\n",
    "\n",
    "Assuming that you simply want to answer the binary question \"Are all tests passing\" without wasting time and resources, what is the correct command to run all tests till the first failure is encountered?\n",
    "Possible answers\n",
    "\n",
    "    !pytest -k\n",
    "    \n",
    "    !pytest\n",
    "    \n",
    "    !pytest -x {Answer}\n",
    "---\n",
    "Question\n",
    "\n",
    "When you ran the tests using the !pytest -x command, how many tests ran in total before test execution stopped because of the first failing test?\n",
    "Possible answers\n",
    "    \n",
    "    16\n",
    "    \n",
    "    15 {Answer}\n",
    "    \n",
    "    7\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! In real life, the !pytest or !pytest -x command is often used in CI servers. It can also be useful if there is a major update to the code base, which changes many application modules at the same time. Running all tests is the only way to check if anything was broken due to the update.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThat's correct! The -k flag is really useful, because it helps you select tests and test classes by typing only a unique part of its name. This saves a lot of typing, and you must admit that TestSplitIntoTrainingAndTestingSets is a horrendously long name! In your projects, you will often run tests with the node IDs and the -k flag because you are often not interested in running all tests, but only a subset depending on the functions you are currently working on.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Running test classes\n",
    "\n",
    "When you ran the !pytest command in the last exercise, the test test_on_six_rows() failed. This is a test for the function split_into_training_and_testing_sets(). This means that this function is broken.\n",
    "\n",
    "Short recap in case you forgot: this function takes a NumPy array containing housing area and prices as argument. The function randomly splits the argument array into training and testing arrays in the ratio 3:1, and returns the resulting arrays in a tuple.\n",
    "\n",
    "A quick look revealed that during the code update, someone inadvertently changed the split from 3:1 to 9:1. This has to be changed back and the unit tests for the function, which now lives in the test class TestSplitIntoTrainingAndTestingSets, needs to be run again. Are you up to the challenge?\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Fill in with a float between 0 and 1 so that num_training is approximately\n",
    "---\n",
    "Question\n",
    "\n",
    "Now let's see if that modification fixed the broken function. The current working directory in the IPython console is the tests folder that contains all tests. The test class TestSplitIntoTrainingAndTestingSets resides in the test module tests/models/test_train.py.\n",
    "\n",
    "What is the correct command to run all the tests in this test class using node IDs?\n",
    "Possible answers\n",
    "    \n",
    "    !pytest models::test_train.py::TestSplitIntoTrainingAndTestingSets\n",
    "    \n",
    "    !pytest -k \"TestSplitIntoTrainingAndTestingSets\"\n",
    "    \n",
    "    !pytest models/test_train.py/TestSplitIntoTrainingAndTestingSets\n",
    "    \n",
    "    !pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets {Answer}\n",
    "---\n",
    "Question\n",
    "\n",
    "What is the correct command to run only the previously failing test test_on_six_rows() using node IDs?\n",
    "Possible answers\n",
    "    \n",
    "    !pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows {Answer}\n",
    "    \n",
    "    !pytest models/test_train.py::test_on_six_rows\n",
    "    \n",
    "    !pytest test_on_six_rows\n",
    "---\n",
    "Question\n",
    "\n",
    "What is the correct command to run the tests in TestSplitIntoTrainingAndTestingSets using keyword expressions?\n",
    "Possible answers\n",
    "    \n",
    "    !pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets\n",
    "    \n",
    "    !pytest -x \"TestSplitIntoTrainingAndTestingSets\"\n",
    "    \n",
    "    !pytest -k \"SplitInto\" {Answer}\n",
    "    \n",
    "    !pytest -k \"Test\"\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_into_training_and_testing_sets(data_array):\n",
    "    dim = data_array.ndim\n",
    "    if dim != 2:\n",
    "        raise ValueError(\"Argument data_array must be two dimensional. Got {0} dimensional array instead!\".format(dim))\n",
    "    num_rows = data_array.shape[0]\n",
    "    if num_rows < 2:\n",
    "        raise ValueError(\"Argument data_array must have at least 2 rows, it actually has just {0}\".format(num_rows))\n",
    "    # Fill in with the correct float\n",
    "    num_training = int(0.75 * data_array.shape[0])\n",
    "    permuted_indices = np.random.permutation(data_array.shape[0])\n",
    "    return data_array[permuted_indices[:num_training], :], data_array[permuted_indices[num_training:], :]\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That's correct! The -k flag is really useful, because it helps you select tests and test classes by typing only a unique part of its name. This saves a lot of typing, and you must admit that TestSplitIntoTrainingAndTestingSets is a horrendously long name! In your projects, you will often run tests with the node IDs and the -k flag because you are often not interested in running all tests, but only a subset depending on the functions you are currently working on.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Mark a test class as expected to fail\n",
    "\n",
    "A new function model_test() is being developed and it returns the accuracy of a given linear regression model on a testing dataset. Test Driven Development (TDD) is being used to implement it. The procedure is: write tests first and then implement the function.\n",
    "\n",
    "A test class TestModelTest has been created within the test module models/test_train.py. In the test class, there are two unit tests called test_on_linear_data() and test_on_one_dimensional_array(). But the function model_test() has not been implemented yet.\n",
    "\n",
    "Throughout this exercise, pytest and numpy as np will be imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Question\n",
    "\n",
    "Run the tests in the test class TestModelTest in the IPython console. What is the outcome?\n",
    "Possible answers\n",
    "    \n",
    "    The tests fail with IndexError because some arguments to format the variable message are missing.\n",
    "    \n",
    "    The tests pass.\n",
    "    \n",
    "    The tests fail with NameError since the function model_test() has not yet been defined. {Answer}\n",
    "    \n",
    "    The tests fail with AssertionError.\n",
    "---\n",
    "\n",
    "    Mark the whole test class TestModelTest as \"expected to fail\".\n",
    "---\n",
    "\n",
    "    Add the following reason for the expected failure: \"Using TDD, model_test() has not yet been implemented\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Add a reason for the expected failure\n",
    "@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):\n",
    "        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "        expected = 1.0\n",
    "        actual = model_test(test_input, 2.0, 1.0)\n",
    "        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "        \n",
    "    def test_on_one_dimensional_array(self):\n",
    "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            model_test(test_input, 1.0, 1.0)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! The reason you provided for the expected failure is useful for your colleagues, who might be wondering why you marked this test as expected to fail.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Mark a test as conditionally skipped\n",
    "\n",
    "In Python 2, there was a built-in function called xrange(). In Python 3, xrange() was removed. Therefore, if any test uses xrange(), it's going to fail with a NameError in Python 3.\n",
    "\n",
    "Remember the function get_data_as_numpy_array()? You saw it in Chapter 2. It converted data in a preprocessed data file into a NumPy array.\n",
    "\n",
    "range() has been deliberately replaced with the obsolete xrange() in the function. Evil laughter! But no worries, it will be changed back after you're done with this exercise.\n",
    "\n",
    "You wrote a test called test_on_clean_file() for this function. This test currently resides in a test class TestGetDataAsNumpyArray inside the test module features/test_as_numpy.py.\n",
    "\n",
    "pytest, numpy as np and get_data_as_numpy_array() has been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Question\n",
    "\n",
    "Run the tests in the test class TestGetDataAsNumpyArray in the IPython console. What is the outcome?\n",
    "Possible answers:\n",
    "    \n",
    "    The test test_on_clean_file() fails with a NameError because Python 3 does not recognize the xrange() function. {Answer}\n",
    "    \n",
    "    The test test_on_clean_file() passes.\n",
    "    \n",
    "    The test test_on_clean_file() fails with an AssertionError.\n",
    "---\n",
    "\n",
    "    Import the sys module.\n",
    "---\n",
    "\n",
    "    Mark the test test_on_clean_file() as skipped if the Python version is greater than 2.7.\n",
    "---\n",
    "\n",
    "    Add the following reason for skipping the test: \"Works only on Python 2.7 or lower\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the sys module\n",
    "import sys\n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    # Add a reason for skipping the test\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                             ]\n",
    "                            )\n",
    "        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! You can use any boolean expression as the first argument of pytest.mark.skipif. One other common situation is to skip tests that won't run on particular platforms like Windows, Linux or Mac using the sys.platform attribute.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Reasoning in the test result report\n",
    "\n",
    "In the last exercises, you marked the test class TestModelTest in the test module models/test_train.py as expected to fail. You also marked the test test_on_clean_file() in the test class TestGetDataAsNumpyArray belonging to the test module features/test_as_numpy.py as skipped if the Python version is greater than 2.7.\n",
    "\n",
    "In both cases, you provided a reason argument which detailed why they are expected to fail or skipped. In this exercise, your job is to make this reason show up in the test result report when you run all tests in the IPython console.\n",
    "\n",
    "Feel free to run the !pytest command with different options and flags in the IPython console while doing the exercise.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Question\n",
    "\n",
    "What is the command that would only show the reason for expected failures in the test result report?\n",
    "Possible answers:\n",
    "    \n",
    "    !pytest -r\n",
    "    \n",
    "    !pytest -rx {Answer}\n",
    "    \n",
    "    !pytest -x\n",
    "    \n",
    "    !pytest -rs\n",
    "---\n",
    "Question\n",
    "\n",
    "What is the command that would only show the reason for skipped tests in the test result report?\n",
    "Possible answers:\n",
    "    \n",
    "    !pytest -r.\n",
    "    \n",
    "    !pytest -rx.\n",
    "    \n",
    "    !pytest -s.\n",
    "    \n",
    "    !pytest -rs. {Answer}\n",
    "---\n",
    "Question\n",
    "\n",
    "What is the command that would show the reason for both skipped tests and tests that are expected to fail in the test result report?\n",
    "Possible answers:\n",
    "    \n",
    "    !pytest -rsx. {Answer}\n",
    "    \n",
    "    !pytest -sx.\n",
    "    \n",
    "    !pytest -s -x\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Seems like you have become a pro at the pytest command line tool. Congratulations!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build failing\n",
    "\n",
    "In the GitHub repository of a Python package, you see the following badge:\n",
    "\n",
    "![build_fail](/home/nero/Documents/Estudos/DataCamp/Python/courses/unit-testing-for-data-science-in-python/build_status_failing_no_whitespace.png)\n",
    "\n",
    "What can you, as a user, conclude from this badge?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    There are no unit tests in the package.\n",
    "    \n",
    "    \n",
    "    The package has a code coverage less than 75%.\n",
    "    \n",
    "    \n",
    "    There's no .travis.yml configuration file at the root of the repository.\n",
    "    \n",
    "    \n",
    "    The package has bugs, which is either causing installation to error out or some of the unit tests in the test suite to fail. {Answer}\n",
    "    \n",
    "**Since a build failing badge is indicative of bugs, the maintainer of any package should strive to keep this badge green (\"passing\").**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does code coverage mean?\n",
    "\n",
    "In a Github repository of a Python package, you see the following badge:\n",
    "\n",
    "![code_cov](/home/nero/Documents/Estudos/DataCamp/Python/courses/unit-testing-for-data-science-in-python/code_coverage_badge.png)\n",
    "\n",
    "What does it mean?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    The test suite tests about 85% of the application code. {Answer}\n",
    "    \n",
    "    \n",
    "    Unit tests make up 85% of the code base of the package.\n",
    "    \n",
    "    \n",
    "    Historically, the test suite has failed about 85% of the times it was ran.\n",
    "    \n",
    "    \n",
    "    The insurance pays 85% of any financial damages caused by malfunctioning of the package"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
