{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGood work! pytest keeps the fixtures separate from the tests as this encourages reusing fixtures for tests that need the same/similar setup and teardown code.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Use a fixture for a clean data file\n",
    "\n",
    "In the video, you saw how the preprocess() function creates a clean data file.\n",
    "\n",
    "The get_data_as_numpy_array() function takes the path to this clean data file as the first argument and the number of columns of data as the second argument. It returns a NumPy array holding the data.\n",
    "\n",
    "In a previous exercise, you wrote the test test_on_clean_file() without using a fixture. That's bad practice! This time, you'll use the fixture clean_data_file(), which\n",
    "\n",
    "    creates a clean data file in the setup,\n",
    "    yields the path to the clean data file,\n",
    "    removes the clean data file in the teardown.\n",
    "\n",
    "The contents of the clean data file that you will use for testing is printed in the IPython console.\n",
    "\n",
    "pytest, os, numpy as np and get_data_as_numpy_array() have been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    Add the correct decorator that would turn clean_data_file() into a fixture.\n",
    "    ---\n",
    "    Pass an argument to the test test_on_clean_file() so that it uses the fixture.\n",
    "    ---\n",
    "    Pass the clean data file path yielded by the fixture as the first argument to the function get_data_as_numpy_array().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import pytest\n",
    "\n",
    "# Add a decorator to make this function a fixture\n",
    "@pytest.fixture\n",
    "def clean_data_file():\n",
    "    file_path = \"clean_data_file.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
    "    yield file_path\n",
    "    os.remove(file_path)\n",
    "    \n",
    "# Pass the correct argument so that the test can use the fixture\n",
    "def test_on_clean_file(clean_data_file):\n",
    "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
    "    # Pass the clean data file path yielded by the fixture as the first argument\n",
    "    actual = get_data_as_numpy_array(clean_data_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good work! pytest keeps the fixtures separate from the tests as this encourages reusing fixtures for tests that need the same/similar setup and teardown code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRight you are! Notice that fixtures like empty_file() are quite reusable, since any function which accepts data files as arguments needs to be tested with an empty file.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Write a fixture for an empty data file\n",
    "\n",
    "When a function takes a data file as an argument, you need to write a fixture that takes care of creating and deleting that data file. This exercise will test your ability to write such a fixture.\n",
    "\n",
    "get_data_as_numpy_array() should return an empty numpy array if it gets an empty data file as an argument. To test this behavior, you need to write a fixture empty_file() that does the following.\n",
    "\n",
    "    Creates an empty data file empty.txt relative to the current working directory in setup.\n",
    "    Yields the path to the empty data file.\n",
    "    Deletes the empty data file in teardown.\n",
    "\n",
    "The fixture will be used by the test test_on_empty_file(), which is available for you to see in the script.\n",
    "\n",
    "os, pytest, numpy as np and get_data_as_numpy_array have been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    In the setup, assign the variable file_path to the correct string.\n",
    "    After the setup, yield the variable file_path so that the test can use it.\n",
    "    In the teardown, remove the file.\n",
    "---\n",
    "Question\n",
    "\n",
    "The test test_on_empty_file() was added to the test class TestGetDataAsNumpyArray, which lives in the test module tests/features/test_as_numpy.py. The fixture empty_file() was also written to this test module.\n",
    "\n",
    "Remembering that the current working directory in the IPython console is tests, run the test test_on_empty_file(). What is the outcome?\n",
    "Possible answers:\n",
    "    \n",
    "    The test passes. {Answer}\n",
    "    \n",
    "    The test fails because get_data_as_numpy_array() does not return an empty NumPy array, and instead returns None.\n",
    "    \n",
    "    The test fails with a NameError as Python 3 does not recognize the xrange() function.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "@pytest.fixture\n",
    "def empty_file():\n",
    "    # Assign the file path \"empty.txt\" to the variable\n",
    "    file_path = \"empty.txt\"\n",
    "    open(file_path, \"w\").close()\n",
    "    # Yield the variable file_path\n",
    "    yield file_path\n",
    "    # Remove the file in the teardown\n",
    "    os.remove(file_path)\n",
    "    \n",
    "def test_on_empty_file(self, empty_file):\n",
    "    expected = np.empty((0, 2))\n",
    "    actual = get_data_as_numpy_array(empty_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Right you are! Notice that fixtures like empty_file() are quite reusable, since any function which accepts data files as arguments needs to be tested with an empty file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWell ordered! Notice how you didn't have to write any teardown code to delete the empty file, because tmpdir is going to take care of that for us in its teardown step, which is executed last.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Fixture chaining using tmpdir\n",
    "\n",
    "The built-in tmpdir fixture is very useful when dealing with files in setup and teardown. tmpdir combines seamlessly with user defined fixture via fixture chaining.\n",
    "\n",
    "In this exercise, you will use the power of tmpdir to redefine and improve the empty_file() fixture that you wrote in the last exercise and get some experience with fixture chaining.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add the correct argument to the fixture empty_file() so that it chains with the built-in fixture tmpdir.\n",
    "    Use the appropriate method to create an empty file \"empty.txt\" inside the temporary directory created by tmpdir.\n",
    "---\n",
    "Question\n",
    "\n",
    "In what order will the setup and teardown of empty_file() and tmpdir be executed?\n",
    "Possible answers:\n",
    "\n",
    "    setup of empty_file() -> setup of tmpdir teardown of tmpdir -> teardown of empty_file().\n",
    "\n",
    "    setup of tmpdir -> setup of empty_file() -> teardown of empty_file() -> teardown of tmpdir. {Answer}\n",
    "\n",
    "    setup of tmpdir -> setup of empty_file() teardown of tmpdir teardown of -> empty_file()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "def empty_file(tmpdir):\n",
    "    # Use the appropriate method to create an empty file in the temporary directory\n",
    "    file_path = tmpdir.join(\"empty.txt\")\n",
    "    open(file_path, \"w\").close()\n",
    "    yield file_path\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well ordered! Notice how you didn't have to write any teardown code to delete the empty file, because tmpdir is going to take care of that for us in its teardown step, which is executed last.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPerfectly done! Note that the bug-free version of the dependency is usually much simpler that the dependency itself, since it just needs to be bug-free for the arguments that it receives within the test. It does not need to bug-free in the general case i.e. for any argument imaginable.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Program a bug-free dependency\n",
    "\n",
    "In the video, row_to_list() was mocked. But preprocess() has another dependency convert_to_int(). Generally, its best to mock all dependencies of the function under test. It's your job to mock convert_to_int() in this and the following exercises.\n",
    "\n",
    "The raw data file used in the test is printed in the IPython console. The second row \"1,767565,112\\n\" is dirty, so row_to_list() will filter it out. The rest will be converted to lists and convert_to_int() will process the areas and prices.\n",
    "\n",
    "The mocked convert_to_int() should process these areas and prices correctly. Here is the dictionary holding the correct return values.\n",
    "\n",
    "{\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define a function convert_to_int_bug_free() which takes one argument called comma_separated_integer_string.\n",
    "    Assign return_values to the dictionary holding the correct return values in the context of the raw data file used in the test.\n",
    "    Return the correct return value by looking up the dictionary return_values for the key comma_separated_integer_string.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Define a function convert_to_int_bug_free\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dictionary holding the correct return values \n",
    "    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
    "    # Return the correct result using the dictionary return_values\n",
    "    return return_values[comma_separated_integer_string]\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Perfectly done! Note that the bug-free version of the dependency is usually much simpler that the dependency itself, since it just needs to be bug-free for the arguments that it receives within the test. It does not need to bug-free in the general case i.e. for any argument imaginable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWow! You are turning into a seasoned tester! The results tell us that preprocess() is defined correctly and is bug-free. But one of its dependencies convert_to_int() has bugs. This kind of precise result is only possible using mocking.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Mock a dependency\n",
    "\n",
    "Mocking helps us replace a dependency with a MagicMock() object. Usually, the MagicMock() is programmed to be a bug-free version of the dependency. To verify whether the function under test works properly with the dependency, you simply check whether the MagicMock() is called with the correct arguments and in the right order.\n",
    "\n",
    "In the last exercise, you programmed a bug-free version of the dependency data.preprocessing_helpers.convert_to_int in the context of the test test_on_raw_data(), which applies preprocess() on a raw data file. The data file is printed out in the IPython console.\n",
    "\n",
    "pytest, unittest.mock.call, preprocess raw_and_clean_data_file and convert_to_int_bug_free has been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    In the test test_on_raw_data(), add the correct argument that enables the use of the mocking fixture.\n",
    "---\n",
    "\n",
    "    Replace the dependency \"data.preprocessing_helpers.convert_to_int\" with the bug-free version convert_to_int_bug_free() by using the correct method and side effect.\n",
    "---\n",
    "\n",
    "    Use the correct attribute which returns the list of calls to the mock, and check if the mock was called with this sequence of arguments: \"1,801\", \"201,411\", \"2,002\", \"333,209\", \"1990\", \"782,911\", \"1,285\", \"389129\".\n",
    "---\n",
    "Question\n",
    "\n",
    "The test that you wrote was written to the test class TestPreprocess in the test module data/test_preprocessing_helpers.py. The same test module also contains the test class TestConvertToInt.\n",
    "\n",
    "Run the tests in TestPreprocess and TestConvertToInt. Based on the test result report, which of the following is correct?\n",
    "Possible answers:\n",
    "    \n",
    "    The tests for both convert_to_int() and preprocess() passes.\n",
    "    \n",
    "    The tests for convert_to_int() passes but the test for preprocess() fails.\n",
    "    \n",
    "    Some tests for convert_to_int() fail but the test for preprocess() passes.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Wow! You are turning into a seasoned tester! The results tell us that preprocess() is defined correctly and is bug-free. But one of its dependencies convert_to_int() has bugs. This kind of precise result is only possible using mocking.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! When testing complex models, it is an art to figure out special testing sets where you can assert some equalities or inequalities. Remember, the more complex the model, the higher the chance that there is a bug. It's equally difficult but absolutely imperative to test them.\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Testing on linear data\n",
    "\n",
    "The model_test() function, which measures how well the model fits unseen data, returns a quantity called\n",
    "which is very difficult to compute in the general case. Therefore, you need to find special testing sets where computing\n",
    "\n",
    "is easy.\n",
    "\n",
    "One important special case is when the model fits the testing set perfectly. This happens when the testing set is perfectly linear. One such testing set is printed out in the IPython console for you.\n",
    "\n",
    "In this special case, model_test() should return 1.0 if the model's slope and intercept match the testing set, because 1.0 is usually the highest possible value that\n",
    "\n",
    "can take.\n",
    "\n",
    "Remember that for data points (X_n, Y_n), the slope is (Y_2-Y_1)/(X_2-X_1) and the intercept is Y_1 - slope * X_1.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Assign the variable test_argument to a NumPy array holding the perfectly linear testing data printed out in the IPython console.\n",
    "Assign the variable expected to the expected value of\n",
    "in the special case of a perfect fit.\n",
    "Fill in with the model's slope and intercept that matches the testing set.\n",
    "Remembering that actual is a float, complete the assert statement to check if actual returned by model_test() is equal to the expected return value expected.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from train import model_test\n",
    "\n",
    "def test_on_perfect_fit():\n",
    "    # Assign to a NumPy array containing a linear testing set\n",
    "    test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "    # Fill in with the expected value of r^2 in the case of perfect fit\n",
    "    expected = 1\n",
    "    # Fill in with the slope and intercept of the model\n",
    "    actual = model_test(test_argument, slope=2, intercept=1)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! When testing complex models, it is an art to figure out special testing sets where you can assert some equalities or inequalities. Remember, the more complex the model, the higher the chance that there is a bug. It's equally difficult but absolutely imperative to test them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Testing on circular data\n",
    "\n",
    "Another special case where it is easy to guess the value of\n",
    "is when the model does not fit the testing dataset at all. In this case,R²\n",
    "\n",
    "takes its lowest possible value 0.0.\n",
    "\n",
    "The plot shows such a testing dataset and model. The testing dataset consists of data arranged in a circle of radius 1.0. The x and y co-ordinates of the data is shown on the plot. The model corresponds to a straight line y=0.\n",
    "\n",
    "As one can easily see, the straight line does not fit the data at all. In this particular case, the value of R²is known to be 0.0.\n",
    "\n",
    "Your job is to write a test test_on_circular_data() for the function model_test() that performs this sanity check. pytest, numpy as np, model_test, sin, cos and pi have been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "    Assign test_argument to a NumPy array holding the circular testing data shown in the plot, starting with (1.0, 0.0) and moving anticlockwise.\n",
    "---\n",
    "\n",
    "    Fill in with the slope and intercept of the straight line shown in the plot.\n",
    "---\n",
    "    Remembering that model_test() returns a float, complete the assert statement to check if model_test() returns the expected value of R² in this special case.\n",
    "---\n",
    "Question\n",
    "\n",
    "The tests test_on_perfect_fit() and test_on_circular_data() that you wrote in the last two exercises has been written to the test class TestModelTest in the test module models/test_train.py. Run the test class in the IPython console. What is the outcome?\n",
    "Possible answers:\n",
    "    \n",
    "    The sanity checks are all passing. {Answer}\n",
    "    \n",
    "    The test test_on_circular_data() is failing because model_test() returns 0.52 instead of 0.0 for circular testing data.\n",
    "    \n",
    "    The test test_on_perfect_fit() is failing because model_test() returns 0.0 instead of 1.0 for linear data.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Assign to a NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                             )\n",
    "    # Fill in with the slope and intercept of the straight line\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(0.0)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That's correct! model_test() seems to pass all the sanity checks. While this function is complicated and cannot be well tested, these sanity checks greatly reduce the chance of having a bug in it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Generate the baseline image\n",
    "\n",
    "In this exercise, you will get one step closer to the real thing. During this whole course, you've built a library of tests using a Python script and an IPython console. In real life, you're more likely to use an IDE (Integrated Development Environment), that lets you write scripts in the language you want, organize them into your directories, and execute shell commands. Basically, an IDE increases your productivity by gathering the most common activities of software development into a single application: writing source code, executing, and debugging.\n",
    "\n",
    "Here, you can see the directory you've built on the left pane. The upper right pane is where you will write your Python scripts, and the bottom right pane is a shell console, which replaces the IPython console you've used so far.\n",
    "\n",
    "Parts of an integrated development environment\n",
    "\n",
    "In this exercise, you will test the function introduced in the video get_plot_for_best_fit_line() on another set of test arguments. Here is the test data.\n",
    "\n",
    "1.0    3.0\n",
    "2.0    8.0\n",
    "3.0    11.0\n",
    "\n",
    "The best fit line that the test will draw follows the equation\n",
    "\n",
    ". Two points, (1.0, 3.0) and (2.0, 8.0) will fall on the line. The point (3.0, 11.0) won't. The title of the plot will be \"Test plot for almost linear data\".\n",
    "\n",
    "The test is called test_plot_for_almost_linear_data() and it's your job to complete the test and generate the baseline image. pytest, numpy as np and get_plot_for_best_fit_line has been imported for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    Add the correct pytest marker that helps in generating baselines and comparing images.\n",
    "    \n",
    "    Return the matplotlib figure returned by the function under test.\n",
    "    \n",
    "    The test that you wrote was written to a test class TestGetPlotForBestFitLine in the test module visualization/test_plots.py. In the shell console, execute the command required to create the baseline image for this test only. The baseline folder should be in project/tests/visualization. Because it's a shell console and not an IPython one, you don't need to use the ! at the beginning of your command. Once you've ran your command and created your baseline, click Submit Answer.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from visualization.plots import get_plot_for_best_fit_line\n",
    "\n",
    "class TestGetPlotForBestFitLine(object):\n",
    "    # Add the pytest marker which generates baselines and compares images\n",
    "    @pytest.mark.mpl_image_compare\n",
    "    def test_plot_for_almost_linear_data(self):\n",
    "        slope = 5.0\n",
    "        intercept = -2.0\n",
    "        x_array = np.array([1.0, 2.0, 3.0])\n",
    "        y_array = np.array([3.0, 8.0, 11.0])\n",
    "        title = \"Test plot for almost linear data\"\n",
    "        # Return the matplotlib figure returned by the function under test\n",
    "        return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Shell command to create baseline image\n",
    "\"\"\"\n",
    "pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\"\n",
    "\"\"\"\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job completing the function and running the command required! After creating the baseline image, you should inspect it manually and verify if it looks as expected. Then you can then run the test against it, which you will do in the next exercise!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGood job! Unfortunately, your colleague has done something horrible and both tests are failing. In the test result report, you probably saw that pytest saved the actual plot, the baseline and the pixelwise difference to a temporary folder. In the next exercise, you will look at these images and try to fix the wreck that your colleague has caused.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Run the tests for the plotting function\n",
    "\n",
    "Shortly after the baseline image was generated, one of your colleagues modified the plotting function. You have to run the tests in order to check whether the function still works as expected.\n",
    "\n",
    "Remember the following:\n",
    "\n",
    "    The tests were housed in a test class TestGetPlotForBestFitLine in the test module visualization/test_plots.py. You can specify this test class in the pytest command by either using its node ID or the -k command line flag.\n",
    "    To ensure plots are compared to the baseline during testing, the pytest command must include a special command line flag that comes from the pytest-mpl package.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Run the tests in this test class in the console. Because it's a shell console and not an IPython one, you don't need to use the ! at the beginning of your command. You should see two failures.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\"\"\"\n",
    "pytest -k\"TestGetPlotForBestFitLine\" --mpl\n",
    "\"\"\"\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! Unfortunately, your colleague has done something horrible and both tests are failing. In the test result report, you probably saw that pytest saved the actual plot, the baseline and the pixelwise difference to a temporary folder. In the next exercise, you will look at these images and try to fix the wreck that your colleague has caused.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThat is amazing! The linear regression project is now well-tested with 25 tests and they are all passing, thanks to your efforts throughout the course. With the skills and techniques you learned here, you can now go and test your own data science projects!\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Fix the plotting function\n",
    "\n",
    "In the last exercise, pytest saved the baseline images, actual images, and images containing the pixelwise difference in a temporary folder. The difference image for one of the tests test_on_almost_linear_data() is shown below.\n",
    "\n",
    "The black areas are where the actual image and the baseline matches. The white areas are where they don't match.\n",
    "\n",
    "This clearly tells us that something is wrong with the axis labels. Take a look at the plots section to see the baseline (plot 1/2) and the actual plot (plot 2/2). Based on that, it's your job to fix the plotting function.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fill in the axis labels xlabel and ylabel so that they match the baseline plot (plot 1/2).\n",
    "---\n",
    "Question\n",
    "\n",
    "Now that you have fixed the function, run all the tests in the tests directory, remembering that the current working directory in the IPython console is tests. What is the outcome?\n",
    "Possible answers:\n",
    "    \n",
    "    All 25 tests pass. {Answer}\n",
    "    \n",
    "    22 tests are passing and 3 are failing.\n",
    "    \n",
    "    24 tests are passing and 1 is failing.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_array, y_array, \".\")\n",
    "    ax.plot([0, np.max(x_array)], [intercept, slope * np.max(x_array) + intercept], \"-\")\n",
    "    # Fill in with axis labels so that they match the baseline\n",
    "    ax.set(ylabel=\"price (dollars)\", xlabel=\"area (square feet)\", title=title)\n",
    "    return fig\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "That is amazing! The linear regression project is now well-tested with 25 tests and they are all passing, thanks to your efforts throughout the course. With the skills and techniques you learned here, you can now go and test your own data science projects!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
