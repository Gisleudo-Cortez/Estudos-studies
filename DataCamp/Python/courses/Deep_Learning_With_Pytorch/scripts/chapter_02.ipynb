{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_layer = torch.tensor([[ 0.0401, -0.9005,  0.0397, -0.0876]])\n",
    "\n",
    "weight_1 = torch.tensor([[-0.1094, -0.8285,  0.0416, -1.1222],[ 0.3327, -0.0461,  1.4473, -0.8070],[ 0.0681, -0.7058, -1.8017,  0.5857],[ 0.8764,  0.9618, -0.4505,  0.2888]])\n",
    "\n",
    "weight_2 = torch.tensor([[ 0.6856, -1.7650,  1.6375, -1.5759],\n",
    "        [-0.1092, -0.1620,  0.1951, -0.1169],\n",
    "        [-0.5120,  1.1997,  0.8483, -0.2476],\n",
    "        [-0.3369,  0.5617, -0.6658,  0.2221]])\n",
    "\n",
    "weight_3 = torch.tensor([[ 0.8824,  0.1268,  1.1951,  1.3061],\n",
    "        [-0.8753, -0.3277, -0.1454, -0.0167],\n",
    "        [ 0.3582,  0.3254, -1.8509, -1.4205],\n",
    "        [ 0.3786,  0.5999, -0.5665, -0.3975]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n",
      "tensor([[0.2653, 0.1311, 3.8219, 3.0032]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGood job! You see how the results are the same.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Neural networks\n",
    "\n",
    "Let us see the differences between neural networks which apply ReLU and those which do not apply ReLU. We have already initialized the input called input_layer, and three sets of weights, called weight_1, weight_2 and weight_3.\n",
    "\n",
    "We are going to convince ourselves that networks with multiple layers which do not contain non-linearity can be expressed as neural networks with one layer.\n",
    "\n",
    "The network and the shape of layers and weights is shown below.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Calculate the first and second hidden layer by multiplying the appropriate inputs with the corresponding weights.\n",
    "    Calculate and print the results of the output.\n",
    "    Set weight_composed_1 to the product of weight_1 with weight_2, then set weight to the product of weight_composed_1 with weight_3.\n",
    "    Calculate and print the output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Calculate the first and second hidden layer\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
    "\n",
    "# Calculate the output\n",
    "print(torch.matmul(hidden_2, weight_3))\n",
    "\n",
    "# Calculate weight_composed_1 and weight\n",
    "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
    "weight = torch.matmul(weight_composed_1, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! You see how the results are the same.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2770, -0.0345, -0.1410, -0.0664]])\n",
      "tensor([[-0.2117, -0.4782,  4.0438,  3.0417]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAwesome! As expected, the results are different from the previous exercise.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "ReLU activation\n",
    "\n",
    "In this exercise, we have the same settings as the previous exercise. But now we are going to build a neural network which has non-linearity. By doing so, we are going to convince ourselves that networks with multiple layers and non-linearity functions cannot be expressed as a neural network with one layer.\n",
    "\n",
    "We have already instantiated the ReLU activation function called relu() for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Apply the non-linearity to the two hidden layers and print the result.\n",
    "    Apply the non-linearity to the product of first two weights.\n",
    "    Multiply the result of the previous step with weight_3.\n",
    "    Multiply input_layer with weight and print the results.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate non-linearity\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Apply non-linearity on the hidden layers\n",
    "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
    "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
    "print(torch.matmul(hidden_2_activated, weight_3))\n",
    "\n",
    "# Apply non-linearity in the product of first two weights. \n",
    "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
    "\n",
    "# Multiply `weight_composed_1_activated` with `weight_3\n",
    "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! As expected, the results are different from the previous exercise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! You can now build neural networks with different number of neurons on each layer.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "ReLU activation again\n",
    "\n",
    "Neural networks don't need to have the same number of units in each layer. Here, you are going to experiment with the ReLU activation function again, but this time we are going to have a different number of units in the layers of the neural network. The input layer will still have 4 features, but then the first hidden layer will have 6 units and the output layer will have 2 units.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Instantiate the ReLU() activation function as relu (the function is part of nn module).\n",
    "    Initialize weight_1 and weight_2 with random numbers.\n",
    "    Multiply the input_layer with weight_1, storing results in hidden_1.\n",
    "    Apply the relu activation function over hidden_1, and then multiply the output of it with weight_2.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Instantiate ReLU activation function as relu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Initialize weight_1 and weight_2 with random numbers\n",
    "weight_1 = torch.rand(4, 6)\n",
    "weight_2 = torch.rand(6, 2)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
    "hidden_1_activated = relu(hidden_1)\n",
    "print(torch.matmul(hidden_1_activated, weight_2))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! You can now build neural networks with different number of neurons on each layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 0.01168918061661097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations, you computed correctly the softmax cross-entropy loss. You rock!\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Calculating loss function by hand\n",
    "\n",
    "Let's start the exercises by calculating the loss function by hand. Don't do this exercise in PyTorch, it is important to first do it using only pen and paper (and a calculator).\n",
    "\n",
    "We have the same example as before but now our object is actually a frog, and the predicted scores are -1.2 for class 0 (cat), 0.12 for class 1 (car) and 4.8 for class 2 (frog).\n",
    "\n",
    "What is the result of the softmax cross-entropy loss function?\n",
    "Class \tPredicted Score\n",
    "Cat \t-1.2\n",
    "Car \t0.12\n",
    "Frog \t4.8\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Possible Answers\n",
    "\n",
    "    6.0117\n",
    "\n",
    "    4.6917\n",
    "\n",
    "    0.0117\n",
    "\n",
    "    Score for frog is high, so loss is 0.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import numpy as np\n",
    "\n",
    "# Calculate normalized softmax\n",
    "def softmax_std(f):\n",
    "    f = f - np.max(f)\n",
    "    prob = np.exp(f) / np.sum(np.exp(f))\n",
    "    return prob\n",
    "\n",
    "# Calculate cross-entropy loss\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "\n",
    "    # Calcualte the softmax array\n",
    "    soft = softmax_std(y_pred)\n",
    "    # Initialize the loss value\n",
    "    loss = 0\n",
    "\n",
    "    # The loss for each class is calculated by the formula Loss = -Y_true * log(Y_pred)\n",
    "    # The step -1 * y_true[i] adjusts the -Y_true to the  correct value in the array\n",
    "    # The total loss is the sum of all the losses\n",
    "    for i in range(len(soft)):\n",
    "        # Calculate and add the loss value for each prediction\n",
    "        loss = loss + (-1 * y_true[i] * np.log(soft[i]))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "true_pred = [0,0,1] # Cat, Car, Frog\n",
    "pred_array = np.array([-1.2, 0.12, 4.8])\n",
    "\n",
    "loss = cross_entropy_loss(pred_array, true_pred)\n",
    "print(f'The loss is {loss}')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations, you computed correctly the softmax cross-entropy loss. You rock!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAs you can see, the loss function PyTorch calculated gives the same number as the loss function you calculated. Being proficient in understanding and calculating loss functions is a very important skill in deep learning.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Calculating loss function in PyTorch\n",
    "\n",
    "You are going to code the previous exercise, and make sure that we computed the loss correctly. Predicted scores are -1.2 for class 0 (cat), 0.12 for class 1 (car) and 4.8 for class 2 (frog). The ground truth is class 2 (frog). Compute the loss function in PyTorch.\n",
    "Class \tPredicted Score\n",
    "Cat \t-1.2\n",
    "Car \t0.12\n",
    "Frog \t4.8\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Initialize the tensor of scores with numbers [[-1.2, 0.12, 4.8]], and the tensor of ground truth [2].\n",
    "    Instantiate the cross-entropy loss and call it criterion.\n",
    "    Compute and print the loss.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Initialize the scores and ground truth\n",
    "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
    "ground_truth = torch.tensor([2])\n",
    "\n",
    "# Instantiate cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "As you can see, the loss function PyTorch calculated gives the same number as the loss function you calculated. Being proficient in understanding and calculating loss functions is a very important skill in deep learning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6232)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe score is close to -ln(1/1000) = 6.9. This is not surprising, considering that scores were random and close to each other, so the probability for each class was approximately the same (1/1000) = 0.001.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Loss function of random scores\n",
    "\n",
    "If the neural network predicts random scores, what would be its loss function? Let's find it out in PyTorch. The neural network is going to have 1000 classes, each having a random score. For ground truth, it will have class 111. Calculate the loss function.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import torch and torch.nn as nn\n",
    "    Initialize logits with a random tensor of shape (1, 1000) and ground_truth with a tensor containing the number 111.\n",
    "    Instantiate the cross-entropy loss in a variable called criterion.\n",
    "    Calculate and print the loss function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import torch and torch.nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize logits and ground truth\n",
    "logits = torch.rand(1,1000)\n",
    "ground_truth = torch.tensor([111])\n",
    "\n",
    "# Instantiate cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate and print the loss\n",
    "loss = criterion(logits, ground_truth)\n",
    "print(loss)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "The score is close to -ln(1/1000) = 6.9. This is not surprising, considering that scores were random and close to each other, so the probability for each class was approximately the same (1/1000) = 0.001.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:19<00:00, 513307.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/train-images-idx3-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 15922146.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/train-labels-idx1-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:04<00:00, 408130.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/t10k-images-idx3-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1828265.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw/t10k-labels-idx1-ubyte.gz to /run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice! Now you know how to prepare datasets in PyTorch. You are very close to reaching the grand goal, training neural networks!\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Preparing MNIST dataset\n",
    "\n",
    "You are going to prepare dataloaders for MNIST training and testing set. As we explained in the lecture, MNIST has some differences to CIFAR-10, with the main difference being that MNIST images are grayscale (1 channel based) instead of RGB (3 channels).\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Transform the data to torch tensors and normalize it to have mean is 0.1307 and std is 0.3081.\n",
    "    Prepare the trainset and the testset.\n",
    "    Prepare the dataloaders for training and testing so that only 32 pictures are processed at a time and the training data is shuffled each time.\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# solution\n",
    "\n",
    "# Transform the data to torch tensors and normalize it \n",
    "# The Normalize recives the mean and std for each channel\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\ttransforms.Normalize((0.1307), ((0.3081)))])\n",
    "\n",
    "# Prepare training set and testing set\n",
    "trainset = torchvision.datasets.MNIST('/run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/', train=True, \n",
    "\t\t\t\t\t\t\t\t\t  download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST('/run/media/nero/e753e46e-c457-4237-9670-c71cee45c7bc/Datasets/torchvision/', train=False, \n",
    "\t\t\t\t\t\t\t\t\t  download=True, transform=transform)\n",
    "\n",
    "# Prepare training loader and testing loader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "\t\t\t\t\t\t\t\t\t\t shuffle=False, num_workers=4) \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice! Now you know how to prepare datasets in PyTorch. You are very close to reaching the grand goal, training neural networks!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n",
      "32 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAfter you have armed yourself with the knowledge of building and exploring dataloaders, it is time to start training neural networks!\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Inspecting the dataloaders\n",
    "\n",
    "Now you are going to explore a bit the dataloaders you created in the previous exercise. In particular, you will compute the shape of the dataset in addition to the minibatch size.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Find the shapes of the trainset and testset.\n",
    "    Print the computed values.\n",
    "    Find the size of the minibatch for both trainset and testset.\n",
    "    Print the minibatch size.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute the shape of the training set and testing set\n",
    "trainset_shape = trainloader.dataset.data.shape\n",
    "testset_shape = testloader.dataset.data.shape\n",
    "\n",
    "# Print the computed shapes\n",
    "print(trainset_shape, testset_shape)\n",
    "\n",
    "# Compute the size of the minibatch for training set and testing set\n",
    "trainset_batchsize = trainloader.batch_size\n",
    "testset_batchsize =testloader.batch_size\n",
    "\n",
    "# Print sizes of the minibatch\n",
    "print(trainset_batchsize, testset_batchsize)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "After you have armed yourself with the knowledge of building and exploring dataloaders, it is time to start training neural networks!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGreat! You have not forgotten how to build neural networks.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Building a neural network - again\n",
    "\n",
    "You haven't created a neural network since the end of the first chapter, so this is a good time to build one (practice makes perfect). Build a class for a neural network which will be used to train on the MNIST dataset. The dataset contains images of shape (28, 28, 1), so you should deduct the size of the input layer. For hidden layer use 200 units, while for output layer use 10 units (1 for each class). For activation function, use relu in a functional way (nn.Functional is already imported as F).\n",
    "\n",
    "For context, the same net will be trained and used to make predictions in the next two exercises.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Define the class called Net which inherits from nn.Module.\n",
    "    In the __init__() method, define the parameters for the two fully connected layers.\n",
    "    In the .forward() method, do the forward step.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the class Net\n",
    "class Net(nn.Module) :\n",
    "    def __init__(self):    \n",
    "    \t# Define all the parameters of the net\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):   \n",
    "    \t# Do the forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! You have not forgotten how to build neural networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nero/Documents/Estudos/estudos/lib/python3.11/site-packages/torch/autograd/__init__.py:200: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFantastic! Now you can train neural networks.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Training a neural network\n",
    "\n",
    "Given the fully connected neural network (called model) which you built in the previous exercise and a train loader called train_loader containing the MNIST dataset (which we created for you), you're to train the net in order to predict the classes of digits. You will use the Adam optimizer to optimize the network, and considering that this is a classification problem you are going to use cross entropy as loss function.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Instantiate the Adam optimizer with learning rate 3e-4 and instantiate Cross-Entropy as loss function.\n",
    "    Complete a forward pass on the neural network using the input data.\n",
    "    Using backpropagation, compute the gradients of the weights, and then change the weights using the Adam optimizer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
    "model = Net()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "for batch_idx, data_target in enumerate(trainloader):\n",
    "    data = data_target[0]\n",
    "    target = data_target[1]\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Complete a forward pass\n",
    "    output = model(data)\n",
    "\n",
    "    # Compute the loss, gradients and change the weights\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fantastic! Now you can train neural networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing set accuracy of the network is: 95 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCongratulations, you are able to use neural networks to make predictions. NB: The accuracy of the net is too low compared to what we can do with neural networks. We used only a subpart of the dataset in order for the training to happen fast. You can achieve a much higher accuracy by using the entire dataset, and by using a larger neural network.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Using the network to make predictions\n",
    "\n",
    "Now that you have trained the network, use it to make predictions for the data in the testing set. The network is called model (same as in the previous exercise), and the loader is called test_loader. We have already initialized variables total and correct to 0.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Set the network in testing (eval) mode.\n",
    "    Put each image into a vector using inputs.view(-1, number_of_features) where the number of features should be deducted by multiplying spatial dimensions (shape) of the image.\n",
    "    Do the forward pass and put the predictions in output variable.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# Set the model in eval mode\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # Put each image into a vector\n",
    "    inputs = inputs.view(-1, 28*28*1)\n",
    "    \n",
    "    # Do the forward pass and get the predictions\n",
    "    outputs = model(inputs)\n",
    "    _, outputs = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (outputs == labels).sum().item()\n",
    "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations, you are able to use neural networks to make predictions. NB: The accuracy of the net is too low compared to what we can do with neural networks. We used only a subpart of the dataset in order for the training to happen fast. You can achieve a much higher accuracy by using the entire dataset, and by using a larger neural network.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
