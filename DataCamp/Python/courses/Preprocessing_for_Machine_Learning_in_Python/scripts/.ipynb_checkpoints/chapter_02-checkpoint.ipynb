{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:21:59.191148Z",
     "iopub.status.busy": "2023-05-26T11:21:59.190682Z",
     "iopub.status.idle": "2023-05-26T11:21:59.195670Z",
     "shell.execute_reply": "2023-05-26T11:21:59.194880Z",
     "shell.execute_reply.started": "2023-05-26T11:21:59.191108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Preprocessing_for_Machine_Learning_in_Python/datasets/'\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:21:59.695023Z",
     "iopub.status.busy": "2023-05-26T11:21:59.694751Z",
     "iopub.status.idle": "2023-05-26T11:21:59.708060Z",
     "shell.execute_reply": "2023-05-26T11:21:59.706525Z",
     "shell.execute_reply.started": "2023-05-26T11:21:59.694999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load wine data and knn module\n",
    "data_path = path_data + 'wine_types.csv'\n",
    "wine = pd.read_csv(data_path)\n",
    "\n",
    "X = wine.drop('type', axis = 1)\n",
    "y = wine['type']\n",
    "\n",
    "# import knn and train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:22:00.164681Z",
     "iopub.status.busy": "2023-05-26T11:22:00.164270Z",
     "iopub.status.idle": "2023-05-26T11:22:00.173425Z",
     "shell.execute_reply": "2023-05-26T11:22:00.171967Z",
     "shell.execute_reply.started": "2023-05-26T11:22:00.164621Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.dropna(inplace = True)\n",
    "y.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:22:00.659384Z",
     "iopub.status.busy": "2023-05-26T11:22:00.658826Z",
     "iopub.status.idle": "2023-05-26T11:22:00.684263Z",
     "shell.execute_reply": "2023-05-26T11:22:00.682779Z",
     "shell.execute_reply.started": "2023-05-26T11:22:00.659356Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat work. You can see that the accuracy score is pretty low. Let's explore methods to improve this score.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Modeling without normalizing\n",
    "\n",
    "Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first.\n",
    "\n",
    "Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n",
    "\n",
    "The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split up the X and y sets into training and test sets, ensuring that class labels are equally distributed in both sets.\n",
    "    Fit the knn model to the training features and labels.\n",
    "    Print the test set accuracy of the knn model using the .score() method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the knn model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work. You can see that the accuracy score is pretty low. Let's explore methods to improve this score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:22:01.426020Z",
     "iopub.status.busy": "2023-05-26T11:22:01.425604Z",
     "iopub.status.idle": "2023-05-26T11:22:01.432436Z",
     "shell.execute_reply": "2023-05-26T11:22:01.430701Z",
     "shell.execute_reply.started": "2023-05-26T11:22:01.425985Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:22:01.908104Z",
     "iopub.status.busy": "2023-05-26T11:22:01.907597Z",
     "iopub.status.idle": "2023-05-26T11:22:01.924686Z",
     "shell.execute_reply": "2023-05-26T11:22:01.923817Z",
     "shell.execute_reply.started": "2023-05-26T11:22:01.908059Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99166.71735542436\n",
      "0.17231366191842012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice work! The np.log() function is an easy way to log normalize a column.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Log normalization in Python\n",
    "\n",
    "Now that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.\n",
    "\n",
    "numpy has been imported as np.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print out the variance of the Proline column for reference.\n",
    "    Use the np.log() function on the Proline column to create a new, log-normalized column named Proline_log.\n",
    "    Print out the variance of the Proline_log column to see the difference.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine['Proline_log'].var())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! The np.log() function is an easy way to log normalize a column.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:23:46.660068Z",
     "iopub.status.busy": "2023-05-26T11:23:46.659191Z",
     "iopub.status.idle": "2023-05-26T11:23:46.674198Z",
     "shell.execute_reply": "2023-05-26T11:23:46.673534Z",
     "shell.execute_reply.started": "2023-05-26T11:23:46.660032Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGood job! In scikit-learn, running .fit_transform() during preprocessing will both fit the method to the data as well as transform the data in a single step.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Scaling data - standardizing columns\n",
    "\n",
    "Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the StandardScaler class.\n",
    "    Instantiate a StandardScaler() and store it in the variable, scaler.\n",
    "    Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset.\n",
    "    Fit and transform the standard scaler to wine_subset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Subset the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash','Alcalinity of ash','Magnesium']]\n",
    "\n",
    "# Apply the scaler to wine_subset\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! In scikit-learn, running .fit_transform() during preprocessing will both fit the method to the data as well as transform the data in a single step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:27:53.155676Z",
     "iopub.status.busy": "2023-05-26T11:27:53.154137Z",
     "iopub.status.idle": "2023-05-26T11:27:53.182327Z",
     "shell.execute_reply": "2023-05-26T11:27:53.181612Z",
     "shell.execute_reply.started": "2023-05-26T11:27:53.155601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! This accuracy definitely isn't poor, but let's see if we can improve it by standardizing the data.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "KNN on non-scaled data\n",
    "\n",
    "Before adding standardization to your scikit-learn workflow, you'll first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.\n",
    "\n",
    "The knn model as well as the X and y data and labels sets have been created already.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split the dataset into training and test sets.\n",
    "    Fit the knn model to the training data.\n",
    "    Print out the test set accuracy of your trained knn model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! This accuracy definitely isn't poor, but let's see if we can improve it by standardizing the data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T11:29:53.721206Z",
     "iopub.status.busy": "2023-05-26T11:29:53.720439Z",
     "iopub.status.idle": "2023-05-26T11:29:53.748167Z",
     "shell.execute_reply": "2023-05-26T11:29:53.747430Z",
     "shell.execute_reply.started": "2023-05-26T11:29:53.721141Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent! That's quite the improvement, and definitely made scaling the data worthwhile.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "KNN on scaled data\n",
    "\n",
    "The accuracy score on the unscaled wine dataset was decent, but let's see what you can achieve by using standardization. Once again, the knn model as well as the X and y data and labels set have already been created for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create the StandardScaler() method, stored in a variable named scaler.\n",
    "    Scale the training and test features, being careful not to introduce data leakage.\n",
    "    Fit the knn model to the scaled training data.\n",
    "    Evaluate the model's performance by computing the test set accuracy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Instantiate a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test_scaled, y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent! That's quite the improvement, and definitely made scaling the data worthwhile.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
