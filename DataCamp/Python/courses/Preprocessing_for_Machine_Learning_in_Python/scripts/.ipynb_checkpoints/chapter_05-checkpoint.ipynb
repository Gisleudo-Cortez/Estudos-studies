{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:52.418222Z",
     "iopub.status.busy": "2023-05-26T18:04:52.418051Z",
     "iopub.status.idle": "2023-05-26T18:04:53.446172Z",
     "shell.execute_reply": "2023-05-26T18:04:53.445663Z",
     "shell.execute_reply.started": "2023-05-26T18:04:52.418204Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Preprocessing_for_Machine_Learning_in_Python/datasets/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:53.450450Z",
     "iopub.status.busy": "2023-05-26T18:04:53.450253Z",
     "iopub.status.idle": "2023-05-26T18:04:53.471462Z",
     "shell.execute_reply": "2023-05-26T18:04:53.470877Z",
     "shell.execute_reply.started": "2023-05-26T18:04:53.450433Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load ufo data\n",
    "ufo = pd.read_csv(path_data + 'ufo_sightings_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:53.475993Z",
     "iopub.status.busy": "2023-05-26T18:04:53.475084Z",
     "iopub.status.idle": "2023-05-26T18:04:53.993482Z",
     "shell.execute_reply": "2023-05-26T18:04:53.991796Z",
     "shell.execute_reply.started": "2023-05-26T18:04:53.475900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            4935 non-null   object \n",
      " 1   city            4926 non-null   object \n",
      " 2   state           4516 non-null   object \n",
      " 3   country         4255 non-null   object \n",
      " 4   type            4776 non-null   object \n",
      " 5   seconds         4935 non-null   float64\n",
      " 6   length_of_time  4792 non-null   object \n",
      " 7   desc            4932 non-null   object \n",
      " 8   recorded        4935 non-null   object \n",
      " 9   lat             4935 non-null   object \n",
      " 10  long            4935 non-null   float64\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 424.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            4935 non-null   datetime64[ns]\n",
      " 1   city            4926 non-null   object        \n",
      " 2   state           4516 non-null   object        \n",
      " 3   country         4255 non-null   object        \n",
      " 4   type            4776 non-null   object        \n",
      " 5   seconds         4935 non-null   float64       \n",
      " 6   length_of_time  4792 non-null   object        \n",
      " 7   desc            4932 non-null   object        \n",
      " 8   recorded        4935 non-null   object        \n",
      " 9   lat             4935 non-null   object        \n",
      " 10  long            4935 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(8)\n",
      "memory usage: 424.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice job on transforming the column types! This will make feature engineering and standardization much easier.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Checking column types\n",
    "\n",
    "Take a look at the UFO dataset's column types using the .info() method. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object, and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Call the .info() method on the ufo dataset.\n",
    "    Convert the type of the seconds column to the float data type.\n",
    "    Convert the type of the date column to the datetime data type.\n",
    "    Call .info() on ufo again to see if the changes worked.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Print the DataFrame info\n",
    "print(ufo.info())\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo['seconds'].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo['date'])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo.info())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job on transforming the column types! This will make feature engineering and standardization much easier.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:54.002424Z",
     "iopub.status.busy": "2023-05-26T18:04:54.001107Z",
     "iopub.status.idle": "2023-05-26T18:04:54.316917Z",
     "shell.execute_reply": "2023-05-26T18:04:54.313701Z",
     "shell.execute_reply.started": "2023-05-26T18:04:54.002372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "(3891, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAwesome! We'll work with this set going forward\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Dropping missing data\n",
    "\n",
    "In this exercise, you'll remove some of the rows where certain columns have missing values. You're going to look at the length_of_time column, the state column, and the type column. You'll drop any row that contains a missing value in at least one of these three columns.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print out the number of missing values in the length_of_time, state, and type columns, in that order, using .isna() and .sum().\n",
    "    Drop rows that have missing values in at least one of these columns.\n",
    "    Print out the shape of the new ufo_no_missing dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[['length_of_time', 'state', 'type']].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo.dropna()\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! We'll work with this set going forward\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:54.333847Z",
     "iopub.status.busy": "2023-05-26T18:04:54.332053Z",
     "iopub.status.idle": "2023-05-26T18:04:54.636670Z",
     "shell.execute_reply": "2023-05-26T18:04:54.634615Z",
     "shell.execute_reply.started": "2023-05-26T18:04:54.333767Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ufo = ufo_no_missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:54.649133Z",
     "iopub.status.busy": "2023-05-26T18:04:54.648393Z",
     "iopub.status.idle": "2023-05-26T18:04:55.143706Z",
     "shell.execute_reply": "2023-05-26T18:04:55.141385Z",
     "shell.execute_reply.started": "2023-05-26T18:04:54.649063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix ufo data frame due to time_length beig diferent from the exercises\n",
    "\n",
    "ufo = ufo[ufo['length_of_time'].str.contains(r'minutes', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:55.153939Z",
     "iopub.status.busy": "2023-05-26T18:04:55.153201Z",
     "iopub.status.idle": "2023-05-26T18:04:55.549207Z",
     "shell.execute_reply": "2023-05-26T18:04:55.548032Z",
     "shell.execute_reply.started": "2023-05-26T18:04:55.153872Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     length_of_time  minutes\n",
      "3   about 5 minutes      5.0\n",
      "5        10 minutes     10.0\n",
      "8         2 minutes      2.0\n",
      "9         2 minutes      2.0\n",
      "10        5 minutes      5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1258500/1415427063.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ufo.loc[:, \"minutes\"] = ufo.loc[: , \"length_of_time\"].apply(return_minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice job! The minutes information is now in a form where it can be inputted into a model.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Extracting numbers from strings\n",
    "\n",
    "The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from that text field using regular expressions.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Search time_string for numbers using an appropriate RegEx pattern.\n",
    "    Use the .apply() method to call the return_minutes() on every row of the length_of_time column.\n",
    "    Print out the .head() of both the length_of_time and minutes columns to compare.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def return_minutes(time_string):\n",
    "\n",
    "    # Search for numbers in time_string\n",
    "    num = re.search('\\d+', time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "\n",
    "\n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo.loc[:, \"minutes\"] = ufo.loc[: , \"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[['length_of_time','minutes']].head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job! The minutes information is now in a form where it can be inputted into a model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:55.557621Z",
     "iopub.status.busy": "2023-05-26T18:04:55.556371Z",
     "iopub.status.idle": "2023-05-26T18:04:55.928354Z",
     "shell.execute_reply": "2023-05-26T18:04:55.925819Z",
     "shell.execute_reply.started": "2023-05-26T18:04:55.557510Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1258500/2526033063.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ufo.dropna(inplace = True)\n"
     ]
    }
   ],
   "source": [
    "ufo.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:55.940737Z",
     "iopub.status.busy": "2023-05-26T18:04:55.939973Z",
     "iopub.status.idle": "2023-05-26T18:04:56.326996Z",
     "shell.execute_reply": "2023-05-26T18:04:56.324935Z",
     "shell.execute_reply.started": "2023-05-26T18:04:55.940668Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    438825.908015\n",
      "minutes       119.601130\n",
      "dtype: float64\n",
      "0.8343492294526914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1258500/917144917.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ufo.loc[: , \"seconds_log\"] = np.log(ufo['seconds'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood work! Now it's time to engineer new features in the ufo dataset.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Identifying features for standardization\n",
    "\n",
    "In this exercise, you'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the seconds and minutes column, you'll see that the variance of the seconds column is extremely high. Because seconds and minutes are related to each other (an issue we'll deal with when we select features for modeling), let's log normalize the seconds column.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Calculate the variance in the seconds and minutes columns and take a close look at the results.\n",
    "    Perform log normalization on the seconds column, transforming it into a new column named seconds_log.\n",
    "    Print out the variance of the seconds_log column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[['seconds','minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo.loc[: , \"seconds_log\"] = np.log(ufo['seconds'])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo[\"seconds_log\"].var())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good work! Now it's time to engineer new features in the ufo dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:56.337273Z",
     "iopub.status.busy": "2023-05-26T18:04:56.336018Z",
     "iopub.status.idle": "2023-05-26T18:04:56.727440Z",
     "shell.execute_reply": "2023-05-26T18:04:56.726075Z",
     "shell.execute_reply.started": "2023-05-26T18:04:56.337212Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1258500/4032511632.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ufo.loc[: , \"country_enc\"] = ufo.loc[ : , \"country\"].apply(lambda x: 1 if x == 'us' else 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAwesome work! Let's continue on by extracting date components.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Encoding categorical variables\n",
    "\n",
    "There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You'll do that transformation here, using both binary and one-hot encoding methods.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Using apply(), write a conditional lambda function that returns a 1 if the value is \"us\", else return 0.\n",
    "    Print out the number of .unique() values in the type column.\n",
    "    Using pd.get_dummies(), create a one-hot encoded set of the type column.\n",
    "    Finally, use pd.concat() to concatenate the type_set encoded variables to the ufo dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Use pandas to encode us values as 1 and others as 0\n",
    "ufo.loc[: , \"country_enc\"] = ufo.loc[ : , \"country\"].apply(lambda x: 1 if x == 'us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'], dtype=int)\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome work! Let's continue on by extracting date components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:56.732964Z",
     "iopub.status.busy": "2023-05-26T18:04:56.732203Z",
     "iopub.status.idle": "2023-05-26T18:04:57.143621Z",
     "shell.execute_reply": "2023-05-26T18:04:57.139968Z",
     "shell.execute_reply.started": "2023-05-26T18:04:56.732920Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    2002-11-21 05:45:00\n",
      "5    2012-06-16 23:00:00\n",
      "8    2013-06-09 00:00:00\n",
      "9    2013-04-26 23:27:00\n",
      "10   2013-09-13 20:30:00\n",
      "Name: date, dtype: datetime64[ns]\n",
      "                  date  month  year\n",
      "3  2002-11-21 05:45:00     11  2002\n",
      "5  2012-06-16 23:00:00      6  2012\n",
      "8  2013-06-09 00:00:00      6  2013\n",
      "9  2013-04-26 23:27:00      4  2013\n",
      "10 2013-09-13 20:30:00      9  2013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice job on extracting dates! The pandas series attributes .dt.month and .dt.year are extremely useful for extraction tasks.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Features from dates\n",
    "\n",
    "Another feature engineering task to perform is month and year extraction. Perform this task on the date column of the ufo dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print out the .head() of the date column.\n",
    "    Retrieve the month attribute of the date column.\n",
    "    Retrieve the year attribute of the date column.\n",
    "    Take a look at the .head() of the date, month, and year columns.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Look at the first 5 rows of the date column\n",
    "print(ufo['date'].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].dt.month\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].dt.year\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date','month','year']].head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job on extracting dates! The pandas series attributes .dt.month and .dt.year are extremely useful for extraction tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:57.166071Z",
     "iopub.status.busy": "2023-05-26T18:04:57.160941Z",
     "iopub.status.idle": "2023-05-26T18:04:57.641506Z",
     "shell.execute_reply": "2023-05-26T18:04:57.639975Z",
     "shell.execute_reply.started": "2023-05-26T18:04:57.165924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3     It was a large&#44 triangular shaped flying ob...\n",
      "5     Dancing lights that would fly around and then ...\n",
      "8     Brilliant orange light or chinese lantern at o...\n",
      "9     Bright red light moving north to north west fr...\n",
      "10    North-east moving south-west. First 7 or so li...\n",
      "Name: desc, dtype: object\n",
      "(1470, 2949)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat! You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Text vectorization\n",
    "\n",
    "You'll now transform the desc column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print out the .head() of the desc column.\n",
    "    Instantiate a TfidfVectorizer() object.\n",
    "    Fit and transform the desc column using vec.\n",
    "    Print out the .shape of the desc_tfidf vector, to take a look at the number of columns this created.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Take a look at the head of the desc field\n",
    "print(ufo['desc'].head())\n",
    "\n",
    "# Instantiate the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform desc using vec\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns and rows\n",
    "print(desc_tfidf.shape)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:57.650348Z",
     "iopub.status.busy": "2023-05-26T18:04:57.649975Z",
     "iopub.status.idle": "2023-05-26T18:04:57.876629Z",
     "shell.execute_reply": "2023-05-26T18:04:57.873381Z",
     "shell.execute_reply.started": "2023-05-26T18:04:57.650311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating words_to_filter funcion\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:57.920347Z",
     "iopub.status.busy": "2023-05-26T18:04:57.918783Z",
     "iopub.status.idle": "2023-05-26T18:04:58.283750Z",
     "shell.execute_reply": "2023-05-26T18:04:58.280797Z",
     "shell.execute_reply.started": "2023-05-26T18:04:57.920268Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create return_weights\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:58.298003Z",
     "iopub.status.busy": "2023-05-26T18:04:58.295911Z",
     "iopub.status.idle": "2023-05-26T18:04:58.674396Z",
     "shell.execute_reply": "2023-05-26T18:04:58.672183Z",
     "shell.execute_reply.started": "2023-05-26T18:04:58.297887Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating vocab and vector\n",
    "\n",
    "vocab = {v:k for k,v in\n",
    "vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:04:58.686957Z",
     "iopub.status.busy": "2023-05-26T18:04:58.685960Z",
     "iopub.status.idle": "2023-05-26T18:05:00.035547Z",
     "shell.execute_reply": "2023-05-26T18:05:00.035007Z",
     "shell.execute_reply.started": "2023-05-26T18:04:58.686884Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGreat job! You're almost done. In the next exercises, you'll model the UFO data in a couple of different ways.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Selecting the ideal dataset\n",
    "\n",
    "Now to get rid of some of the unnecessary features in the ufo dataset. Because the country column has been encoded as country_enc, you can select it and drop the other columns related to location: city, country, lat, long, and state.\n",
    "\n",
    "You've engineered the month and year columns, so you no longer need the date or recorded columns. You also standardized the seconds column as seconds_log, so you can drop seconds and minutes.\n",
    "\n",
    "You vectorized desc, so it can be removed. For now you'll keep type.\n",
    "\n",
    "You can also get rid of the length_of_time column, which is unnecessary after extracting minutes.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Make a list of all the columns to drop, to_drop.\n",
    "    Drop these columns from ufo.\n",
    "    Use the words_to_filter() function you created previously; pass in vocab, vec.vocabulary_, desc_tfidf, and keep the top 4 words as the last parameter.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Make a list of features to drop\n",
    "to_drop = ['country','city','lat','long','state','date','recorded','seconds','minutes','desc','length_of_time']\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis = 1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! You're almost done. In the next exercises, you'll model the UFO data in a couple of different ways.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:07:01.952469Z",
     "iopub.status.busy": "2023-05-26T18:07:01.950997Z",
     "iopub.status.idle": "2023-05-26T18:07:01.959158Z",
     "shell.execute_reply": "2023-05-26T18:07:01.957300Z",
     "shell.execute_reply.started": "2023-05-26T18:07:01.952412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set ufo = ufo_dropped\n",
    "ufo = ufo_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:09:12.353835Z",
     "iopub.status.busy": "2023-05-26T18:09:12.353472Z",
     "iopub.status.idle": "2023-05-26T18:09:12.359442Z",
     "shell.execute_reply": "2023-05-26T18:09:12.358575Z",
     "shell.execute_reply.started": "2023-05-26T18:09:12.353809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and instanciate model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Create X and y\n",
    "\n",
    "X = ufo.drop(['country_enc','type'], axis = 1)\n",
    "y = ufo['country_enc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:09:55.501372Z",
     "iopub.status.busy": "2023-05-26T18:09:55.500896Z",
     "iopub.status.idle": "2023-05-26T18:09:55.618100Z",
     "shell.execute_reply": "2023-05-26T18:09:55.616633Z",
     "shell.execute_reply.started": "2023-05-26T18:09:55.501333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
      "       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
      "       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
      "       'teardrop', 'triangle', 'unknown', 'month', 'year'],\n",
      "      dtype='object')\n",
      "0.9510869565217391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAwesome work! This model performs pretty well. It seems like you've made pretty good feature selection choices here.\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Modeling the UFO dataset, part 1\n",
    "\n",
    "In this exercise, you're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. The X dataset contains the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The y labels are the encoded country column, where 1 is \"us\" and 0 is \"ca\".\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print out the .columns of the X set.\n",
    "    Split the X and y sets, ensuring that the class distribution of the labels is the same in the training and tests sets, and using a random_state of 42.\n",
    "    Fit knn to the training data.\n",
    "    Print the test set accuracy of the knn model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(X_test,y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome work! This model performs pretty well. It seems like you've made pretty good feature selection choices here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:13:26.854827Z",
     "iopub.status.busy": "2023-05-26T18:13:26.854372Z",
     "iopub.status.idle": "2023-05-26T18:13:26.859349Z",
     "shell.execute_reply": "2023-05-26T18:13:26.858407Z",
     "shell.execute_reply.started": "2023-05-26T18:13:26.854766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instanciate nb\n",
    "\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T18:13:28.795097Z",
     "iopub.status.busy": "2023-05-26T18:13:28.794556Z",
     "iopub.status.idle": "2023-05-26T18:13:28.903345Z",
     "shell.execute_reply": "2023-05-26T18:13:28.902264Z",
     "shell.execute_reply.started": "2023-05-26T18:13:28.795068Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907608695652174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCongrats, you've completed the course! As you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type.\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Modeling the UFO dataset, part 2\n",
    "\n",
    "Finally, you'll build a model using the text vector we created, desc_tfidf, using the filtered_words list to create a filtered text vector. Let's see if you can predict the type of the sighting based on the text. You'll use a Naive Bayes model for this.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Filter the desc_tfidf vector by passing a list of filtered_words into the index.\n",
    "    Split the filtered_text features and y, ensuring an equal class distribution in the training and test sets; use a random_state of 42.\n",
    "    Use the nb model's .fit() to fit X_train and y_train.\n",
    "    Print out the .score() of the nb model on the X_test and y_test sets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify = y, random_state=42)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(X_test, y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congrats, you've completed the course! As you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
