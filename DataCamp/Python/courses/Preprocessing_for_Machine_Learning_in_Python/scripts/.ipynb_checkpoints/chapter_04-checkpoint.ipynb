{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:41:51.304488Z",
     "iopub.status.busy": "2023-05-26T12:41:51.303439Z",
     "iopub.status.idle": "2023-05-26T12:41:51.763032Z",
     "shell.execute_reply": "2023-05-26T12:41:51.762075Z",
     "shell.execute_reply.started": "2023-05-26T12:41:51.304389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Preprocessing_for_Machine_Learning_in_Python/datasets/'\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:48:15.210032Z",
     "iopub.status.busy": "2023-05-26T12:48:15.209545Z",
     "iopub.status.idle": "2023-05-26T12:48:15.222830Z",
     "shell.execute_reply": "2023-05-26T12:48:15.222335Z",
     "shell.execute_reply.started": "2023-05-26T12:48:15.209996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load volunteer data\n",
    "volunteer = pd.read_csv(path_data + 'volunteer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:48:19.681121Z",
     "iopub.status.busy": "2023-05-26T12:48:19.680814Z",
     "iopub.status.idle": "2023-05-26T12:48:19.702793Z",
     "shell.execute_reply": "2023-05-26T12:48:19.701830Z",
     "shell.execute_reply.started": "2023-05-26T12:48:19.681092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  hits   \n",
      "0           1                                       Web designer    22  \\\n",
      "1           2      Urban Adventures - Ice Skating at Lasker Rink    62   \n",
      "2           3  Fight global hunger and support women farmers ...    14   \n",
      "3           4                                      Stop 'N' Swap    31   \n",
      "4           5                               Queens Stop 'N' Swap   135   \n",
      "\n",
      "   postalcode  vol_requests_lognorm  created_month  Education   \n",
      "0     10010.0              0.693147              1          0  \\\n",
      "1     10026.0              2.995732              1          0   \n",
      "2      2114.0              6.214608              1          0   \n",
      "3     10455.0              2.708050              1          0   \n",
      "4     11372.0              2.708050              1          0   \n",
      "\n",
      "   Emergency Preparedness  Environment  Health  Helping Neighbors in Need   \n",
      "0                       0            0       0                          0  \\\n",
      "1                       0            0       0                          0   \n",
      "2                       0            0       0                          0   \n",
      "3                       0            1       0                          0   \n",
      "4                       0            1       0                          0   \n",
      "\n",
      "   Strengthening Communities  \n",
      "0                          1  \n",
      "1                          1  \n",
      "2                          1  \n",
      "3                          0  \n",
      "4                          0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNice job! It's often easier to collect a list of columns to drop, rather than dropping them individually.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Selecting relevant features\n",
    "\n",
    "In this exercise, you'll identify the redundant columns in the volunteer dataset, and perform feature selection on the dataset to return a DataFrame of the relevant features.\n",
    "\n",
    "For example, if you explore the volunteer dataset in the console, you'll see three features which are related to location: locality, region, and postalcode. They contain related information, so it would make sense to keep only one of the features.\n",
    "\n",
    "Take some time to examine the features of volunteer in the console, and try to identify the redundant features.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a list of redundant column names and store it in the to_drop variable:\n",
    "        Out of all the location-related features, keep only postalcode.\n",
    "        Features that have gone through the feature engineering process are redundant as well.\n",
    "    Drop the columns in the to_drop list from the dataset.\n",
    "    Print out the .head() of volunteer_subset to see the selected columns.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"locality\", \"region\", \"vol_requests\", \"category_desc\", \"created_date\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job! It's often easier to collect a list of columns to drop, rather than dropping them individually.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:53:34.068654Z",
     "iopub.status.busy": "2023-05-26T12:53:34.067887Z",
     "iopub.status.idle": "2023-05-26T12:53:34.078657Z",
     "shell.execute_reply": "2023-05-26T12:53:34.077281Z",
     "shell.execute_reply.started": "2023-05-26T12:53:34.068589Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "wine = pd.read_csv(path_data + 'wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:53:35.519311Z",
     "iopub.status.busy": "2023-05-26T12:53:35.518594Z",
     "iopub.status.idle": "2023-05-26T12:53:35.539889Z",
     "shell.execute_reply": "2023-05-26T12:53:35.539072Z",
     "shell.execute_reply.started": "2023-05-26T12:53:35.519268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Unnamed: 0  Flavanoids  Total phenols   \n",
      "Unnamed: 0                      1.000000   -0.758609      -0.650084  \\\n",
      "Flavanoids                     -0.758609    1.000000       0.864564   \n",
      "Total phenols                  -0.650084    0.864564       1.000000   \n",
      "Malic acid                      0.487630   -0.411007      -0.335167   \n",
      "OD280/OD315 of diluted wines   -0.697445    0.787194       0.699949   \n",
      "Hue                            -0.662205    0.543479       0.433681   \n",
      "\n",
      "                              Malic acid  OD280/OD315 of diluted wines   \n",
      "Unnamed: 0                      0.487630                     -0.697445  \\\n",
      "Flavanoids                     -0.411007                      0.787194   \n",
      "Total phenols                  -0.335167                      0.699949   \n",
      "Malic acid                      1.000000                     -0.368710   \n",
      "OD280/OD315 of diluted wines   -0.368710                      1.000000   \n",
      "Hue                            -0.561296                      0.565468   \n",
      "\n",
      "                                   Hue  \n",
      "Unnamed: 0                   -0.662205  \n",
      "Flavanoids                    0.543479  \n",
      "Total phenols                 0.433681  \n",
      "Malic acid                   -0.561296  \n",
      "OD280/OD315 of diluted wines  0.565468  \n",
      "Hue                           1.000000  \n",
      "   Unnamed: 0  Total phenols  Malic acid  OD280/OD315 of diluted wines   Hue\n",
      "0           0           2.80        1.71                          3.92  1.04\n",
      "1           1           2.65        1.78                          3.40  1.05\n",
      "2           2           2.80        2.36                          3.17  1.03\n",
      "3           3           3.85        1.95                          3.45  0.86\n",
      "4           4           2.80        2.59                          2.93  1.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGood work! Dropping correlated features is often an iterative process, so you may need to try different combinations in your model.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Checking for correlated features\n",
    "\n",
    "You'll now return to the wine dataset, which consists of continuous, numerical features. Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Print out the Pearson correlation coefficients for each pair of features in the wine dataset.\n",
    "    Drop any columns from wine that have a correlation coefficient above 0.75 with at least two other columns.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop(['Flavanoids'], axis=1)\n",
    "\n",
    "print(wine.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good work! Dropping correlated features is often an iterative process, so you may need to try different combinations in your model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:07:05.008735Z",
     "iopub.status.busy": "2023-05-26T13:07:05.008327Z",
     "iopub.status.idle": "2023-05-26T13:07:05.022939Z",
     "shell.execute_reply": "2023-05-26T13:07:05.022268Z",
     "shell.execute_reply.started": "2023-05-26T13:07:05.008699Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the vector data\n",
    "volunteer_2 = volunteer[['title','category_desc']]\n",
    "\n",
    "# import module for vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# instanciate model\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "text_tfidf = tfidf_vec.fit_transform(volunteer_2['title'])\n",
    "\n",
    "# create and inver vocab\n",
    "\n",
    "vocab = {v:k for k,v in\n",
    "tfidf_vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:07:12.095539Z",
     "iopub.status.busy": "2023-05-26T13:07:12.095248Z",
     "iopub.status.idle": "2023-05-26T13:07:12.105268Z",
     "shell.execute_reply": "2023-05-26T13:07:12.104646Z",
     "shell.execute_reply.started": "2023-05-26T13:07:12.095510Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189, 942, 466]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNice job! This is a little complicated, but you'll see how it comes together in the next exercise.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Exploring text vectors, part 1\n",
    "\n",
    "Let's expand on the text vector exploration method we just learned about, using the volunteer dataset's title tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add parameters called original_vocab, for the tfidf_vec.vocabulary_, and top_n.\n",
    "    Call pd.Series() on the zipped dictionary. This will make it easier to operate on.\n",
    "    Use the .sort_values() function to sort the series and slice the index up to top_n words.\n",
    "    Call the function, setting original_vocab=tfidf_vec.vocabulary_, setting vector_index=8 to grab the 9th row, and setting top_n=3, to grab the top 3 weighted words.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job! This is a little complicated, but you'll see how it comes together in the next exercise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:09:50.504191Z",
     "iopub.status.busy": "2023-05-26T13:09:50.503929Z",
     "iopub.status.idle": "2023-05-26T13:09:50.842696Z",
     "shell.execute_reply": "2023-05-26T13:09:50.842114Z",
     "shell.execute_reply.started": "2023-05-26T13:09:50.504167Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent! In the next exercise, you'll train a model using the filtered vector.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Exploring text vectors, part 2\n",
    "\n",
    "Using the return_weights() function you wrote in the previous exercise, you're now going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Call return_weights() to return the top weighted words for that document.\n",
    "    Call set() on the returned filter_list to remove duplicated numbers.\n",
    "    Call words_to_filter, passing in the following parameters: vocab for the vocab parameter, tfidf_vec.vocabulary_ for the original_vocab parameter, text_tfidf for the vector parameter, and 3 to grab the top_n 3 weighted words from each document.\n",
    "    Finally, pass that filtered_words set into a list to use as a filter for the text vector.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent! In the next exercise, you'll train a model using the filtered vector.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:14:08.006888Z",
     "iopub.status.busy": "2023-05-26T13:14:08.006377Z",
     "iopub.status.idle": "2023-05-26T13:14:08.032284Z",
     "shell.execute_reply": "2023-05-26T13:14:08.031639Z",
     "shell.execute_reply.started": "2023-05-26T13:14:08.006830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preparing the Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create y\n",
    "\n",
    "y = volunteer['category_desc']\n",
    "\n",
    "# instanciate model\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:14:36.475345Z",
     "iopub.status.busy": "2023-05-26T13:14:36.474598Z",
     "iopub.status.idle": "2023-05-26T13:14:36.498314Z",
     "shell.execute_reply": "2023-05-26T13:14:36.497582Z",
     "shell.execute_reply.started": "2023-05-26T13:14:36.475312Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5161290322580645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAwesome! You can see that our accuracy score wasn't that different from the score at the end of Chapter 3. But don't worry, this is mainly because of how small the title field is.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Training Naive Bayes with feature selection\n",
    "\n",
    "You'll now re-run the Naive Bayes text classification model that you ran at the end of Chapter 3 with our selection choices from the previous exercise: the volunteer dataset's title and category_desc columns.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use train_test_split() on the filtered_text text vector, the y labels (which is the category_desc labels), and pass the y set to the stratify parameter, since we have an uneven class distribution.\n",
    "    Fit the nb Naive Bayes model to X_train and y_train.\n",
    "    Calculate the test set accuracy of nb.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test,y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! You can see that our accuracy score wasn't that different from the score at the end of Chapter 3. But don't worry, this is mainly because of how small the title field is.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:23:28.993602Z",
     "iopub.status.busy": "2023-05-26T13:23:28.992664Z",
     "iopub.status.idle": "2023-05-26T13:23:29.005311Z",
     "shell.execute_reply": "2023-05-26T13:23:29.004608Z",
     "shell.execute_reply.started": "2023-05-26T13:23:28.993552Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load correct dataser\n",
    "wine = pd.read_csv(path_data + 'wine_types.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:23:31.289785Z",
     "iopub.status.busy": "2023-05-26T13:23:31.289213Z",
     "iopub.status.idle": "2023-05-26T13:23:31.308900Z",
     "shell.execute_reply": "2023-05-26T13:23:31.307267Z",
     "shell.execute_reply.started": "2023-05-26T13:23:31.289755Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium   \n",
       "0   1.0    14.23        1.71  2.43               15.6      127.0  \\\n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins   \n",
       "0            2.8        3.06                  0.28             2.29  \\\n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92   1065.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:23:50.087148Z",
     "iopub.status.busy": "2023-05-26T13:23:50.086710Z",
     "iopub.status.idle": "2023-05-26T13:24:10.207187Z",
     "shell.execute_reply": "2023-05-26T13:24:10.205682Z",
     "shell.execute_reply.started": "2023-05-26T13:23:50.087111Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.97795009e-01 2.02071827e-03 9.88350594e-05 5.66222566e-05\n",
      " 1.26161135e-05 8.93235789e-06 3.13856866e-06 1.57406401e-06\n",
      " 1.15918860e-06 7.49332354e-07 3.70332305e-07 1.94185373e-07\n",
      " 8.08440051e-08]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent! In the next exercise, you'll train a model using the PCA-transformed vector.\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Using PCA\n",
    "\n",
    "In this exercise, you'll apply PCA to the wine dataset, to see if you can increase the model's accuracy.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Instantiate a PCA object.\n",
    "    Define the features (X) and labels (y) from wine, using the labels in the \"Type\" column.\n",
    "    Apply PCA to X_train and X_test, ensuring no data leakage, and store the transformed values as pca_X_train and pca_X_test.\n",
    "    Print out the .explained_variance_ratio_ attribute of pca to check how much variance is explained by each component.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop('Type', axis=1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent! In the next exercise, you'll train a model using the PCA-transformed vector.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:25:52.248581Z",
     "iopub.status.busy": "2023-05-26T13:25:52.248177Z",
     "iopub.status.idle": "2023-05-26T13:25:52.819611Z",
     "shell.execute_reply": "2023-05-26T13:25:52.818090Z",
     "shell.execute_reply.started": "2023-05-26T13:25:52.248548Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and instanciate knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T13:25:55.342111Z",
     "iopub.status.busy": "2023-05-26T13:25:55.341813Z",
     "iopub.status.idle": "2023-05-26T13:25:55.353898Z",
     "shell.execute_reply": "2023-05-26T13:25:55.353130Z",
     "shell.execute_reply.started": "2023-05-26T13:25:55.342086Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n    Fit the knn model to the PCA-transformed features, pca_X_train, and training labels, y_train.\\n    Print the test set accuracy of the knn model using pca_X_test and y_test.\\n\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Training a model with PCA\n",
    "\n",
    "Now that you have run PCA on the wine dataset, you'll finally train a KNN model using the transformed data.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fit the knn model to the PCA-transformed features, pca_X_train, and training labels, y_train.\n",
    "    Print the test set accuracy of the knn model using pca_X_test and y_test.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Fit knn to the training data\n",
    "knn.fit(pca_X_train, y_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "print(knn.score(pca_X_test, y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fit the knn model to the PCA-transformed features, pca_X_train, and training labels, y_train.\n",
    "    Print the test set accuracy of the knn model using pca_X_test and y_test.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
