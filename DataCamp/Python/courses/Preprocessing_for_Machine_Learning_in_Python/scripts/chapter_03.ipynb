{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:44.456779Z",
     "iopub.status.busy": "2023-06-01T18:39:44.456389Z",
     "iopub.status.idle": "2023-06-01T18:39:44.940310Z",
     "shell.execute_reply": "2023-06-01T18:39:44.938689Z",
     "shell.execute_reply.started": "2023-06-01T18:39:44.456743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Preprocessing_for_Machine_Learning_in_Python/datasets/'\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:45.458211Z",
     "iopub.status.busy": "2023-06-01T18:39:45.457203Z",
     "iopub.status.idle": "2023-06-01T18:39:46.075501Z",
     "shell.execute_reply": "2023-06-01T18:39:46.074837Z",
     "shell.execute_reply.started": "2023-06-01T18:39:45.458142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = path_data + 'hiking.json'\n",
    "hiking = pd.read_json(data_path)\n",
    "\n",
    "# load module\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:46.492585Z",
     "iopub.status.busy": "2023-06-01T18:39:46.492099Z",
     "iopub.status.idle": "2023-06-01T18:39:46.509074Z",
     "shell.execute_reply": "2023-06-01T18:39:46.508168Z",
     "shell.execute_reply.started": "2023-06-01T18:39:46.492551Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accessible_enc Accessible\n",
      "0               1          Y\n",
      "1               0          N\n",
      "2               0          N\n",
      "3               0          N\n",
      "4               0          N\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice work! .fit_transform() is a good way to both fit an encoding and transform the data in a single step.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Encoding categorical variables - binary\n",
    "\n",
    "Take a look at the hiking dataset. There are several columns here that need encoding before they can be modeled, one of which is the Accessible column. Accessible is a binary feature, so it has two values, Y or N, which need to be encoded into 1's and 0's. Use scikit-learn's LabelEncoder method to perform this transformation.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Store LabelEncoder() in a variable named enc.\n",
    "    Using the encoder's .fit_transform() method, encode the hiking dataset's \"Accessible\" column. Call the new column Accessible_enc.\n",
    "    Compare the two columns side-by-side to see the encoding.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible_enc', 'Accessible']].head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! .fit_transform() is a good way to both fit an encoding and transform the data in a single step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:47.407920Z",
     "iopub.status.busy": "2023-06-01T18:39:47.406388Z",
     "iopub.status.idle": "2023-06-01T18:39:47.497538Z",
     "shell.execute_reply": "2023-06-01T18:39:47.496818Z",
     "shell.execute_reply.started": "2023-06-01T18:39:47.407870Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load volunteer dataset\n",
    "volunteer = pd.read_csv(path_data + 'volunteer_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:48.047842Z",
     "iopub.status.busy": "2023-06-01T18:39:48.046937Z",
     "iopub.status.idle": "2023-06-01T18:39:48.070095Z",
     "shell.execute_reply": "2023-06-01T18:39:48.067827Z",
     "shell.execute_reply.started": "2023-06-01T18:39:48.047783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Education  Emergency Preparedness  Environment  Health   \n",
      "0          0                       0            0       0  \\\n",
      "1          0                       0            0       0   \n",
      "2          0                       0            0       0   \n",
      "3          0                       0            1       0   \n",
      "4          0                       0            1       0   \n",
      "\n",
      "   Helping Neighbors in Need  Strengthening Communities  \n",
      "0                          0                          1  \n",
      "1                          0                          1  \n",
      "2                          0                          1  \n",
      "3                          0                          0  \n",
      "4                          0                          0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGood job! get_dummies() is a simple and quick way to encode categorical variables.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Encoding categorical variables - one-hot\n",
    "\n",
    "One of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use pandas' pd.get_dummies() function to do so.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Call get_dummies() on the volunteer[\"category_desc\"] column to create the encoded columns and assign it to category_enc.\n",
    "    Print out the .head() of the category_enc variable to take a look at the encoded columns.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'], dtype = int)\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! get_dummies() is a simple and quick way to encode categorical variables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:48.983784Z",
     "iopub.status.busy": "2023-06-01T18:39:48.982769Z",
     "iopub.status.idle": "2023-06-01T18:39:48.994875Z",
     "shell.execute_reply": "2023-06-01T18:39:48.993286Z",
     "shell.execute_reply.started": "2023-06-01T18:39:48.983735Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data set\n",
    "running_times_5k = pd.read_csv(path_data + 'running_times_5k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:50.202219Z",
     "iopub.status.busy": "2023-06-01T18:39:50.201070Z",
     "iopub.status.idle": "2023-06-01T18:39:50.223995Z",
     "shell.execute_reply": "2023-06-01T18:39:50.222924Z",
     "shell.execute_reply.started": "2023-06-01T18:39:50.202069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   name  run1  run2  run3  run4  run5   mean\n",
      "0         0.0    Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
      "1         1.0   Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
      "2         2.0   Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
      "3         3.0   Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
      "4         4.0  Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNice work! .loc[] is especially helpful for operating across columns.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Aggregating numerical features\n",
    "\n",
    "A good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use the .loc[] method to select all rows and columns to find the .mean() of the each columns.\n",
    "    Print the .head() of the DataFrame to see the mean column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Use .loc to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, 'run1':'run5'].mean(axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! .loc[] is especially helpful for operating across columns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:50.965448Z",
     "iopub.status.busy": "2023-06-01T18:39:50.964418Z",
     "iopub.status.idle": "2023-06-01T18:39:50.988390Z",
     "shell.execute_reply": "2023-06-01T18:39:50.987601Z",
     "shell.execute_reply.started": "2023-06-01T18:39:50.965396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_date_converted  start_date_month\n",
      "0           2011-02-01                 2\n",
      "1           2011-01-29                 1\n",
      "2           2011-02-14                 2\n",
      "3           2011-02-05                 2\n",
      "4           2011-02-12                 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAwesome! You can also use attributes like .day to get the day and .year to get the year from datetime columns.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Extracting datetime components\n",
    "\n",
    "There are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the start_date_date column and extract just the month to use as a feature for modeling.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Convert the start_date_date column into a pandas datetime column and store it in a new column called start_date_converted.\n",
    "    Retrieve the month component of start_date_converted and store it in a new column called start_date_month.\n",
    "    Print the .head() of just the start_date_converted and start_date_month columns.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].dt.month\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[['start_date_converted', 'start_date_month']].head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! You can also use attributes like .day to get the day and .year to get the year from datetime columns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:51.804960Z",
     "iopub.status.busy": "2023-06-01T18:39:51.803239Z",
     "iopub.status.idle": "2023-06-01T18:39:51.820790Z",
     "shell.execute_reply": "2023-06-01T18:39:51.819933Z",
     "shell.execute_reply.started": "2023-06-01T18:39:51.804910Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length  Length_num\n",
      "0   0.8 miles        0.80\n",
      "1    1.0 mile        1.00\n",
      "2  0.75 miles        0.75\n",
      "3   0.5 miles        0.50\n",
      "4   0.5 miles        0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! Regular expressions are a useful way to perform text extraction.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Extracting string patterns\n",
    "\n",
    "The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Search the text in the length argument for numbers and decimals using an appropriate pattern.\n",
    "    Extract the matched pattern and convert it to a float.\n",
    "    Apply the return_mileage() function to each row in the hiking[\"Length\"] column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import re\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search('\\d+\\.\\d+', str(length))\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking['Length'].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Regular expressions are a useful way to perform text extraction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:52.736400Z",
     "iopub.status.busy": "2023-06-01T18:39:52.735584Z",
     "iopub.status.idle": "2023-06-01T18:39:52.747113Z",
     "shell.execute_reply": "2023-06-01T18:39:52.746041Z",
     "shell.execute_reply.started": "2023-06-01T18:39:52.736351Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:54.362528Z",
     "iopub.status.busy": "2023-06-01T18:39:54.360918Z",
     "iopub.status.idle": "2023-06-01T18:39:54.384770Z",
     "shell.execute_reply": "2023-06-01T18:39:54.384034Z",
     "shell.execute_reply.started": "2023-06-01T18:39:54.362448Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNice job. scikit-learn provides several methods for text vectorization.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Vectorizing text\n",
    "\n",
    "You'll now transform the volunteer dataset's title column into a text vector, which you'll use in a prediction task in the next exercise.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Store the volunteer[\"title\"] column in a variable named title_text.\n",
    "    Instantiate a TfidfVectorizer as tfidf_vec.\n",
    "    Transform the text in title_text into a tf-idf vector using tfidf_vec.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Take the title text\n",
    "title_text = volunteer['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice job. scikit-learn provides several methods for text vectorization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T18:39:56.015685Z",
     "iopub.status.busy": "2023-06-01T18:39:56.015296Z",
     "iopub.status.idle": "2023-06-01T18:39:56.024733Z",
     "shell.execute_reply": "2023-06-01T18:39:56.023807Z",
     "shell.execute_reply.started": "2023-06-01T18:39:56.015652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           Web designer\n",
       "1          Urban Adventures - Ice Skating at Lasker Rink\n",
       "2      Fight global hunger and support women farmers ...\n",
       "3                                          Stop 'N' Swap\n",
       "4                                   Queens Stop 'N' Swap\n",
       "                             ...                        \n",
       "607            Volunteer for NYLAG's Food Stamps Project\n",
       "608      Iridescent Science Studio Open House Volunteers\n",
       "609                                    French Translator\n",
       "610                    Marketing & Advertising Volunteer\n",
       "611    Volunteer filmmakers to help Mayor's Office wi...\n",
       "Name: title, Length: 612, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:27:12.384698Z",
     "iopub.status.busy": "2023-05-26T12:27:12.384198Z",
     "iopub.status.idle": "2023-05-26T12:27:12.408665Z",
     "shell.execute_reply": "2023-05-26T12:27:12.408018Z",
     "shell.execute_reply.started": "2023-05-26T12:27:12.384656Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load modules\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:27:21.153275Z",
     "iopub.status.busy": "2023-05-26T12:27:21.152949Z",
     "iopub.status.idle": "2023-05-26T12:27:21.178935Z",
     "shell.execute_reply": "2023-05-26T12:27:21.177661Z",
     "shell.execute_reply.started": "2023-05-26T12:27:21.153219Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNice work! Notice that the model doesn't score very well. We'll work on selecting the best features for modeling in the next chapter.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Text classification using tf/idf vectors\n",
    "\n",
    "Now that you've encoded the volunteer dataset's title column into tf/idf vectors, you'll use those vectors to predict the category_desc column.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Split the text_tfidf vector and y target variable into training and test sets, setting the stratify parameter equal to y, since the class distribution is uneven. Notice that we have to run the .toarray() method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "    Fit the X_train and y_train data to the Naive Bayes model, nb.\n",
    "    Print out the test set accuracy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! Notice that the model doesn't score very well. We'll work on selecting the best features for modeling in the next chapter.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
