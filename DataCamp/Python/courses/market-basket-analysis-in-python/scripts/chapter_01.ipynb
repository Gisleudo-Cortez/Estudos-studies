{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics of market basket analysis\n",
    "\n",
    "Market basket analysis uses lists of transactions to identify useful associations between items. Such associations can be written in the form of a rule that has an antecedent and a consequent. Let's assume a small grocery store has asked you to look at their transaction data. After some analysis, you find the rule given below.\n",
    "\n",
    "{cereal} -> {milk}\n",
    "\n",
    "Which statement about this rule is correct?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    {cereal} is the antecedent, {milk} is the consequent, and both are items. {Answer}\n",
    "    \n",
    "    \n",
    "    {milk} is the antecedent, {cereal} is the consequent, and both are items.\n",
    "    \n",
    "    \n",
    "    {cereal} is the antecedent, but neither is an item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [['bread', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['cereal', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['bread', 'gum'],\n",
    " ['coffee', 'gum'],\n",
    " ['coffee', 'gum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee: 40\n",
      "cereal: 25\n",
      "bread: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExcellent work! Based on our results, which were printed to the console, we can recommend that the store owner cross-sell chewing gum next to the coffee. In the following video, we will consider how to identify useful association rules in more challenging environments.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Cross-selling products\n",
    "\n",
    "The small grocery store has decided to cross-sell chewing gum with either coffee, cereal, or bread. To determine which of the three items is best to use, the store owner has performed an experiment. For one week, she sold chewing gum next to the register and recorded all transactions where it was purchased with either coffee, cereal, or bread. The transactions from that day are available as a list of lists named transactions. Each transaction is either ['coffee','gum'], ['cereal','gum'], or ['bread','gum'].\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Count the number of transactions that contain coffee and gum.\n",
    "\n",
    "    Count the number of transactions that contain cereal and gum.\n",
    "\n",
    "    Count the number of transactions that contain bread and gum.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Count the number of transactions with coffee and gum\n",
    "coffee = transactions.count(['coffee', 'gum'])\n",
    "\n",
    "# Count the number of transactions with cereal and gum\n",
    "cereal = transactions.count(['cereal', 'gum'])\n",
    "\n",
    "# Count the number of transactions with bread and gum\n",
    "bread = transactions.count(['bread', 'gum'])\n",
    "\n",
    "# Print the counts for each transaction.\n",
    "print('coffee:', coffee)\n",
    "print('cereal:', cereal)\n",
    "print('bread:', bread)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Based on our results, which were printed to the console, we can recommend that the store owner cross-sell chewing gum next to the coffee. In the following video, we will consider how to identify useful association rules in more challenging environments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple antecedents and consequents\n",
    "\n",
    "Market basket analysis revolves around the use of association rules, which are if-then statements about the relationship between two sets of items. The rule {coffee}\n",
    "{milk}, for instance, is read as \"if coffee then milk,\" where coffee is the antecedent and milk is the consequent. Many rules have multiple antecedents and consequents. We will examine such rules in this exercise.\n",
    "\n",
    "![Answer](images/ch01-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['milk', 'bread', 'biscuit'], ['bread', 'milk', 'biscuit', 'cereal'], ['bread', 'tea'], ['jam', 'bread', 'milk'], ['tea', 'biscuit'], ['bread', 'tea'], ['tea', 'cereal'], ['bread', 'tea', 'biscuit'], ['jam', 'bread', 'tea'], ['bread', 'milk'], ['coffee', 'orange', 'biscuit', 'cereal'], ['coffee', 'orange', 'biscuit', 'cereal'], ['coffee', 'sugar'], ['bread', 'coffee', 'orange'], ['bread', 'sugar', 'biscuit'], ['coffee', 'sugar', 'cereal'], ['bread', 'sugar', 'biscuit'], ['bread', 'coffee', 'sugar'], ['bread', 'coffee', 'sugar'], ['tea', 'milk', 'coffee', 'cereal']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExcellent work! In the rest of the chapter, we will discuss how to put all of the pieces together to perform market basket analysis. You will load and prepare data, generate association rules, and then discard the subset of rules that are not useful.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Preparing data for market basket analysis\n",
    "\n",
    "Throughout this course, you will typically encounter data in one of two formats: a pandas DataFrame or a list of lists. DataFrame objects will be constructed by importing a csv file using pandas. They will consist of a single column of data, where each element contains a string of items in a transaction, separated by a comma, as in the table below.\n",
    "\n",
    "In this exercise, you will practice loading the data from a csv file and will prepare it for use as a list of lists. Note that the path to the grocery store dataset has been defined and is available to you as groceries_path.\n",
    "Transaction\n",
    "'milk,bread,biscuit'\n",
    "'bread,milk,biscuit,cereal'\n",
    "â€¦\n",
    "'tea,milk,coffee,cereal'\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the pandas package under the alias pd.\n",
    "\n",
    "    Use pandas to read the csv file at the path specified by groceries_path.\n",
    "\n",
    "    Select the Transaction column from the DataFrame and split each string of comma-separated items into a list.\n",
    "\n",
    "    Convert the DataFrame of transactions into a list of lists.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "groceries_path = 'datasets/groceries.csv'\n",
    "# Import pandas under the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load transactions from pandas\n",
    "groceries = pd.read_csv(groceries_path)\n",
    "\n",
    "# Split transaction strings into lists\n",
    "transactions = groceries['Transaction'].apply(lambda t: t.split(','))\n",
    "\n",
    "# Convert DataFrame column into list of strings\n",
    "transactions = list(transactions)\n",
    "\n",
    "# Print the list of transactions\n",
    "print(transactions)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! In the rest of the chapter, we will discuss how to put all of the pieces together to perform market basket analysis. You will load and prepare data, generate association rules, and then discard the subset of rules that are not useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tea', 'cereal'), ('tea', 'milk'), ('tea', 'orange'), ('tea', 'coffee'), ('tea', 'sugar'), ('tea', 'jam'), ('tea', 'biscuit'), ('tea', 'bread'), ('cereal', 'tea'), ('cereal', 'milk'), ('cereal', 'orange'), ('cereal', 'coffee'), ('cereal', 'sugar'), ('cereal', 'jam'), ('cereal', 'biscuit'), ('cereal', 'bread'), ('milk', 'tea'), ('milk', 'cereal'), ('milk', 'orange'), ('milk', 'coffee'), ('milk', 'sugar'), ('milk', 'jam'), ('milk', 'biscuit'), ('milk', 'bread'), ('orange', 'tea'), ('orange', 'cereal'), ('orange', 'milk'), ('orange', 'coffee'), ('orange', 'sugar'), ('orange', 'jam'), ('orange', 'biscuit'), ('orange', 'bread'), ('coffee', 'tea'), ('coffee', 'cereal'), ('coffee', 'milk'), ('coffee', 'orange'), ('coffee', 'sugar'), ('coffee', 'jam'), ('coffee', 'biscuit'), ('coffee', 'bread'), ('sugar', 'tea'), ('sugar', 'cereal'), ('sugar', 'milk'), ('sugar', 'orange'), ('sugar', 'coffee'), ('sugar', 'jam'), ('sugar', 'biscuit'), ('sugar', 'bread'), ('jam', 'tea'), ('jam', 'cereal'), ('jam', 'milk'), ('jam', 'orange'), ('jam', 'coffee'), ('jam', 'sugar'), ('jam', 'biscuit'), ('jam', 'bread'), ('biscuit', 'tea'), ('biscuit', 'cereal'), ('biscuit', 'milk'), ('biscuit', 'orange'), ('biscuit', 'coffee'), ('biscuit', 'sugar'), ('biscuit', 'jam'), ('biscuit', 'bread'), ('bread', 'tea'), ('bread', 'cereal'), ('bread', 'milk'), ('bread', 'orange'), ('bread', 'coffee'), ('bread', 'sugar'), ('bread', 'jam'), ('bread', 'biscuit')]\n",
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent work! Later in the chapter, you'll move beyond generating and counting association rules to selecting rules that are useful.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Generating association rules\n",
    "\n",
    "As you saw, the function permutations from the module itertools can be used to quickly generate the set of all one-antecedent, one-consequent rules. You do not, of course, know which of these rules are useful. You simply know that each is a valid way to combine two items.\n",
    "\n",
    "Let's practice generating and counting the set of all rules for a subset of the grocery dataset: coffee, tea, milk, and sugar.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Complete the import statement to import the permutations function.\n",
    "\n",
    "    Generate all association rules from the groceries list.\n",
    "\n",
    "    Print the set of rules.\n",
    "\n",
    "    Print the number of rules.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import permutations from the itertools module\n",
    "from itertools import permutations\n",
    "\n",
    "# Define the set of groceries\n",
    "flattened = [i for t in transactions for i in t]\n",
    "groceries = list(set(flattened))\n",
    "\n",
    "# Generate all possible rules from groceries list\n",
    "rules = list(permutations(groceries, 2))\n",
    "\n",
    "# Print the set of rules\n",
    "print(rules)\n",
    "\n",
    "# Print the number of rules\n",
    "print(len(rules))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Later in the chapter, you'll move beyond generating and counting association rules to selecting rules that are useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    biscuit  bread  cereal  coffee    jam   milk  orange  sugar    tea\n",
      "0      True   True   False   False  False   True   False  False  False\n",
      "1      True   True    True   False  False   True   False  False  False\n",
      "2     False   True   False   False  False  False   False  False   True\n",
      "3     False   True   False   False   True   True   False  False  False\n",
      "4      True  False   False   False  False  False   False  False   True\n",
      "5     False   True   False   False  False  False   False  False   True\n",
      "6     False  False    True   False  False  False   False  False   True\n",
      "7      True   True   False   False  False  False   False  False   True\n",
      "8     False   True   False   False   True  False   False  False   True\n",
      "9     False   True   False   False  False   True   False  False  False\n",
      "10     True  False    True    True  False  False    True  False  False\n",
      "11     True  False    True    True  False  False    True  False  False\n",
      "12    False  False   False    True  False  False   False   True  False\n",
      "13    False   True   False    True  False  False    True  False  False\n",
      "14     True   True   False   False  False  False   False   True  False\n",
      "15    False  False    True    True  False  False   False   True  False\n",
      "16     True   True   False   False  False  False   False   True  False\n",
      "17    False   True   False    True  False  False   False   True  False\n",
      "18    False   True   False    True  False  False   False   True  False\n",
      "19    False  False    True    True  False   True   False  False   True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent work! In the next exercise, we'll make use of the one-hot encoded transaction dataset to compute the support metric.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "One-hot encoding transaction data\n",
    "\n",
    "Throughout the course, we will use a common pipeline for preprocessing data for use in market basket analysis. The first step is to import a pandas DataFrame and select the column that contains transactions. Each transaction in the column will be a string that consists of a number of items, each separated by a comma. The next step is to use a lambda function to split each transaction string into a list, thereby transforming the column into a list of lists.\n",
    "\n",
    "In this exercise, you'll start with the list of lists from the grocery dataset, which is available to you as transactions. You will then transform transactions into a one-hot encoded DataFrame, where each column consists of TRUE and FALSE values that indicate whether an item was included in a transaction.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    From the mlxtend.preprocessing, import TransactionEncoder\n",
    "\n",
    "    Instantiate a transaction encoder and identify the unique items in transactions.\n",
    "\n",
    "    One-hot encode transactions in an array and assign its values to onehot.\n",
    "\n",
    "    Convert the array into a pandas DataFrame using the item names as column headers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the transaction encoder function from mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Instantiate transaction encoder and identify unique items in transactions\n",
    "encoder = TransactionEncoder().fit(transactions)\n",
    "\n",
    "# One-hot encode transactions\n",
    "onehot = encoder.transform(transactions)\n",
    "\n",
    "# Convert one-hot encoded data to DataFrame\n",
    "onehot = pd.DataFrame(onehot, columns = encoder.columns_)\n",
    "\n",
    "# Print the one-hot encoded transaction dataset\n",
    "print(onehot)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! In the next exercise, we'll make use of the one-hot encoded transaction dataset to compute the support metric.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biscuit    0.40\n",
      "bread      0.65\n",
      "cereal     0.30\n",
      "coffee     0.40\n",
      "jam        0.10\n",
      "milk       0.25\n",
      "orange     0.15\n",
      "sugar      0.30\n",
      "tea        0.35\n",
      "dtype: float64\n",
      "biscuit      0.40\n",
      "bread        0.65\n",
      "cereal       0.30\n",
      "coffee       0.40\n",
      "jam          0.10\n",
      "milk         0.25\n",
      "orange       0.15\n",
      "sugar        0.30\n",
      "tea          0.35\n",
      "jam+bread    0.10\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent work! In the next chapter, we'll start working with much larger datasets, where it will be necessary to prune both items and rules by support.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Computing the support metric\n",
    "\n",
    "In the previous exercise, you one-hot encoded a small grocery store's transactions as the DataFrame onehot. In this exercise, you'll make use of that DataFrame and the support metric to help the store's owner. First, she has asked you to identify frequently purchased items, which you'll do by computing support at the item-level. And second, she asked you to check whether the rule {jam}\n",
    "{bread} has a support of over 0.05. Note that onehot has been defined and is available. Additionally, pandas has been imported under the alias pd and numpy has been imported under the alias np.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Compute the support value for each item in the one-hot encoded dataset, onehot.\n",
    "    Print the support values.\n",
    "\n",
    "    Add a column, jam+bread, to onehot that is TRUE if both jam and bread are both in the transaction.\n",
    "    Print support.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute the support\n",
    "support = onehot.mean()\n",
    "\n",
    "# Print the support\n",
    "print(support)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Add a jam+bread column to the DataFrame onehot\n",
    "onehot['jam+bread'] = np.logical_and(onehot['jam'], onehot['bread'])\n",
    "\n",
    "# Compute the support\n",
    "support = onehot.mean()\n",
    "\n",
    "# Print the support values\n",
    "print(support)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! In the next chapter, we'll start working with much larger datasets, where it will be necessary to prune both items and rules by support.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
