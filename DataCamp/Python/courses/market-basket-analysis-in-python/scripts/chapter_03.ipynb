{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>50'S CHRISTMAS GIFT BAG LARGE</th>\n",
       "      <th>DOLLY GIRL BEAKER</th>\n",
       "      <th>I LOVE LONDON MINI BACKPACK</th>\n",
       "      <th>RED SPOT GIFT BAG LARGE</th>\n",
       "      <th>SPACEBOY BABY GIFT SET</th>\n",
       "      <th>12 MESSAGE CARDS WITH ENVELOPES</th>\n",
       "      <th>12 PENCIL SMALL TUBE WOODLAND</th>\n",
       "      <th>12 PENCILS SMALL TUBE RED RETROSPOT</th>\n",
       "      <th>12 PENCILS TALL TUBE POSY</th>\n",
       "      <th>12 PENCILS TALL TUBE RED RETROSPOT</th>\n",
       "      <th>...</th>\n",
       "      <th>YELLOW GIANT GARDEN THERMOMETER</th>\n",
       "      <th>YELLOW SHARK HELICOPTER</th>\n",
       "      <th>YOU'RE CONFUSING ME METAL SIGN</th>\n",
       "      <th>ZINC FOLKART SLEIGH BELLS</th>\n",
       "      <th>ZINC METAL HEART DECORATION</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STAR LARGE</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STARS SMALL</th>\n",
       "      <th>ZINC WILLIE WINKIE  CANDLE STICK</th>\n",
       "      <th>amazon adjust</th>\n",
       "      <th>check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1039 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    50'S CHRISTMAS GIFT BAG LARGE   DOLLY GIRL BEAKER  \\\n",
       "0                           False               False   \n",
       "1                           False               False   \n",
       "2                           False               False   \n",
       "3                           False               False   \n",
       "4                           False               False   \n",
       "\n",
       "    I LOVE LONDON MINI BACKPACK   RED SPOT GIFT BAG LARGE  \\\n",
       "0                         False                     False   \n",
       "1                         False                     False   \n",
       "2                         False                     False   \n",
       "3                         False                     False   \n",
       "4                         False                     False   \n",
       "\n",
       "    SPACEBOY BABY GIFT SET  12 MESSAGE CARDS WITH ENVELOPES  \\\n",
       "0                    False                            False   \n",
       "1                    False                            False   \n",
       "2                    False                            False   \n",
       "3                    False                            False   \n",
       "4                    False                            False   \n",
       "\n",
       "   12 PENCIL SMALL TUBE WOODLAND  12 PENCILS SMALL TUBE RED RETROSPOT  \\\n",
       "0                          False                                False   \n",
       "1                          False                                False   \n",
       "2                          False                                False   \n",
       "3                          False                                False   \n",
       "4                          False                                False   \n",
       "\n",
       "   12 PENCILS TALL TUBE POSY  12 PENCILS TALL TUBE RED RETROSPOT  ...  \\\n",
       "0                      False                               False  ...   \n",
       "1                      False                               False  ...   \n",
       "2                      False                               False  ...   \n",
       "3                      False                               False  ...   \n",
       "4                      False                               False  ...   \n",
       "\n",
       "   YELLOW GIANT GARDEN THERMOMETER  YELLOW SHARK HELICOPTER  \\\n",
       "0                            False                    False   \n",
       "1                            False                    False   \n",
       "2                            False                    False   \n",
       "3                            False                    False   \n",
       "4                            False                    False   \n",
       "\n",
       "   YOU'RE CONFUSING ME METAL SIGN   ZINC FOLKART SLEIGH BELLS  \\\n",
       "0                            False                      False   \n",
       "1                            False                      False   \n",
       "2                            False                      False   \n",
       "3                            False                      False   \n",
       "4                            False                      False   \n",
       "\n",
       "   ZINC METAL HEART DECORATION  ZINC T-LIGHT HOLDER STAR LARGE  \\\n",
       "0                        False                           False   \n",
       "1                        False                           False   \n",
       "2                        False                           False   \n",
       "3                        False                           False   \n",
       "4                        False                           False   \n",
       "\n",
       "   ZINC T-LIGHT HOLDER STARS SMALL  ZINC WILLIE WINKIE  CANDLE STICK  \\\n",
       "0                            False                             False   \n",
       "1                            False                             False   \n",
       "2                            False                             False   \n",
       "3                            False                             False   \n",
       "4                            False                             False   \n",
       "\n",
       "   amazon adjust  check  \n",
       "0          False  False  \n",
       "1          False  False  \n",
       "2          False  False  \n",
       "3          False  False  \n",
       "4          False  False  \n",
       "\n",
       "[5 rows x 1039 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#retail = pd.read_csv('datasets/online_retail.csv')\n",
    "#retail.head()\n",
    "\n",
    "#onehot = pd.get_dummies(retail['Description'])\n",
    "#onehot.head()\n",
    "\n",
    "onehot = pd.read_csv('datasets/onehot_retail.csv').drop(columns=['index'], axis=1)\n",
    "onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of Signs: 0.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent work! If you look at the printed statement, you'll notice that support for signs is 0.10, which suggests that signs are an important category of items for the retailer.\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Performing aggregation\n",
    "\n",
    "After completing minor consulting jobs for a library and an ebook seller, you've finally received your first big market basket analysis project: advising an online novelty gifts retailer on cross-promotions. Since the retailer has never previously hired a data scientist, it would like you to start the project by exploring its transaction data. It has asked you to perform aggregation for all signs in the dataset and also compute the support for this category. Note that pandas has been imported for you as pd. Additionally, the data has been imported in one-hot encoded format as onehot.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Select the subset of the DataFrame's columns that contain the string sign.\n",
    "\n",
    "    Print the support for signs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Select the column headers for sign items\n",
    "sign_headers = [i for i in onehot.columns if i.lower().find('sign')>=0]\n",
    "\n",
    "# Select columns of sign items using sign_headers\n",
    "sign_columns = onehot[sign_headers]\n",
    "\n",
    "# Perform aggregation of sign items into sign category\n",
    "signs = sign_columns.sum(axis = 1) >= 1.0\n",
    "\n",
    "# Print support for signs\n",
    "print('Share of Signs: %.2f' % signs.mean())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! If you look at the printed statement, you'll notice that support for signs is 0.10, which suggests that signs are an important category of items for the retailer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGood job! You've now defined a function to perform aggregation, which we can then apply to any category of items.\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Defining an aggregation function\n",
    "\n",
    "Surprised by the high share of sign items in its inventory, the retailer decides that it makes sense to do further aggregation for different categories to explore the data better. This seems trivial to you, but the retailer has not previously been able to perform even a basic descriptive analysis of its transaction and items.\n",
    "\n",
    "The retailer asks you to perform aggregation for the candles, bags, and boxes categories. To simplify the task, you decide to write a function. It will take a string that contains an item's category. It will then output a DataFrame that indicates whether each transaction includes items from that category. Note that pandas has been imported for you as pd. Additionally, the data has been imported in one-hot encoded format as onehot.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "    Complete the list comprehension that extracts a subset of the column headers.\n",
    "    \n",
    "    Select the columns for the item you wish to aggregate.\n",
    "    \n",
    "    Perform aggregation using the function aggregate() for bags, boxes, and candles using the strings bag, box, and candle.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "def aggregate(item):\n",
    "\t# Select the column headers for sign items in onehot\n",
    "\titem_headers = [i for i in onehot.columns if i.lower().find(item)>=0]\n",
    "\n",
    "\t# Select columns of sign items\n",
    "\titem_columns = onehot[item_headers]\n",
    "\n",
    "\t# Return category of aggregated items\n",
    "\treturn item_columns.sum(axis = 1) >= 1.0\n",
    "\n",
    "# Aggregate items for the bags, boxes, and candles categories  \n",
    "bags = aggregate('bag')\n",
    "boxes = aggregate('boxes')\n",
    "candles = aggregate('candles')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! You've now defined a function to perform aggregation, which we can then apply to any category of items.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning and Apriori\n",
    "\n",
    "In the video, we introduced the Apriori algorithm, which made use of the Apriori principle to prune itemsets. The Apriori principle tells us that subsets of frequent itemsets are frequent. Thus, if we find an infrequent itemset, which we'll call {X}, then it must be the case that {X, Y} is also infrequent, so we may eliminate it without computing its support.\n",
    "\n",
    "In this exercise, you'll be given itemsets and information about the frequency of its subsets. You will need to decide whether the information is sufficient to prune the itemset or whether we need to compute its support.\n",
    "\n",
    "![Answer](images/ch03-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support                              itemsets\n",
      "0  0.006767          (HOT WATER BOTTLE KEEP CALM)\n",
      "1  0.007519             (JUMBO BAG RED RETROSPOT)\n",
      "2  0.006015     (PAPER CHAIN KIT 50'S CHRISTMAS )\n",
      "3  0.006015                      (POPCORN HOLDER)\n",
      "4  0.006767  (WHITE HANGING HEART T-LIGHT HOLDER)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat work! Notice that all of the printed itemsets contained only one item. We'll return to this in the next exercise.\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Identifying frequent itemsets with Apriori\n",
    "\n",
    "The aggregation exercise you performed for the online retailer proved helpful. It offered a starting point for understanding which categories of items appear frequently in transactions. The retailer now wants to explore the individual items themselves to find out which are frequent.\n",
    "\n",
    "In this exercise, you'll apply the Apriori algorithm to the online retail dataset without aggregating first. Your objective will be to prune the itemsets using a minimum value of support and a maximum item number threshold. Note that pandas has been imported as pd and the one-hot encoded data is available as onehot.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Pass onehot to the Apriori algorithm.\n",
    "\n",
    "    Set the minimum support value to 0.006.\n",
    "\n",
    "    Set the maximum itemset length to 3.\n",
    "\n",
    "    Print a preview of the first five itemsets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import apriori from mlxtend\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# Compute frequent itemsets using the Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot, \n",
    "                            min_support = 0.006, \n",
    "                            max_len = 3, \n",
    "                            use_colnames = True)\n",
    "\n",
    "# Print a preview of the frequent itemsets\n",
    "print(frequent_itemsets.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Notice that all of the printed itemsets contained only one item. We'll return to this in the next exercise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExcellent work! The DataFrame was, in fact, generated by the Apriori algorithm using a support value of 0.002.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Selecting a support threshold\n",
    "\n",
    "The manager of the online gift store looks at the results you provided from the previous exercise and commends you for the good work. She does, however, raise an issue: all of the itemsets you identified contain only one item. She asks whether it would be possible to use a less restrictive rule and to generate more itemsets, possibly including those with multiple items.\n",
    "\n",
    "After agreeing to do this, you think about what might explain the lack of itemsets with more than 1 item. It can't be the max_len parameter, since that was set to three. You decide it must be support and decide to test two different values, each time checking how many additional itemsets are generated. Note that pandas is available as pd and the one-hot encoded data is available as onehot.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Complete the import statement for the apriori algorithm.\n",
    "    For frequent_itemsets_1, set the min support to 0.003 and the maximum length to 3.\n",
    "    For frequent_itemsets_2, set the min support to 0.001 and the maximum length to 3.\n",
    "---\n",
    "Question\n",
    "\n",
    "The DataFrame frequent_itemsets_3 is available in the console. It was generated using the Apriori algorithm with a max_length of 3. Recall that frequent_itemsets_1 and frequent_itemsets_2 generated 91 and 429 rules. Based on this information and the DataFrame itself, what can you say about the support threshold used to generate frequent_itemsets_3?\n",
    "\n",
    "Possible answers:\n",
    "    \n",
    "    The support value is higher than the one used for frequent_itemsets_2, but lower than for frequent_itemsets_1. {Answer}\n",
    "    \n",
    "    The support value is higher than the support value used for frequent_itemsets_1\n",
    "    \n",
    "    There is not enough information to know with certainty.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import apriori from mlxtend\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# Compute frequent itemsets using a support of 0.003 and length of 3\n",
    "frequent_itemsets_1 = apriori(onehot, min_support = 0.003, \n",
    "                            max_len = 3, use_colnames = True)\n",
    "\n",
    "# Compute frequent itemsets using a support of 0.001 and length of 3\n",
    "frequent_itemsets_2 = apriori(onehot, min_support = 0.001, \n",
    "                            max_len = 3, use_colnames = True)\n",
    "\n",
    "# Print the number of freqeuent itemsets\n",
    "print(len(frequent_itemsets_1), len(frequent_itemsets_2))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! The DataFrame was, in fact, generated by the Apriori algorithm using a support value of 0.002.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExcellent work! Notice that rules_1 generated no association rules; whereas rules_2, which used a lower support threshold for the Apriori algorithm, generated two.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Generating association rules\n",
    "\n",
    "In the final exercise of the previous section, you computed itemsets for the novelty gift store owner using the Apriori algorithm. You told the store owner that relaxing support from 0.005 to 0.003 increased the number of itemsets from 9 to 91. Relaxing it again to 0.001 increased the number to 429. Satisfied with the descriptive work you've done, the store manager asks you to identify some association rules from those two sets of frequent itemsets you computed.\n",
    "\n",
    "Note that pandas has been imported for you as pd and the two frequent itemsets are available as frequent_itemset_1 and frequent_itemset_2. Your objective is to determine what association rules can be mined from these itemsets.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the algorithm from mlxtend that computes association rules from apriori algorithm results.\n",
    "\n",
    "    Complete the statement to compute association rules for frequent_itemsets_1 using the support metric and a threshold of 0.0015.\n",
    "\n",
    "    Complete the statement to compute association rules for frequent_itemsets_2 using the support metric and a threshold of 0.0015.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the association rule function from mlxtend\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Compute all association rules for frequent_itemsets_1\n",
    "rules_1 = association_rules(frequent_itemsets_1, \n",
    "                            metric = \"support\", \n",
    "                         \tmin_threshold = 0.0013)\n",
    "\n",
    "# Compute all association rules for frequent_itemsets_2\n",
    "rules_2 = association_rules(frequent_itemsets_2, \n",
    "                            metric = \"support\", \n",
    "                        \tmin_threshold = 0.0013)\n",
    "\n",
    "# Print the number of association rules generated\n",
    "print(len(rules_1), len(rules_2))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Notice that rules_1 generated no association rules; whereas rules_2, which used a lower support threshold for the Apriori algorithm, generated two.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   antecedents                  consequents  \\\n",
      "0  (BIRTHDAY CARD, RETRO SPOT)    (JUMBO BAG RED RETROSPOT)   \n",
      "1    (JUMBO BAG RED RETROSPOT)  (BIRTHDAY CARD, RETRO SPOT)   \n",
      "\n",
      "   antecedent support  consequent support   support  confidence       lift  \\\n",
      "0            0.002256            0.007519  0.001504    0.666667  88.666667   \n",
      "1            0.007519            0.002256  0.001504    0.200000  88.666667   \n",
      "\n",
      "   leverage  conviction  zhangs_metric  \n",
      "0  0.001487    2.977444       0.990957  \n",
      "1  0.001487    1.247180       0.996212  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExcellent work! It looks like you've ended up with two association rules once again, both with lift values greater than 1.0.\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Pruning with lift\n",
    "\n",
    "Once again, you report back to the novelty gift store manager. This time, you tell her that you identified no rules when you used a higher support threshold for the Apriori algorithm and only two rules when you used a lower threshold. She commends you for the good work, but asks you to consider using another metric to reduce the two rules to one.\n",
    "\n",
    "You remember that lift had a simple interpretation: values greater than 1 indicate that items co-occur more than we would expect if they were independently distributed across transactions. You decide to use lift, since that message will be simple to convey. Note that pandas is available as pd and the one-hot encoded transaction data is available as onehot. Additionally, apriori has been imported from mlxtend.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the association_rules function from mlxtend.\n",
    "\n",
    "    Compute the frequent itemsets using a support of 0.001 and a maximum itemset length of 2.\n",
    "\n",
    "    Complete the statement to retain rules with a lift of at least 1.0.\n",
    "\n",
    "    Print the DataFrame of rules.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the association rules function\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Compute frequent itemsets using the Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.001, \n",
    "                            max_len = 2, use_colnames = True)\n",
    "\n",
    "# Compute all association rules for frequent_itemsets\n",
    "rules = association_rules(frequent_itemsets, \n",
    "                            metric = \"lift\", \n",
    "                         \tmin_threshold = 1.0)\n",
    "\n",
    "# Print association rules\n",
    "print(rules)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! It looks like you've ended up with two association rules once again, both with lift values greater than 1.0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   antecedents                consequents  antecedent support  \\\n",
      "0  (BIRTHDAY CARD, RETRO SPOT)  (JUMBO BAG RED RETROSPOT)            0.002256   \n",
      "\n",
      "   consequent support   support  confidence       lift  leverage  conviction  \\\n",
      "0            0.007519  0.001504    0.666667  88.666667  0.001487    2.977444   \n",
      "\n",
      "   zhangs_metric  \n",
      "0       0.990957  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat work! Notice that we have narrowed things down to just a single rule. We can recommend this to the manager.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Pruning with confidence\n",
    "\n",
    "Once again, you've come up short: you found multiple useful rules, but can't narrow it down to one. Even worse, the two rules you found used the same itemset, but just swapped the antecedents and consequents. You decide to see whether pruning by another metric might allow you to narrow things down to a single association rule.\n",
    "\n",
    "What would be the right metric? Both lift and support are identical for all rules that can be generated from an itemset, so you decide to use confidence instead, which differs for rules produced from the same itemset. Note that pandas is available as pd and the one-hot encoded transaction data is available as onehot. Additionally, apriori has been imported from mlxtend.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import association_rules from mlxtend.\n",
    "\n",
    "    Complete the statement for the apriori algorithm using a support value of 0.0015 and a maximum itemset length of 2.\n",
    "\n",
    "    Complete the statement for association rules using confidence as the metric and a threshold value of 0.5.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the association rules function\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Compute frequent itemsets using the Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.0015, \n",
    "                            max_len = 2, use_colnames = True)\n",
    "\n",
    "# Compute all association rules using confidence\n",
    "rules = association_rules(frequent_itemsets, \n",
    "                            metric = \"confidence\", \n",
    "                         \tmin_threshold = 0.5)\n",
    "\n",
    "# Print association rules\n",
    "print(rules)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Notice that we have narrowed things down to just a single rule. We can recommend this to the manager.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = pd.read_csv('datasets/aggregated.csv').drop(columns=['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   antecedents     consequents  antecedent support  consequent support  \\\n",
      "1        (bag)           (box)            0.466307            0.256065   \n",
      "2        (bag)        (candle)            0.466307            0.088949   \n",
      "9       (sign)           (box)            0.355795            0.256065   \n",
      "10      (sign)        (candle)            0.355795            0.088949   \n",
      "15      (sign)   (bag, candle)            0.355795            0.010782   \n",
      "16       (bag)  (sign, candle)            0.466307            0.008086   \n",
      "\n",
      "     support  confidence      lift  leverage  conviction  zhangs_metric  \n",
      "1   0.021563    0.046243  0.180590 -0.097841    0.780005      -0.894758  \n",
      "2   0.010782    0.023121  0.259940 -0.030696    0.932615      -0.842137  \n",
      "9   0.018868    0.053030  0.207097 -0.072239    0.785596      -0.855975  \n",
      "10  0.008086    0.022727  0.255510 -0.023561    0.932238      -0.818939  \n",
      "15  0.005391    0.015152  1.405303  0.001555    1.004437       0.447699  \n",
      "16  0.005391    0.011561  1.429672  0.001620    1.003515       0.563131  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood job! If you look at the list of remaining association rules, you'll find both bag -> box and sign -> candles. We can tell the store manager that the original proposal is also acceptable under this new criterion.\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Aggregation and filtering\n",
    "\n",
    "In the video, we helped a gift store manager arrange the sections in her physical retail location according to association rules. The layout of the store forced us to group sections into two pairs of product types. After applying advanced filtering techniques, we proposed the floor layout below.\n",
    "\n",
    "The image shows the store layout that was selected in the video.\n",
    "\n",
    "The store manager is now asking you to generate another floorplan proposal, but with a different criterion: each pair of sections should contain one high support product and one low support product. The data, aggregated, has been aggregated and one-hot encoded for you. Additionally, apriori() and association_rules() have been imported from mlxtend.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Generate the set of frequent itemsets with a minimum support threshold to 0.0001.\n",
    "\n",
    "    Identify all rules with a minimum support threshold of 0.0001.\n",
    "\n",
    "    Select all rules with an antecedent support greater than 0.35.\n",
    "\n",
    "    Select all rules with a maximum consequent support lower than 0.35.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Apply the apriori algorithm with a minimum support of 0.0001\n",
    "frequent_itemsets = apriori(aggregated, min_support=0.0001, use_colnames = True)\n",
    "\n",
    "# Generate the initial set of rules using a minimum support of 0.0001\n",
    "rules = association_rules(frequent_itemsets, \n",
    "                          metric = \"support\", min_threshold = 0.0001)\n",
    "\n",
    "# Set minimum antecedent support to 0.35\n",
    "rules = rules[rules['antecedent support'] > 0.35]\n",
    "\n",
    "# Set maximum consequent support to 0.35\n",
    "rules = rules[rules['consequent support'] < 0.35]\n",
    "\n",
    "# Print the remaining rules\n",
    "print(rules)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! If you look at the list of remaining association rules, you'll find both bag -> box and sign -> candles. We can tell the store manager that the original proposal is also acceptable under this new criterion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = pd.read_csv('datasets/frequent.csv').drop(columns=['index'],index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zhangs_rule(rules):\n",
    "\tPAB = rules['support'].copy()\n",
    "\tPA = rules['antecedent support'].copy()\n",
    "\tPB = rules['consequent support'].copy()\n",
    "\tNUMERATOR = PAB - PA*PB\n",
    "\tDENOMINATOR = np.max((PAB*(1-PA).values,PA*(PB-PAB).values), axis = 0)\n",
    "\treturn NUMERATOR / DENOMINATOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Applying Zhang's rule\n",
    "\n",
    "In Chapter 2, we learned that Zhang's rule is a continuous measure of association between two items that takes values in the [-1,+1] interval. A -1 value indicates a perfectly negative association and a +1 value indicates a perfectly positive association. In this exercise, you'll determine whether Zhang's rule can be used to refine a set of rules a gift store is currently using to promote products.\n",
    "\n",
    "Note that the frequent itemsets have been computed for you and are available as frequent_itemsets. Additionally, zhangs_rule() has been defined and association_rules() have been imported from mlxtend. You will start by re-computing the original set of rules. After that, you will apply Zhang's metric to select only those rules with a high and positive association.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Generate the set of association rules with a lift value of at least 1.00.\n",
    "\n",
    "    Set the antecedent support threshold to 0.005.\n",
    "\n",
    "    Compute Zhang's rule and assign the output to the column zhang in rules.\n",
    "\n",
    "    Select the rules that have a Zhang's metric that is greater than 0.98.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Generate the initial set of rules using a minimum lift of 1.00\n",
    "rules = association_rules(frequent_itemsets, metric = \"lift\", min_threshold = 1.0)\n",
    "\n",
    "# Set antecedent support to 0.005\n",
    "rules = rules[rules['antecedent support'] > 0.005]\n",
    "\n",
    "# Set consequent support to 0.005\n",
    "rules = rules[rules['consequent support'] > 0.005]\n",
    "\n",
    "# Compute Zhang's rule\n",
    "rules['zhang'] = zhangs_rule(rules)\n",
    "\n",
    "# Set the lower bound for Zhang's rule to 0.98\n",
    "rules = rules[rules['zhang'] > 0.98]\n",
    "print(rules[['antecedents', 'consequents']])\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Notice that 10 items had a Zhang's metric value of over 0.98, which suggests that the items are nearly perfectly associated in the data. In general, when we see such strong associations, we'll want to think carefully about what explains them. We might, for instance, investigate whether the items be purchased separately or whether they are bundled in a way that prevents this.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Advanced filtering with multiple metrics\n",
    "\n",
    "Earlier, we used data from an online novelty gift store to find antecedents that could be used to promote a targeted consequent. Since the set of potential rules was large, we had to rely on the Apriori algorithm and multi-metric filtering to narrow it down. In this exercise, we'll examine the full set of rules and find a useful one, rather than targeting a particular antecedent.\n",
    "\n",
    "Note that the data has been loaded, preprocessed, and one-hot encoded, and is available as onehot. Additionally apriori() and association_rules() have been imported from mlxtend. In this exercise, you'll apply the Apriori algorithm to identify frequent itemsets. You'll then recover the set of association rules from the itemsets and apply multi-metric filtering.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Apply the Apriori algorithm to the one-hot encoded itemsets with a minimum support threshold of 0.001.\n",
    "\n",
    "    Extract association rules using a minimum support threshold of 0.001.\n",
    "\n",
    "    Set the antecedent_support at 0.002 and consequent_support to 0.01.\n",
    "\n",
    "    Set confidence to be higher than 0.60 and lift to be higher than 2.50.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
