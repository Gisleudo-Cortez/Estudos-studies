{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nero/Documents/Estudos/DataCamp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Visualizing itemset support\n",
    "\n",
    "A content-streaming start-up has approached you for consulting services. To keep licensing fees low, they want to assemble a narrow library of movies that all appeal to the same audience. While they'll provide a smaller selection of content than the big players in the industry, they'll also be able to offer a low subscription fee.\n",
    "\n",
    "You decide to use the MovieLens data and a heatmap for this project. Using a simple support-based heatmap will allow you to identify individual titles that have high support with other titles. The one-hot encoded data is available as the DataFrame onehot. Additionally, pandas is available as pd, seaborn is available as sns, and apriori() and association_rules() have both been imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Compute the frequent itemsets using a minimum support of 0.07.\n",
    "\n",
    "    Compute the association rules and apply no pruning to the rules.\n",
    "---\n",
    "\n",
    "    Replace the frozen sets in the antecedents columns with strings using a lambda function.\n",
    "\n",
    "    Replace the frozen sets in the consequents columns with strings using a lambda function.\n",
    "\n",
    "    Generate a heatmap using support values.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute frequent itemsets using a minimum support of 0.07\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.07, \n",
    "                            use_colnames = True, max_len = 2)\n",
    "\n",
    "# Compute the association rules\n",
    "rules = association_rules(frequent_itemsets, metric = 'support', \n",
    "                          min_threshold = 0.0)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Replace frozen sets with strings\n",
    "rules['antecedents'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules['consequents'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "\n",
    "# Transform data to matrix format and generate heatmap\n",
    "pivot = rules.pivot(index='consequents', columns='antecedents', values='support')\n",
    "sns.heatmap(pivot)\n",
    "\n",
    "# Format and display plot\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! Based on the heatmap, can you identify a narrow set of movies that might provide a good starting point for the streaming service?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Heatmaps with lift\n",
    "\n",
    "The founder likes the heatmap you've produced for her streaming service. After discussing the project further, however, you decide that that it is important to examine other metrics before making a final decision on which movies to license. In particular, the founder suggests that you select a metric that tells you whether the support values are higher than we would expect given the films' individual support values.\n",
    "\n",
    "You recall that lift does this well and decide to use it as a metric. You also remember that lift has an important threshold at 1.0 and decide that it is important to replace the colorbar with annotations, so you can determine whether a value is greater than 1.0. Note that the rules from the previous exercise are available to you as rules.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import seaborn under its standard alias.\n",
    "\n",
    "    Transform the DataFrame containing rules into a matrix using the lift metric.\n",
    "\n",
    "    Generate a heatmap with annotations on and the colorbar off.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import seaborn under its standard alias\n",
    "import seaborn as sns\n",
    "\n",
    "# Transform the DataFrame of rules into a matrix using the lift metric\n",
    "pivot = rules.pivot(index = 'consequents', \n",
    "                   columns = 'antecedents', values= 'lift')\n",
    "\n",
    "# Generate a heatmap with annotations on and the colorbar off\n",
    "sns.heatmap(pivot, annot = True, cbar = False)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Excellent work! In the next question, we'll interpret what the heatmap is telling us.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting heatmaps\n",
    "\n",
    "In the previous exercise, you generated the heatmap shown below. Each cell of the heatmap shows the lift value for an association rule. Recall that your goal was to identify a narrow set of films that were all strongly associated according to the lift metric. These films would form the initial content library for a streaming service. Which of the following statements is true?\n",
    "\n",
    "![heatmap](images/ch04-heatmap.png)\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    Fight Club, Braveheart, and Batman Begins would be the best initial content library.\n",
    "    \n",
    "    \n",
    "    Batman Begins, The Dark Knight, and The Dark Knight Rises would make a good initial library. {Answer}\n",
    "    \n",
    "    \n",
    "    Braveheart and The Dark Knight Rises would make the best initial content library.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Pruning with scatterplots\n",
    "\n",
    "After viewing your Batman-based streaming service proposal from the previous exercise, the founder realizes that her initial plan may have been too narrow. Rather than focusing on initial titles, she asks you to focus on general patterns in the association rules and then perform pruning accordingly. Your goal should be to identify a large set of strong associations.\n",
    "\n",
    "Fortunately, you've just learned how to generate scatterplots. You decide to start by plotting support and confidence, since all optimal rules according to many common metrics are located on the confidence-support border. The one-hot encoded data has been imported for you and is available as onehot. Additionally, apriori() and association_rules() have been imported and pandas is available as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Generate a large number of itemsets with 2 items by setting the minimum support to 0.0075 and setting the maximum length to 2.\n",
    "\n",
    "    Complete the statement for association_rules() in a way that avoids additional filtering.\n",
    "\n",
    "    Complete the statement to generate the scatterplot, setting the y variable to use confidence.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import seaborn under its standard alias\n",
    "import seaborn as sns\n",
    "\n",
    "# Apply the Apriori algorithm with a support value of 0.0075\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.0075, \n",
    "                            use_colnames = True, max_len = 2)\n",
    "\n",
    "# Generate association rules without performing additional pruning\n",
    "rules = association_rules(frequent_itemsets, metric = 'support', \n",
    "                          min_threshold = 0.0)\n",
    "\n",
    "# Generate scatterplot using support and confidence\n",
    "sns.scatterplot(x = \"support\", y = \"confidence\", data = rules)\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Notice that the confidence-support border roughly forms a triangle. This suggests that throwing out some low support rules would also mean that we would discard rules that are strong according to many common metrics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Optimality of the support-confidence border\n",
    "\n",
    "You return to the founder with the scatterplot produced in the previous exercise and ask whether she would like you to use pruning to recover the support-confidence border. You tell her about the Bayardo-Agrawal result, but she seems skeptical and asks whether you can demonstrate this in an example.\n",
    "\n",
    "Recalling that scatterplots can scale the size of dots according to a third metric, you decide to use that to demonstrate optimality of the support-confidence border. You will show this by scaling the dot size using the lift metric, which was one of the metrics to which Bayardo-Agrawal applies. The one-hot encoded data has been imported for you and is available as onehot. Additionally, apriori() and association_rules() have been imported and pandas is available as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Apply the Apriori algorithm to the DataFrame onehot.\n",
    "\n",
    "    Compute the association rules using the support metric and a minimum threshold of 0.0.\n",
    "\n",
    "    Complete the expression for the scatterplot such that the dot size is scaled by lift.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import seaborn under its standard alias\n",
    "import seaborn as sns\n",
    "\n",
    "# Apply the Apriori algorithm with a support value of 0.0075\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.0075, \n",
    "                         use_colnames = True, max_len = 2)\n",
    "\n",
    "# Generate association rules without performing additional pruning\n",
    "rules = association_rules(frequent_itemsets, metric = \"support\", \n",
    "                          min_threshold = 0.0)\n",
    "\n",
    "# Generate scatterplot using support and confidence\n",
    "sns.scatterplot(x = \"support\", y = \"confidence\", \n",
    "                size = \"lift\", data = rules)\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! If you look at the plot carefully, you'll notice that the highest values of lift are always at the support-confidence border for any given value of supply or confidence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules_to_coordinates(rules):\n",
    "\trules['antecedent'] = rules['antecedents'].apply(lambda antecedent: list(antecedent)[0])\n",
    "\trules['consequent'] = rules['consequents'].apply(lambda consequent: list(consequent)[0])\n",
    "\trules['rule'] = rules.index\n",
    "\treturn rules[['antecedent','consequent','rule']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Using parallel coordinates to visualize rules\n",
    "\n",
    "Your visual demonstration in the previous exercise convinced the founder that the supply-confidence border is worthy of further exploration. She now suggests that you extract part of the border and visualize it. Since the rules that fall on the border are strong with respect to most common metrics, she argues that you should simply visualize whether a rule exists, rather than the intensity of the rule according to some metric.\n",
    "\n",
    "You realize that a parallel coordinates plot is ideal for such cases. The data has been imported for you as onehot. Additionally, apriori(), association_rules(), and parallel_coordinates() have been imported, and pandas is available as pd. The function rules_to_coordinates() has been defined and is available.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Complete the Apriori algorithm statement using a minimum support of 0.05.\n",
    "    \n",
    "    Compute association rules using a minimum confidence threshold of 0.50. This is sufficiently high to exclusively capture points near the upper part of the supply-confidence border.\n",
    "    \n",
    "    Convert the rules into coordinates.\n",
    "    \n",
    "    Plot the coordinates using parallel_coordinates().\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Compute the frequent itemsets\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.05, \n",
    "                         use_colnames = True, max_len = 2)\n",
    "\n",
    "# Compute rules from the frequent itemsets with the confidence metric\n",
    "rules = association_rules(frequent_itemsets, metric = 'confidence', \n",
    "                          min_threshold = 0.50)\n",
    "\n",
    "# Convert rules into coordinates suitable for use in a parallel coordinates plot\n",
    "coords = rules_to_coordinates(rules)\n",
    "\n",
    "# Generate parallel coordinates plot\n",
    "parallel_coordinates(coords, 'rule')\n",
    "plt.legend([])\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! Do any patterns in the associations between antecedents and consequents stand out to you?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Refining a parallel coordinates plot\n",
    "\n",
    "After viewing your parallel coordinates plot, the founder concludes that her decision to step away from a Batman-centered streaming platform may have been premature. Indeed, the parallel coordinates plot seems to suggest that many popular movies are strongly associated with The Dark Knight. She decides instead to pitch the idea to her staff in a meeting, but has asked you to make some refinements to the plot to make it more visually appealing for the presentation.\n",
    "\n",
    "Note that the rules have been generated and are available for you as rules. Additionally, pandas is available as pd and the function rules_to_coordinates() has been defined and is available.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the parallel coordinates function from the plotting submodule of pandas.\n",
    "    \n",
    "    Convert the DataFrame of rules into coordinates.\n",
    "    \n",
    "    Complete the statement to generate a parallel coordinates plot, using ocean as the colormap.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import the parallel coordinates plot submodule\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Convert rules into coordinates suitable for use in a parallel coordinates plot\n",
    "coords = rules_to_coordinates(rules)\n",
    "\n",
    "# Generate parallel coordinates plot\n",
    "parallel_coordinates(coords, 'rule', colormap = 'ocean')\n",
    "plt.legend([])\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Notice that the color scheme for the parallel coordinates plot has now changed.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
