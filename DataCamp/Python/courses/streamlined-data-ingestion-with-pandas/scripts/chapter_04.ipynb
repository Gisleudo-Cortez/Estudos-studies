{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             index  adult_families_in_shelter  \\\n",
      "count  1000.000000                1000.000000   \n",
      "mean    499.500000                2074.955000   \n",
      "std     288.819436                 148.020238   \n",
      "min       0.000000                1796.000000   \n",
      "25%     249.750000                1906.000000   \n",
      "50%     499.500000                2129.000000   \n",
      "75%     749.250000                2172.250000   \n",
      "max     999.000000                2356.000000   \n",
      "\n",
      "       adults_in_families_with_children_in_shelter  \\\n",
      "count                                  1000.000000   \n",
      "mean                                  16487.932000   \n",
      "std                                     848.363772   \n",
      "min                                   14607.000000   \n",
      "25%                                   15831.500000   \n",
      "50%                                   16836.000000   \n",
      "75%                                   17118.250000   \n",
      "max                                   17733.000000   \n",
      "\n",
      "       children_in_families_with_children_in_shelter  \\\n",
      "count                                    1000.000000   \n",
      "mean                                    23273.873000   \n",
      "std                                       926.243984   \n",
      "min                                     21291.000000   \n",
      "25%                                     22666.000000   \n",
      "50%                                     23285.500000   \n",
      "75%                                     23610.000000   \n",
      "max                                     25490.000000   \n",
      "\n",
      "       families_with_children_in_shelter  \\\n",
      "count                         1000.00000   \n",
      "mean                         11588.83500   \n",
      "std                            626.41371   \n",
      "min                          10261.00000   \n",
      "25%                          11060.00000   \n",
      "50%                          11743.00000   \n",
      "75%                          12146.00000   \n",
      "max                          12413.00000   \n",
      "\n",
      "       individuals_in_adult_families_in_shelter  single_adult_men_in_shelter  \\\n",
      "count                                1000.00000                  1000.000000   \n",
      "mean                                 4368.05400                  8299.773000   \n",
      "std                                   299.05424                   766.782607   \n",
      "min                                  3811.00000                  6949.000000   \n",
      "25%                                  4026.00000                  7590.500000   \n",
      "50%                                  4473.00000                  8458.500000   \n",
      "75%                                  4567.00000                  8883.250000   \n",
      "max                                  4944.00000                  9599.000000   \n",
      "\n",
      "       single_adult_women_in_shelter  total_adults_in_shelter  \\\n",
      "count                    1000.000000              1000.000000   \n",
      "mean                     3173.107000             32328.857000   \n",
      "std                       353.227146              2150.583637   \n",
      "min                      2588.000000             28127.000000   \n",
      "25%                      2790.000000             30184.250000   \n",
      "50%                      3185.000000             33142.000000   \n",
      "75%                      3538.000000             33940.750000   \n",
      "max                      3779.000000             35294.000000   \n",
      "\n",
      "       total_children_in_shelter  \\\n",
      "count                1000.000000   \n",
      "mean                23273.882000   \n",
      "std                   926.247187   \n",
      "min                 21291.000000   \n",
      "25%                 22666.000000   \n",
      "50%                 23285.500000   \n",
      "75%                 23610.000000   \n",
      "max                 25490.000000   \n",
      "\n",
      "       total_individuals_in_families_with_children_in_shelter_  \\\n",
      "count                                        1000.000000         \n",
      "mean                                        39761.805000         \n",
      "std                                          1677.972788         \n",
      "min                                         35902.000000         \n",
      "25%                                         38775.500000         \n",
      "50%                                         40026.000000         \n",
      "75%                                         40529.500000         \n",
      "max                                         43208.000000         \n",
      "\n",
      "       total_individuals_in_shelter  total_single_adults_in_shelter  \n",
      "count                   1000.000000                      1000.00000  \n",
      "mean                   55602.739000                     11472.88000  \n",
      "std                     2745.294235                      1113.66412  \n",
      "min                    49462.000000                      9610.00000  \n",
      "25%                    53196.500000                     10381.75000  \n",
      "50%                    56713.500000                     11633.50000  \n",
      "75%                    57872.250000                     12437.50000  \n",
      "max                    59068.000000                     13270.00000  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhen getting data from a URL, like with open data portals, be mindful of how much data is being pulled and how often you do it. Requesting lots of data can strain shared resources.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Load JSON data\n",
    "\n",
    "Many open data portals make available JSONs datasets that are particularly easy to parse. They can be accessed directly via URL. Each object is a record, all objects have the same set of attributes, and none of the values are nested objects that themselves need to be parsed.\n",
    "\n",
    "The New York City Department of Homeless Services Daily Report is such a dataset, containing years' worth of homeless shelter population counts. You can view it in the console before loading it to a dataframe with pandas's read_json() function.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Get a sense of the contents of dhs_daily_report.json, which are printed in the console.\n",
    "    Load pandas as pd.\n",
    "    Use read_json() to load dhs_daily_report.json to a dataframe, pop_in_shelters.\n",
    "    View summary statistics about pop_in_shelters with the dataframe's describe() method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the daily report to a dataframe\n",
    "pop_in_shelters = pd.read_json(path_data+'dhs_daily_report.json')\n",
    "\n",
    "# View summary stats about pop_in_shelters\n",
    "print(pop_in_shelters.describe())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "When getting data from a URL, like with open data portals, be mindful of how much data is being pulled and how often you do it. Requesting lots of data can strain shared resources.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File dhs_report_reformatted.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Load the JSON without keyword arguments\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39m'\u001b[39;49m\u001b[39mdhs_report_reformatted.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Plot total population in shelters over time\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mdate_of_census\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m\"\u001b[39m\u001b[39mdate_of_census\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[1;32m    792\u001b[0m     path_or_buf,\n\u001b[1;32m    793\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[1;32m    794\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[1;32m    795\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    796\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[1;32m    797\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[1;32m    798\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[1;32m    799\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[1;32m    800\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[1;32m    801\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    802\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[1;32m    803\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    804\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    805\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[1;32m    806\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    807\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[1;32m    808\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    809\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    810\u001b[0m )\n\u001b[1;32m    812\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mujson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m    905\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/Documents/Estudos/estudos/lib/python3.11/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing literal json to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mread_json\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. To read from a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File dhs_report_reformatted.json does not exist"
     ]
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Work with JSON orientations\n",
    "\n",
    "JSON isn't a tabular format, so pandas makes assumptions about its orientation when loading data. Most JSON data you encounter will be in orientations that pandas can automatically transform into a dataframe.\n",
    "\n",
    "Sometimes, like in this modified version of the Department of Homeless Services Daily Report, data is oriented differently. To reduce the file size, it has been split formatted. You'll see what happens when you try to load it normally versus with the orient keyword argument. The try/except block will alert you if there are errors loading the data.\n",
    "\n",
    "pandas has been loaded as pd.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Try loading dhs_report_reformatted.json without any keyword arguments.\n",
    "---\n",
    "Load dhs_report_reformatted.json to a dataframe with orient specified.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    # Load the JSON without keyword arguments\n",
    "    df = pd.read_json('dhs_report_reformatted.json')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient='split')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "In this case, pandas could not parse the JSON at all without the orient keyword. For some other orientations, like index-oriented data, the data will load with transposed column and index names.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'businesses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Load data to a dataframe\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m cafes \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data[\u001b[39m'\u001b[39;49m\u001b[39mbusinesses\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# View the data's dtypes\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nero/Documents/Estudos/DataCamp/Python/courses/streamlined-data-ingestion-with-pandas/scripts/chapter_04.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(cafes\u001b[39m.\u001b[39mdtypes)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'businesses'"
     ]
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Get data from an API\n",
    "\n",
    "In this exercise, you'll use requests.get() to query the Yelp Business Search API for cafes in New York City. requests.get() needs a URL to get data from. The Yelp API also needs search parameters and authorization headers passed to the params and headers keyword arguments, respectively.\n",
    "\n",
    "You'll need to extract the data from the response with its json() method, and pass it to pandas's DataFrame() function to make a dataframe. Note that the necessary data is under the dictionary key \"businesses\".\n",
    "\n",
    "pandas (as pd) and requests have been loaded. Authorization data is in the dictionary headers, and the needed API parameters are stored as params.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Get data about New York City cafes from the Yelp API (api_url) with requests.get(). The necessary params and headers information has been provided.\n",
    "    Extract the JSON data from the response with its json() method, and assign it to data.\n",
    "    Load the cafe listings to the dataframe cafes with pandas's DataFrame() function. The listings are under the \"businesses\" key in data.\n",
    "    Print the dataframe's dtypes to see what information you're getting.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "import requests\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                headers=headers, \n",
    "                params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a dataframe\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "\n",
    "# View the data's dtypes\n",
    "print(cafes.dtypes)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Bravo! Check the response format whenever you work with a new API -- chances are the data you need is nested under a dictionary key, like here.\n",
    "One important note: to avoid problems like hitting usage limits or potential connection issues, this course mimics API requests in the background, and, if you set up the code right, returns the corresponding response object. The code you'll write looks exactly like what you would use to interact with the actual API, though. If you're curious about how this works, check out this GitHub repo.\n",
    "(https://github.com/adrian-soto/mock-request)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Set API parameters\n",
    "\n",
    "Formatting parameters to get the data you need is an integral part of working with APIs. These parameters can be passed to the get() function's params keyword argument as a dictionary.\n",
    "\n",
    "The Yelp API requires the location parameter be set. It also lets users supply a term to search for. You'll use these parameters to get data about cafes in NYC, then process the result to create a dataframe.\n",
    "\n",
    "pandas (as pd) and requests have been loaded. The API endpoint is stored in the variable api_url. Authorization data is stored in the dictionary headers.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a dictionary, parameters, with the term and location parameters set to search for \"cafe\"s in \"NYC\".\n",
    "    Query the Yelp API (api_url) with requests's get() function and the headers and params keyword arguments set. Save the result as response.\n",
    "    Extract the JSON data from response with the appropriate method. Save the result as data.\n",
    "    Load the \"businesses\" values in data to the dataframe cafes and print the head.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {'term':'cafe',\n",
    "          \t  'location':'NYC'}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url,\n",
    "                headers=headers,\n",
    "                params=parameters)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a dataframe and print head\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "print(cafes.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Notice that some of the values are themselves dictionaries, making them hard to analyze. We'll learn how to unpack those in a later lesson.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Set request headers\n",
    "\n",
    "Many APIs require users provide an API key, obtained by registering for the service. Keys typically are passed in the request header, rather than as parameters.\n",
    "\n",
    "The Yelp API documentation says \"To authenticate API calls with the API Key, set the Authorization HTTP header value as Bearer API_KEY.\"\n",
    "(https://www.yelp.com/developers/documentation/v3/authentication)\n",
    "\n",
    "You'll set up a dictionary to pass this information to get(), call the API for the highest-rated cafes in NYC, and parse the response.\n",
    "\n",
    "pandas (as pd) and requests have been loaded. The API endpoint is stored as api_url, and the key is api_key. Parameters are in the dictionary params.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a dictionary, headers, that passes the formatted key string to the \"Authorization\" header value.\n",
    "    Query the Yelp API (api_url) with get() and the necessary headers and parameters. Save the result as response.\n",
    "    Extract the JSON data from response. Save the result as data.\n",
    "    Load the \"businesses\" values in data to the dataframe cafes and print the names column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create dictionary that passes Authorization and key string\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url, headers=headers, params=params)\n",
    "\n",
    "\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a dataframe and print names\n",
    "cafes = pd.DataFrame(data[\"businesses\"])\n",
    "print(cafes.name)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! API keys are employed to track and moderate API usage. Be sure to keep keys private -- unscrupulous developers use others' keys to get around usage limits.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Flatten nested JSONs\n",
    "\n",
    "A feature of JSON data is that it can be nested: an attribute's value can consist of attribute-value pairs. This nested data is more useful unpacked, or flattened, into its own dataframe columns. The pandas.io.json submodule has a function, json_normalize(), that does exactly this.\n",
    "\n",
    "The Yelp API response data is nested. Your job is to flatten out the next level of data in the coordinates and location columns.\n",
    "\n",
    "pandas (as pd) and requests have been imported. The results of the API call are stored as response.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Load the json_normalize() function from pandas' io.json submodule.\n",
    "    Isolate the JSON data from response and assign it to data.\n",
    "    Use json_normalize() to flatten and load the businesses data to a dataframe, cafes. Set the sep argument to use underscores (_), rather than periods.\n",
    "    Show the data head.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "# Flatten business data into a dataframe, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "             sep='_')\n",
    "\n",
    "# View data\n",
    "print(cafes.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice! Notice that by accessing data['businesses'] we're already working one level down the nested structure. data itself could be flattened with json_normalize().\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Handle deeply nested data\n",
    "\n",
    "Last exercise, you flattened data nested down one level. Here, you'll unpack more deeply nested data.\n",
    "\n",
    "The categories attribute in the Yelp API response contains lists of objects. To flatten this data, you'll employ json_normalize() arguments to specify the path to categories and pick other attributes to include in the dataframe. You should also change the separator to facilitate column selection and prefix the other attributes to prevent column name collisions. We'll work through this in steps.\n",
    "\n",
    "pandas (as pd) and json_normalize() have been imported. JSON-formatted Yelp data on cafes in NYC is stored as data.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Use json_normalize() to flatten records under the businesses key in data, setting underscores (_) as separators.\n",
    "---\n",
    "\n",
    "    Specify the record_path to the categories data.\n",
    "---\n",
    "\n",
    "    Set the meta keyword argument to get business name, alias, rating, and the attributes nested under coordinates: latitude and longitude.\n",
    "    \n",
    "    Add \"biz_\" as a meta_prefix to prevent duplicate column names.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\",\n",
    "                    \t\tmeta=['name', \n",
    "                                  'alias',  \n",
    "                                  'rating',\n",
    "                          \t\t  ['coordinates', 'latitude'], \n",
    "                          \t\t  ['coordinates', 'longitude']],\n",
    "                    \t\tmeta_prefix='biz_')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Naming meta columns can get tedious for datasets with many attributes, and code is susceptible to breaking if column names or nesting levels change. In such cases, you may have to write a custom function and employ techniques like recursion to handle the data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Concatenate dataframes\n",
    "\n",
    "In this exercise, you’ll practice concatenating records by creating a dataset of the 100 highest-rated cafes in New York City according to Yelp.\n",
    "\n",
    "APIs often limit the amount of data returned, since sending large datasets can be time- and resource-intensive. The Yelp Business Search API limits the results returned in a call to 50 records. However, the offset parameter lets a user retrieve results starting after a specified number. By modifying the offset, we can get results 1-50 in one call and 51-100 in another. Then, we can append the dataframes.\n",
    "\n",
    "pandas (as pd), requests, and json_normalize() have been imported. The 50 top-rated cafes are already in a dataframe, top_50_cafes.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Add an \"offset\" parameter to params so that the Yelp API call will get cafes 51-100.\n",
    "    Concatenate the results of the API call to top_50_cafes, setting ignore_index so rows will be renumbered.\n",
    "    Print the shape of the resulting dataframe, cafes, to confirm there are 100 records.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          'offset':50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Concatenate the results, setting ignore_index to renumber rows\n",
    "cafes = pd.concat([top_50_cafes,next_50_cafes], ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! If you were putting multiple dataframes together, one option would be to start with an empty dataframe and use a for or while loop to append additional ones.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Merge dataframes\n",
    "\n",
    "In the last exercise, you built a dataset of the top 100 cafes in New York City according to Yelp. Now, you'll combine that with demographic data to investigate which neighborhood has the most good cafes per capita.\n",
    "\n",
    "To do this, you'll merge two datasets with the DataFrame merge() method. The first,crosswalk, is a crosswalk between ZIP codes and Public Use Micro Data Sample Areas (PUMAs), which are aggregates of census tracts and correspond roughly to NYC neighborhoods. Then, you'll merge in pop_data, which contains 2016 population estimates for each PUMA.\n",
    "\n",
    "pandas (as pd) has been imported, as has the cafes dataframe from last exercise.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Question\n",
    "\n",
    "Explore the cafes and crosswalk dataframes in the console. Which columns should be used as join keys?\n",
    "Possible answers:\n",
    "    \n",
    "    location_zip_code in cafes and zcta5 in crosswalk\n",
    "    \n",
    "    zipcode in both\n",
    "    \n",
    "    location.zipcode in cafes and zipcode in crosswalk\n",
    "    \n",
    "    location_zip_code in cafes and zipcode in crosswalk {Answer}\n",
    "---\n",
    "Question\n",
    "\n",
    "Explore the crosswalk and pop_data dataframes in the console. Which columns should be used as join keys?\n",
    "Possible answers:\n",
    "    \n",
    "    pumaname in crosswalk and puma in pop_data\n",
    "    \n",
    "    puma in both {Answer}\n",
    "    \n",
    "    zipcode in both\n",
    "    \n",
    "    pumaname in crosswalk and geog_name in pop_data\n",
    "---\n",
    "\n",
    "    Use the DataFrame method to merge cafes and crosswalk on location_zip_code and zipcode, respectively. Assign the result to cafes_with_pumas.\n",
    "    \n",
    "    Merge pop_data into cafes_with_pumas on their puma fields. Save the result as cafes_with_pop.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Merge crosswalk into cafes on their zip code fields\n",
    "cafes_with_pumas = cafes.merge(crosswalk, left_on='location_zip_code', right_on='zipcode')\n",
    "\n",
    "\n",
    "\n",
    "# Merge pop_data into cafes_with_pumas on puma field\n",
    "cafes_with_pop = cafes_with_pumas.merge(pop_data, on='puma')\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Congratulations! You've built a pretty sophisticated pipeline that translates geographies to link data from multiple sources. While postal codes are a commonly used areal unit, there are often more meaningful ways to group spatial data, such as by neighborhood here.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
