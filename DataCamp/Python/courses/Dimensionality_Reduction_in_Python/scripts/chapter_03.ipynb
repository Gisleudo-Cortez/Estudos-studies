{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T12:37:55.996346Z",
     "iopub.status.busy": "2023-06-09T12:37:55.996019Z",
     "iopub.status.idle": "2023-06-09T12:38:17.880114Z",
     "shell.execute_reply": "2023-06-09T12:38:17.878291Z",
     "shell.execute_reply.started": "2023-06-09T12:37:55.996315Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "path_data = '/home/nero/Documents/Estudos/DataCamp/Python/Dimensionality_Reduction_in_Python/datasets/'\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T12:56:23.708323Z",
     "iopub.status.busy": "2023-06-09T12:56:23.707990Z",
     "iopub.status.idle": "2023-06-09T12:56:23.714185Z",
     "shell.execute_reply": "2023-06-09T12:56:23.712796Z",
     "shell.execute_reply.started": "2023-06-09T12:56:23.708294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T12:55:54.159455Z",
     "iopub.status.busy": "2023-06-09T12:55:54.158074Z",
     "iopub.status.idle": "2023-06-09T12:55:54.171408Z",
     "shell.execute_reply": "2023-06-09T12:55:54.170582Z",
     "shell.execute_reply.started": "2023-06-09T12:55:54.159403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_data + 'PimaIndians.csv')\n",
    "\n",
    "X = data.drop('test', axis = 1)\n",
    "y = data['test']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T12:56:27.956445Z",
     "iopub.status.busy": "2023-06-09T12:56:27.955465Z",
     "iopub.status.idle": "2023-06-09T12:56:27.963202Z",
     "shell.execute_reply": "2023-06-09T12:56:27.961385Z",
     "shell.execute_reply.started": "2023-06-09T12:56:27.956397Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T12:56:29.419109Z",
     "iopub.status.busy": "2023-06-09T12:56:29.418359Z",
     "iopub.status.idle": "2023-06-09T12:56:29.443610Z",
     "shell.execute_reply": "2023-06-09T12:56:29.442820Z",
     "shell.execute_reply.started": "2023-06-09T12:56:29.419073Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.6% accuracy on test set.\n",
      "{'pregnant': 0.23, 'glucose': 1.22, 'diastolic': 0.06, 'triceps': 0.2, 'insulin': 0.24, 'bmi': 0.48, 'family': 0.45, 'age': 0.43}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! We get almost 80% accuracy on the test set. Take a look at the differences in model coefficients for the different features.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Building a diabetes classifier\n",
    "\n",
    "You'll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset. The data has been split into a training and test set and pre-loaded for you as X_train, y_train, X_test, and y_test.\n",
    "\n",
    "A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Fit the scaler on the training features and transform these features in one go.\n",
    "    Fit the logistic regression model on the scaled training data.\n",
    "    Scale the test features.\n",
    "    Predict diabetes presence on the scaled test set.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scale the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "print(f\"{accuracy_score(y_test, y_pred):.1%} accuracy on test set.\")\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! We get almost 80% accuracy on the test set. Take a look at the differences in model coefficients for the different features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T12:59:54.179520Z",
     "iopub.status.busy": "2023-06-09T12:59:54.179057Z",
     "iopub.status.idle": "2023-06-09T12:59:54.249822Z",
     "shell.execute_reply": "2023-06-09T12:59:54.248022Z",
     "shell.execute_reply.started": "2023-06-09T12:59:54.179478Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.6% accuracy on test set.\n",
      "{'pregnant': 0.05, 'glucose': 1.24, 'triceps': 0.24, 'insulin': 0.2, 'bmi': 0.39, 'family': 0.34, 'age': 0.35}\n",
      "79.6% accuracy on test set.\n",
      "{'glucose': 1.13, 'triceps': 0.25, 'bmi': 0.34, 'family': 0.34, 'age': 0.37}\n",
      "75.5% accuracy on test set.\n",
      "{'glucose': 1.28}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nInteresting! Removing all but one feature only reduced the accuracy by a few percent.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Manual Recursive Feature Elimination\n",
    "\n",
    "Now that we've created a diabetes classifier, let's see if we can reduce the number of features without hurting the model accuracy too much.\n",
    "\n",
    "On the second line of code the features are selected from the original DataFrame. Adjust this selection.\n",
    "\n",
    "A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr.\n",
    "\n",
    "All necessary functions and packages have been pre-loaded too.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    First, run the given code, then remove the feature with the lowest model coefficient from X.\n",
    "\n",
    "    Run the code and remove 2 more features with the lowest model coefficients.\n",
    "\n",
    "    Run the code and only keep the feature with the highest coefficient.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Remove the feature with the lowest model coefficient\n",
    "X = data[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Remove the 2 features with the lowest model coefficients\n",
    "X = data[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Only keep the feature with the highest coefficient\n",
    "X = data[['glucose']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model to the data\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(f\"{acc:.1%} accuracy on test set.\")  \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Interesting! Removing all but one feature only reduced the accuracy by a few percent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:03:27.901041Z",
     "iopub.status.busy": "2023-06-09T13:03:27.900388Z",
     "iopub.status.idle": "2023-06-09T13:03:28.575690Z",
     "shell.execute_reply": "2023-06-09T13:03:28.574762Z",
     "shell.execute_reply.started": "2023-06-09T13:03:27.900998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:04:41.080613Z",
     "iopub.status.busy": "2023-06-09T13:04:41.080189Z",
     "iopub.status.idle": "2023-06-09T13:04:41.095819Z",
     "shell.execute_reply": "2023-06-09T13:04:41.094846Z",
     "shell.execute_reply.started": "2023-06-09T13:04:41.080579Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_data + 'PimaIndians.csv')\n",
    "\n",
    "X = data.drop('test', axis = 1)\n",
    "y = data['test']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:05:19.989347Z",
     "iopub.status.busy": "2023-06-09T13:05:19.988987Z",
     "iopub.status.idle": "2023-06-09T13:05:20.124159Z",
     "shell.execute_reply": "2023-06-09T13:05:20.123061Z",
     "shell.execute_reply.started": "2023-06-09T13:05:19.989316Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "{'pregnant': 2, 'glucose': 3, 'diastolic': 5, 'triceps': 4, 'insulin': 6, 'bmi': 1, 'family': 1, 'age': 1}\n",
      "Index(['bmi', 'family', 'age'], dtype='object')\n",
      "71.4% accuracy on test set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! When we eliminate all but the 3 most relevant features we get a 80.6% accuracy on the test set.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Automatic Recursive Feature Elimination\n",
    "\n",
    "Now let's automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features.\n",
    "\n",
    "All the necessary functions and packages have been pre-loaded and the features have been scaled for you.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create the RFE with a LogisticRegression() estimator and 3 features to select.\n",
    "    Print the features and their ranking.\n",
    "    Print the features that are not eliminated.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter = 999), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! When we eliminate all but the 3 most relevant features we get a 80.6% accuracy on the test set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:10:49.875716Z",
     "iopub.status.busy": "2023-06-09T13:10:49.874919Z",
     "iopub.status.idle": "2023-06-09T13:10:50.821978Z",
     "shell.execute_reply": "2023-06-09T13:10:50.820232Z",
     "shell.execute_reply.started": "2023-06-09T13:10:49.875681Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:11:02.381985Z",
     "iopub.status.busy": "2023-06-09T13:11:02.381597Z",
     "iopub.status.idle": "2023-06-09T13:11:02.710894Z",
     "shell.execute_reply": "2023-06-09T13:11:02.709935Z",
     "shell.execute_reply.started": "2023-06-09T13:11:02.381954Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pregnant': 0.07, 'glucose': 0.25, 'diastolic': 0.09, 'triceps': 0.09, 'insulin': 0.14, 'bmi': 0.12, 'family': 0.12, 'age': 0.13}\n",
      "79.6% accuracy on test set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood job! The random forest model gets 78% accuracy on the test set and 'glucose' is the most important feature (0.21).\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Building a random forest model\n",
    "\n",
    "You'll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You'll fit the model on the training data after performing the train-test split and consult the feature importance values.\n",
    "\n",
    "The feature and target datasets have been pre-loaded for you as X and y. Same goes for the necessary packages and functions.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Set a 25% test size to perform a 75%-25% train-test split.\n",
    "    Fit the random forest classifier to the training data.\n",
    "    Calculate the accuracy on the test set.\n",
    "    Print the feature importances per feature.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"{acc:.1%} accuracy on test set.\") \n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! The random forest model gets 78% accuracy on the test set and 'glucose' is the most important feature (0.21).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:13:07.319298Z",
     "iopub.status.busy": "2023-06-09T13:13:07.318535Z",
     "iopub.status.idle": "2023-06-09T13:13:07.344165Z",
     "shell.execute_reply": "2023-06-09T13:13:07.343105Z",
     "shell.execute_reply.started": "2023-06-09T13:13:07.319252Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'insulin', 'age'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWell done! Only the features 'glucose' and 'age' were considered sufficiently important.\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Random forest for feature selection\n",
    "\n",
    "Now lets use the fitted random model to select the most important features from our input dataset X.\n",
    "\n",
    "The trained model from the previous exercise has been pre-loaded for you as rf.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a mask for features with an importance higher than 0.15.\n",
    "---\n",
    "\n",
    "    Sub-select the most important features by applying the mask to X.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.12\n",
    "\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:, mask]\n",
    "\n",
    "# prints out the selected column names\n",
    "print(reduced_X.columns)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! Only the features 'glucose' and 'age' were considered sufficiently important.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:16:15.125313Z",
     "iopub.status.busy": "2023-06-09T13:16:15.124965Z",
     "iopub.status.idle": "2023-06-09T13:16:16.368355Z",
     "shell.execute_reply": "2023-06-09T13:16:16.367158Z",
     "shell.execute_reply.started": "2023-06-09T13:16:15.125277Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 4 features.\n",
      "Index(['glucose', 'bmi'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat! Compared to the quick and dirty single threshold method from the previous exercise one of the selected features is different.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 06\n",
    "\n",
    "\"\"\"\n",
    "Recursive Feature Elimination with random forests\n",
    "\n",
    "You'll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others.\n",
    "\n",
    "You'll need these pre-loaded datasets: X, X_train, y_train.\n",
    "\n",
    "Functions and classes that have been pre-loaded for you are: RandomForestClassifier(), RFE(), train_test_split().\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Create a recursive feature eliminator that will select the 2 most important features using a random forest model.\n",
    "---\n",
    "\n",
    "    Fit the recursive feature eliminator to the training data.\n",
    "---\n",
    "\n",
    "    Create a mask using the fitted eliminator's support_ attribute, then apply it to the feature dataset X.\n",
    "---\n",
    "\n",
    "    Change the settings of RFE() to eliminate 2 features at each step.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Set the feature eliminator to remove 2 features on each step\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Compared to the quick and dirty single threshold method from the previous exercise one of the selected features is different.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:23:15.337026Z",
     "iopub.status.busy": "2023-06-09T13:23:15.336542Z",
     "iopub.status.idle": "2023-06-09T13:23:15.447480Z",
     "shell.execute_reply": "2023-06-09T13:23:15.446267Z",
     "shell.execute_reply.started": "2023-06-09T13:23:15.336984Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ansur_df_1 = pd.read_csv(path_data + 'ANSUR_II_MALE.csv')\n",
    "ansur_df_2 = pd.read_csv(path_data + 'ANSUR_II_FEMALE.csv')\n",
    "\n",
    "ansur_df = pd.concat([ansur_df_1, ansur_df_2])\n",
    "# Non-numerical columns in the dataset\n",
    "non_numeric = ['Branch', 'Component', 'BMI_class', 'Height_class', 'Gender']\n",
    "\n",
    "# Drop the non-numerical columns from df\n",
    "ansur_df = ansur_df.drop(non_numeric, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:23:55.313866Z",
     "iopub.status.busy": "2023-06-09T13:23:55.312906Z",
     "iopub.status.idle": "2023-06-09T13:23:55.320002Z",
     "shell.execute_reply": "2023-06-09T13:23:55.319027Z",
     "shell.execute_reply.started": "2023-06-09T13:23:55.313828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = ansur_df.drop('BMI', axis = 1)\n",
    "y = ansur_df['BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:24:10.218888Z",
     "iopub.status.busy": "2023-06-09T13:24:10.218435Z",
     "iopub.status.idle": "2023-06-09T13:24:10.224164Z",
     "shell.execute_reply": "2023-06-09T13:24:10.222879Z",
     "shell.execute_reply.started": "2023-06-09T13:24:10.218850Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:24:13.848356Z",
     "iopub.status.busy": "2023-06-09T13:24:13.847581Z",
     "iopub.status.idle": "2023-06-09T13:24:14.024116Z",
     "shell.execute_reply": "2023-06-09T13:24:14.023539Z",
     "shell.execute_reply.started": "2023-06-09T13:24:13.848313Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGood job! You've fitted the Lasso model to the standardized training data. Now let's look at the results!\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 07\n",
    "\n",
    "\"\"\"\n",
    "Creating a LASSO regressor\n",
    "\n",
    "You'll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the pre-imported Lasso() regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge.\n",
    "\n",
    "You'll standardize the data first using the StandardScaler() that has been instantiated for you as scaler to make sure all coefficients face a comparable regularizing force trying to bring them down.\n",
    "\n",
    "All necessary functions and classes plus the input datasets X and y have been pre-loaded.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Set the test size to 30% to get a 70-30% train test split.\n",
    "    Fit the scaler on the training features and transform these in one go.\n",
    "    Create the Lasso model.\n",
    "    Fit it to the scaled training data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! You've fitted the Lasso model to the standardized training data. Now let's look at the results!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:28:01.013248Z",
     "iopub.status.busy": "2023-06-09T13:28:01.011903Z",
     "iopub.status.idle": "2023-06-09T13:28:01.031635Z",
     "shell.execute_reply": "2023-06-09T13:28:01.030718Z",
     "shell.execute_reply.started": "2023-06-09T13:28:01.013188Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 82.9% of the variance in the test set.\n",
      "The model has ignored 83 out of 93 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGood! We can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The R^2 could be higher though.\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 08\n",
    "\n",
    "\"\"\"\n",
    "Lasso model results\n",
    "\n",
    "Now that you've trained the Lasso model, you'll score its predictive capacity (\n",
    "\n",
    ") on the test set and count how many features are ignored because their coefficient is reduced to zero.\n",
    "\n",
    "The X_test and y_test datasets have been pre-loaded for you.\n",
    "\n",
    "The Lasso() model and StandardScaler() have been instantiated as la and scaler respectively and both were fitted to the training data.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Transform the test set with the pre-fitted scaler.\n",
    "Calculate the R²\n",
    "value on the scaled test data.\n",
    "Create a list that has True values when coefficients equal 0.\n",
    "Calculate the total number of features with a coefficient of 0.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std,y_test)\n",
    "print(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(f\"The model has ignored {n_ignored} out of {len(la.coef_)} features.\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good! We can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The R^2 could be higher though.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:32:55.487132Z",
     "iopub.status.busy": "2023-06-09T13:32:55.486589Z",
     "iopub.status.idle": "2023-06-09T13:32:55.751230Z",
     "shell.execute_reply": "2023-06-09T13:32:55.749075Z",
     "shell.execute_reply.started": "2023-06-09T13:32:55.487086Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 98.5% of the variance in the test set.\n",
      "69 out of 93 features were ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWow! With this more appropriate regularization strength we can predict 98% of the variance in the BMI value while ignoring 2/3 of the features.\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 09\n",
    "\n",
    "\"\"\"\n",
    "Adjusting the regularization strength\n",
    "\n",
    "Your current Lasso model has an R² score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power.\n",
    "\n",
    "Let's improve the balance between predictive power and model simplicity by tweaking the alpha parameter.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Find the highest value for alpha that gives an R² value above 98% from the options: 1, 0.5, 0.1, and 0.01.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(alpha=0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\n",
    "print(f\"{n_ignored_features} out of {len(la.coef_)} features were ignored.\")\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Wow! With this more appropriate regularization strength we can predict 98% of the variance in the BMI value while ignoring 2/3 of the features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:37:44.542199Z",
     "iopub.status.busy": "2023-06-09T13:37:44.541699Z",
     "iopub.status.idle": "2023-06-09T13:37:45.990468Z",
     "shell.execute_reply": "2023-06-09T13:37:45.989909Z",
     "shell.execute_reply.started": "2023-06-09T13:37:44.542155Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha = 0.406\n",
      "The model explains 98.9% of the test set variance\n",
      "38 features out of 93 selected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGreat! We got a decent R squared and removed 10 features. We'll save the lcv_mask for later on.\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 10\n",
    "\n",
    "\"\"\"\n",
    "Creating a LassoCV regressor\n",
    "\n",
    "You'll be predicting biceps circumference on a subsample of the male ANSUR dataset using the LassoCV() regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation.\n",
    "\n",
    "The standardized training and test data has been pre-loaded for you as X_train, X_test, y_train, and y_test.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Create and fit the LassoCV model on the training set.\n",
    "Calculate R² on the test set.\n",
    "Create a mask for coefficients not equal to zero.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV(max_iter = 2000)\n",
    "lcv.fit(X_train, y_train)\n",
    "print(f'Optimal alpha = {lcv.alpha_:.3f}')\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print(f'The model explains {r_squared:.1%} of the test set variance')\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print(f'{sum(lcv_mask)} features out of {len(lcv_mask)} selected')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! We got a decent R squared and removed 10 features. We'll save the lcv_mask for later on.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T13:43:24.933431Z",
     "iopub.status.busy": "2023-06-09T13:43:24.932015Z",
     "iopub.status.idle": "2023-06-09T13:53:06.561810Z",
     "shell.execute_reply": "2023-06-09T13:53:06.560811Z",
     "shell.execute_reply.started": "2023-06-09T13:43:24.933385Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 12 features.\n",
      "The model can explain 97.1% of the variance in the test set\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 12 features.\n",
      "The model can explain 97.5% of the variance in the test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGood job! Including the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important.\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 11\n",
    "\n",
    "\"\"\"\n",
    "Ensemble models for extra votes\n",
    "\n",
    "The LassoCV() model selected 22 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let's use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE).\n",
    "\n",
    "The standardized training and test data has been pre-loaded for you as X_train, X_test, y_train, and y_test.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Select 10 features with RFE on a GradientBoostingRegressor and drop 3 features on each step.\n",
    "---\n",
    "    Calculate the R² on the test set.\n",
    "---\n",
    "\n",
    "    Assign the support array of the fitted model to gb_mask.\n",
    "---\n",
    "\n",
    "    Modify the first step to select 10 features with RFE on a RandomForestRegressor() and drop 3 features on each step.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=10, step=10, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print(f'The model can explain {r_squared:.1%} of the variance in the test set')\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=10, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print(f'The model can explain {r_squared:.1%} of the variance in the test set')\n",
    "\n",
    "# Assign the support array to rf_mask\n",
    "rf_mask = rfe_rf.support_\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Good job! Including the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T16:34:11.496841Z",
     "iopub.status.busy": "2023-06-09T16:34:11.496268Z",
     "iopub.status.idle": "2023-06-09T16:34:11.503962Z",
     "shell.execute_reply": "2023-06-09T16:34:11.502734Z",
     "shell.execute_reply.started": "2023-06-09T16:34:11.496797Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T16:34:13.547166Z",
     "iopub.status.busy": "2023-06-09T16:34:13.545479Z",
     "iopub.status.idle": "2023-06-09T16:34:14.080541Z",
     "shell.execute_reply": "2023-06-09T16:34:14.078861Z",
     "shell.execute_reply.started": "2023-06-09T16:34:13.547005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 97.1% of the variance in the test set using 7 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAwesome! Using the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy!\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 12\n",
    "\n",
    "\"\"\"\n",
    "Combining 3 feature selectors\n",
    "\n",
    "We'll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset.\n",
    "\n",
    "The per model votes have been pre-loaded as lcv_mask, rf_mask, and gb_mask and the feature and target datasets as X and y.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Sum the votes of the three models using np.sum().\n",
    "---\n",
    "\n",
    "    Create a mask for features selected by all 3 models.\n",
    "---\n",
    "\n",
    "    Apply the dimensionality reduction on X and print which features were selected.\n",
    "---\n",
    "\n",
    "    Plug the reduced dataset into the code for simple linear regression that has been written for you.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes == 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print(f'The model can explain {r_squared:.1%} of the variance in the test set using {len(lm.coef_)} features.')\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Awesome! Using the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
