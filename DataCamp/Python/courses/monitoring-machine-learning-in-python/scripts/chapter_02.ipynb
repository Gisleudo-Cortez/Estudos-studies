{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this to shorten the data import from the files\n",
    "import os\n",
    "cwd = os.path.dirname(os.getcwd())+'/'\n",
    "path_data = os.path.join(os.path.dirname(os.getcwd()), 'datasets/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When performance estimation is off\n",
    "\n",
    "Imagine you're a data scientist at a bank, working on a loan default use case. You receive labels to validate your model and performance estimation algorithm every month. During one particular month, you observe that many customers with well-paid jobs are defaulting more often due to a significant surge in inflation and a corresponding job crisis.\n",
    "\n",
    "As you compare the estimated and realized performance, you notice a significant disparity between them.\n",
    "\n",
    "What could be why the performance estimation algorithm is not as effective in this situation?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    The algorithm is not implemented correctly\n",
    "    \n",
    "    \n",
    "    Concept drift{Answer}\n",
    "    \n",
    "    \n",
    "    Covariate shift\n",
    "    \n",
    "    \n",
    "    None of the above\n",
    "\n",
    "**Perfect! CBPE and DLE algorithms will not work well under the concept drift. Now that you understand the possibilities and limitations of the algorithms let's validate them on our tip prediction model and US Consensus dataset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nero/Documents/Estudos/DataCamp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Comparing estimated and realized performance\n",
    "\n",
    "Now that you have seen how performance calculation works, your task is to calculate the realized performance for our tip prediction model for the NYC green taxi dataset.\n",
    "\n",
    "The reference and analysis set is already loaded and saved in the reference and analysis variables.\n",
    "\n",
    "In addition, results from the DLE algorithm for tip prediction are stored in the estimated_results variable.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Specify problem type as regression in calculator initialization.\n",
    "\n",
    "    Fit the calculator with reference data and calculate performance for the analysis set.\n",
    "\n",
    "    Show comparison plot between realized_results and estimated_results using compare() method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Intialize the calculator\n",
    "calculator = nannyml.PerformanceCalculator(\n",
    "    y_true='tip_amount',\n",
    "    y_pred='y_pred',\n",
    "    chunk_period='d',\n",
    "  \tmetrics=['mae'],\n",
    "    timestamp_column_name='lpep_pickup_datetime',\n",
    "    problem_type='regression')\n",
    "\n",
    "# Fit the calculator\n",
    "calculator.fit(reference)\n",
    "realized_results = calculator.calculate(analysis)\n",
    "\n",
    "# Show comparison plot for realized and estimated performance\n",
    "realized_results.compare(estimated_results).plot().show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Wonderful! See how the estimated performance is usually closely matched with the realized performance, with a few exceptions during the holiday periods where the performance degradation is greater than estimated. Now, let's explore what else we can do with our results!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Different chunking methods\n",
    "\n",
    "A chunk represents a single data point in the monitoring results. Recall that there are three methods for chunking your data: based on time, size, or the number of chunks.\n",
    "\n",
    "In this exercise, you will chunk and visualize the results of the CBPE algorithm for the US Census dataset using size-based and number-based chunking methods.\n",
    "\n",
    "The nannyml library is already imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "Load reference, analysis, and analysis labels using load_us_census_ma_employment_data() method and set chunk size to 5000.\n",
    "---\n",
    "Add f1 metric to the monitored metrics and set chunk number to 8.\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "reference, analysis, analysis_gt = nannyml.load_us_census_ma_employment_data()\n",
    "\n",
    "# Initialize the CBPE algorithm\n",
    "cbpe = nannyml.CBPE(\n",
    "    y_pred_proba='predicted_probability',\n",
    "    y_pred='prediction',\n",
    "    y_true='employed',\n",
    "    metrics = ['roc_auc', 'accuracy'],\n",
    "    problem_type = 'classification_binary',\n",
    "    chunk_size = 5000,\n",
    ")\n",
    "\n",
    "cbpe = cbpe.fit(reference)\n",
    "estimated_results = cbpe.estimate(analysis)\n",
    "estimated_results.plot().show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "reference, analysis, analysis_gt = nannyml.load_us_census_ma_employment_data()\n",
    "\n",
    "# Initialize the CBPE algorithm\n",
    "cbpe = nannyml.CBPE(\n",
    "    y_pred_proba='predicted_probability',\n",
    "    y_pred='prediction',\n",
    "    y_true='employed',\n",
    "    metrics = ['roc_auc', 'accuracy', 'f1'],\n",
    "    problem_type = 'classification_binary',\n",
    "\tchunk_number = 8,\n",
    ")\n",
    "\n",
    "cbpe = cbpe.fit(reference)\n",
    "estimated_results = cbpe.estimate(analysis)\n",
    "estimated_results.plot().show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Nice work! You can notice the difference in the graphs based on the method you use. A good rule of thumb is to make the chunk size about 10% of the reference data size for reliable results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Modifying the thresholds\n",
    "\n",
    "In the video, you observed how NannyML calculates threshold values and learned how to customize them to suit your solution.\n",
    "\n",
    "In this exercise, your task is to define two custom standard deviation and custom thresholds and then apply them to the results obtained from the CBPE algorithm for the US Census dataset.\n",
    "\n",
    "The reference and analysis sets have been pre-loaded as reference and analysis, along with the nannyml library.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Import ConstantThreshold, and StandardDeviationThreshold from nannyml.thresholds.\n",
    "\n",
    "    Initialize the standard deviation method and set std_lower_multiplier and std_upper_multiplier parameters to 2.\n",
    "\n",
    "    Initialize the constant threshold method and set the lower parameter to 0.9 and upper to 0.98.\n",
    "\n",
    "    Pass the constant threshold method for the f1 metric and the standard deviation method for accuracy to the CBPE algorithm.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import custom thresholds\n",
    "from nannyml.thresholds  import ConstantThreshold, StandardDeviationThreshold\n",
    "\n",
    "# Initialize custom thresholds\n",
    "stdt = StandardDeviationThreshold(std_lower_multiplier=2, std_upper_multiplier=2)\n",
    "ct = ConstantThreshold(lower=0.9, upper=0.98)\n",
    "\n",
    "# Initialize the CBPE algorithm\n",
    "estimator = nannyml.CBPE(\n",
    "    problem_type='classification_binary',\n",
    "    y_pred_proba='predicted_probability',\n",
    "    y_pred='prediction',\n",
    "    y_true='employed',\n",
    "    metrics=['roc_auc', 'accuracy', 'f1'],\n",
    "    thresholds={'f1': ct, 'accuracy': stdt})\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great! Custom thresholds are a valuable tool, especially in situations where you need to be alerted when a specific value is exceeded.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Interacting with results\n",
    "\n",
    "In this exercise, you will filter, plot, and convert to the DataFrame the CBPE results obtained for the US Consensus dataset from the previous example. The display method here is used to show the plots and DataFrames that are called in the middle of the code.\n",
    "\n",
    "The results from the CBPE estimator are preloaded in the estimated_results variable.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Interact with the estimated results based on the comments above each code snippet.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Filter estimated results for the roc_auc metric and convert them to a dataframe\n",
    "display(estimated_results.filter(metrics=['roc_auc']).to_df())\n",
    "\n",
    "# Filter estimated results for the reference period and convert them to a dataframe\n",
    "display(estimated_results.filter(period='reference').to_df())\n",
    "\n",
    "# Filter the estimated results for the accuracy metric\n",
    "display(estimated_results.filter(metrics=['accuracy']).plot().show())\n",
    "\n",
    "# Filter the estimated results for the analysis period, as well as for accuracy and roc_auc metrics\n",
    "display(estimated_results.filter(period='analysis', metrics=['accuracy', 'roc_auc']).plot().show())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Fantastic job! You can now adjust and get the results that suit your requirements. Now, let's explore calculating and estimating the business value of your model!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business value calculation\n",
    "\n",
    "Recall that you can determine your model's business value by either directly calculating it using a performance calculator or estimating it with the CBPE algorithm. In the last video, we also delved into how these algorithms work behind the scenes.\n",
    "\n",
    "Now, using the provided confusion matrix and business value matrix shown in the image below, you need to calculate the monetary value of the model.\n",
    "\n",
    "![image](images/Exercise_2_3_confusion_matrix.png)\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    -$20\n",
    "    \n",
    "    \n",
    "    $20 {Answer}\n",
    "    \n",
    "    \n",
    "    $40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop in monetary value\n",
    "\n",
    "Now, you are in the production environment, and your model is up and running.\n",
    "\n",
    "Based on the given graph, establish days when the alerted drop in the F1 score overlaps with the alerted drop in the business value of the model.\n",
    "\n",
    "![images](images/Exercise_2_3_performance_estimation_business.png)\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    4th and 5th of July {Answer}\n",
    "    \n",
    "    \n",
    "    4th, 5th and 10th of July\n",
    "    \n",
    "    \n",
    "    From 3rd to 7th of July\n",
    "\n",
    "\n",
    "**Well done! As you can observe, a drop in performance doesn't necessarily translate to a decline in the business value of your model. Now, let's see how this applies to our hotel booking dataset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 05\n",
    "\n",
    "\"\"\"\n",
    "Business calculation for hotel booking dataset\n",
    "\n",
    "Previously, you were introduced to the challenge of predicting booking cancellations. Here, you will work with the actual Hotel Booking dataset, where a model predicts booking cancellations based on the customer's country of origin, time between booking and arrival, required parking spaces, and the chosen hotel.\n",
    "\n",
    "The reference and analysis sets have already been loaded for you. Here are the first two rows:\n",
    "\n",
    "  country  lead_time  parking_spaces       hotel  y_pred  y_pred_proba  is_canceled  timestamp\n",
    "0  FRA     120        0               City Hotel  0       0.239983      0           2016-05-01\n",
    "1  ITA     120        1               City Hotel  0       0.003965      0           2016-05-01\n",
    "\n",
    "Your task is to check the model's monetary value and ROC AUC performance.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Initialize a custom threshold with 0 as the lower value and 150,000 as the upper value.\n",
    "\n",
    "    Specify the business value and roc_auc metric for monitoring.\n",
    "\n",
    "    Set TN to 0, FP to -100, FN to -200, and TP to 1500 in business_value_matrix.\n",
    "\n",
    "    Assign custom threshold to the business value metric.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Custom business value thresholds\n",
    "ct = ConstantThreshold(lower=0, upper=150000)\n",
    "# Intialize the performance calculator\n",
    "calc = PerformanceCalculator(problem_type='classification_binary',\n",
    "\t\t\ty_pred_proba='y_pred_proba',\n",
    "  \t\t\ttimestamp_column_name=\"timestamp\", \t\t\n",
    "  \t\t\ty_pred='y_pred',\n",
    "  \t\t\ty_true='is_canceled',\n",
    "            chunk_period='m',\n",
    "  \t\t\tmetrics=['business_value', 'roc_auc'],\n",
    "  \t\t\tbusiness_value_matrix = [[0, -100],[-200, 1500]],\n",
    "  \t\t\tthresholds={'business_value': ct})\n",
    "calc = calc.fit(reference)\n",
    "calc_res = calc.calculate(analysis)\n",
    "calc_res.filter(period='analysis').plot().show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Well done! You can see that our model brings losses in December and January, which also overlaps with a drop in ROC AUC performance during that period. In the next chapter, we will investigate why this happened!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
