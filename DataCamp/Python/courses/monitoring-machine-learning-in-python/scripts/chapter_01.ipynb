{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key features of NannyML\n",
    "\n",
    "NannyML is an open-source library that helps data scientists monitor their models in production. Do you recall what its key features are?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "    Performance estimation and calculation{Answer}\n",
    "\n",
    "\n",
    "    Multivariate and univariate drift detection{Answer}\n",
    "\n",
    "\n",
    "    Measuring operational metrics\n",
    "\n",
    "\n",
    "    Data quality monitoring{Answer}\n",
    "\n",
    "\n",
    "    Business value calculation and estimation {Answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nannyml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Import the nannyml libary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# solution\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Import nannyml\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnannyml\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Load US Census Employment dataset\u001b[39;00m\n\u001b[1;32m     33\u001b[0m reference, analysis, analysis_gt \u001b[38;5;241m=\u001b[39m nannyml\u001b[38;5;241m.\u001b[39mload_us_census_ma_employment_data()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nannyml'"
     ]
    }
   ],
   "source": [
    "# exercise 01\n",
    "\n",
    "\"\"\"\n",
    "Load the dataset\n",
    "\n",
    "NannyML comes with a set of internal datasets in order to make it easier to demo use cases and test different algorithms. To load the dataset, you only need to use the nannyml.load_us_census_ma_employment_data() function.\n",
    "\n",
    "The function returns three Pandas DataFrame objects: the reference set (the test set), the analysis set (unseen production data), and the ground truth for the analysis set. These data frames should be named according to the convention as reference, analysis, and analysis_gt.\n",
    "\n",
    "In this exercise, you will load the US Census Employment dataset and print the data frames to understand what they look like.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Import the nannyml libary.\n",
    "\n",
    "    Load the US Census Employment dataset from the nannyml library.\n",
    "\n",
    "    Print the head of the reference data.\n",
    "\n",
    "    Print the head of the analysis data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Import nannyml\n",
    "import nannyml\n",
    "\n",
    "# Load US Census Employment dataset\n",
    "reference, analysis, analysis_gt = nannyml.load_us_census_ma_employment_data()\n",
    "\n",
    "# Print head of the reference data\n",
    "print(reference.head())\n",
    "\n",
    "# Print head of the analysis data\n",
    "print(analysis.head())\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great work! Now you know the basics of NannyML. In the next video, you will learn how to create reference and analysis sets from any raw data!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference or analysis period?\n",
    "\n",
    "In the last video, you learned about two data periods NannyML operates on: the reference and analysis periods. These periods are really important to understand before you can move to monitor performance and covariate shifts in production.\n",
    "\n",
    "Now, let's test your memory. Can you remember the characteristics of each of these periods?\n",
    "\n",
    "![Answer](images/ch01-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>pickup_time</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01 00:00:02</td>\n",
       "      <td>82</td>\n",
       "      <td>129</td>\n",
       "      <td>0.60</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-01 00:01:57</td>\n",
       "      <td>255</td>\n",
       "      <td>7</td>\n",
       "      <td>4.53</td>\n",
       "      <td>17.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-01 00:04:17</td>\n",
       "      <td>65</td>\n",
       "      <td>195</td>\n",
       "      <td>1.94</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-01 00:06:45</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-01 00:09:18</td>\n",
       "      <td>74</td>\n",
       "      <td>42</td>\n",
       "      <td>2.02</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime  PULocationID  DOLocationID  trip_distance  \\\n",
       "0  2016-12-01 00:00:02            82           129           0.60   \n",
       "1  2016-12-01 00:01:57           255             7           4.53   \n",
       "2  2016-12-01 00:04:17            65           195           1.94   \n",
       "3  2016-12-01 00:06:45            41            41           1.00   \n",
       "4  2016-12-01 00:09:18            74            42           2.02   \n",
       "\n",
       "   fare_amount  tip_amount  pickup_time partition  \n",
       "0          5.0         1.0            0     train  \n",
       "1         17.5         4.7            0     train  \n",
       "2          9.5         0.0            0     train  \n",
       "3          5.5         2.0            0     train  \n",
       "4          8.5         1.0            0     train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nGreat job! Data split and loading are the first steps to create reference and analysis sets for monitoring in production!\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 02\n",
    "\n",
    "\"\"\"\n",
    "Loading and splitting the data\n",
    "\n",
    "To deploy and monitor a model in production, you must first create it. In the last video, you've been introduced to loading and processing data, building the model, and creating reference and analysis sets.\n",
    "\n",
    "In this exercise, you'll follow a similar process, but to simplify matters, you'll use the NYC Green Taxi dataset provided in a csv file that's already been processed.\n",
    "\n",
    "For this exercise, pandas has been imported as pd and is ready for you to use.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Pass the green_taxi_dataset.csv to the dataset_name variable.\n",
    "    Use pd.read_csv() to load the dataset.\n",
    "    Show the head of the dataset.\n",
    "---\n",
    "\n",
    "    Split the dataset into a training set, a test set, and a production(prod) set.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "# Load the dataset\n",
    "import pandas as pd\n",
    "dataset_name = \"datasets/green_taxi_dataset.csv\"\n",
    "data = pd.read_csv(dataset_name, parse_dates=['lpep_pickup_datetime'], low_memory=False)\n",
    "features = ['lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'pickup_time']\n",
    "target = 'tip_amount'\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "# Split the training data\n",
    "X_train = data.loc[data['partition'] == 'train', features]\n",
    "y_train = data.loc[data['partition'] == 'train', target]\n",
    "\n",
    "# Split the test data\n",
    "X_test = data.loc[data['partition'] == 'test', features]\n",
    "y_test = data.loc[data['partition'] == 'test', target]\n",
    "\n",
    "# Split the prod data\n",
    "X_prod = data.loc[data['partition'] == 'prod', features]\n",
    "y_prod = data.loc[data['partition'] == 'prod', target]\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Data split and loading are the first steps to create reference and analysis sets for monitoring in production!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exercise 03\n",
    "\n",
    "\"\"\"\n",
    "Creating reference and analysis set\n",
    "\n",
    "After your data is split into train, test, and production sets, you can build and deploy your model. The testing and production data will later be used to create the reference and analysis set.\n",
    "\n",
    "In this exercise, you will go through this process. You have all of your X_train/test/prod, and y_train/test/prod datasets created in the previous exercise already loaded here.\n",
    "\n",
    "For this exercise, pandas has been imported as pd and is ready for use.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Train the model using fit method and pass X_train and y_train sets.\n",
    "    Make predictions on train and test sets.\n",
    "    Deploy the model by making predictions for production data.\n",
    "---\n",
    "\n",
    "    Add a y_pred(models predictions) column for the reference and analysis sets, assigning the model's predictions from the test and production sets, respectively.\n",
    "    Add a tip_amount column to reference set set and assign values from y_test(labels) to it.\n",
    "    Join the lpep_pickup_datetime timestamp column for reference and analysis set.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Fit the model\n",
    "model = LGBMRegressor(random_state=111, n_estimators=50, n_jobs=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get model's prediction on train, test, and production set\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_prod = model.predict(X_prod)\n",
    "\n",
    "# Create reference and analysis set\n",
    "reference = X_test.copy() # Copy test set features\n",
    "reference['y_pred'] = y_pred_test # Add models predictions on test set\n",
    "reference['tip_amount'] = y_test # Add labels(ground truth)\n",
    "reference = reference.join(data['lpep_pickup_datetime']) # Add timestamp column\n",
    "\n",
    "analysis = X_prod.copy() # Add production set features\n",
    "analysis['y_pred'] = y_pred_prod # Add models predictions on production set\n",
    "analysis = analysis.join(data['lpep_pickup_datetime']) # Add timestamp column\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Amazing! Creating reference and analysis sets is the first thing you need to do to keep an eye on your model in production. In the following video, we will use sets created in this exercise to estimate our model's performance!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the algorithm and problem type\n",
    "\n",
    "Imagine you are a data scientist consultant working for a hotel chain. Your task is to build a model to predict whether the customer will arrive (or not). Many bookings are made months in advance, which means you're dealing with the delayed ground truth issue.\n",
    "\n",
    "What is the name of the performance estimation algorithm you would use and the problem type you would describe it? \n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    CBPE, multi-label classification\n",
    "    \n",
    "    \n",
    "    DLE, binary classification\n",
    "    \n",
    "    \n",
    "    CBPE, binary classification {Answer}\n",
    "    \n",
    "    \n",
    "    None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting results\n",
    "\n",
    "In this scenario, you've successfully implemented your performance estimation algorithm in a production environment. As a result, you have a plot for estimated ROC AUC metric.\n",
    "\n",
    "Your task now is to select correct information based on the plot about the following:\n",
    "\n",
    "![image](images/lesson_3_exercise_cbpe.png)\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "\n",
    "    chunk period = d daily, upper threshold = 1, lower threshold = 0.6, alert month = September\n",
    "    \n",
    "    \n",
    "    chunk period = m monthly, upper threshold = 0.9, lower threshold = 0.7, alert month = November\n",
    "    \n",
    "    \n",
    "    chunk period = m monthly, upper threshold = 1, lower threshold = 0.7, alert month = September {Answer}\n",
    "    \n",
    "    \n",
    "    chunk period = w weekly, upper threshold = 1, lower threshold = 0.7, alert month = September\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBPE and DLE workflow\n",
    "\n",
    "Recall that NannyML provides a repeatable workflow similar to scikit-learn for estimating your model's technical performance.\n",
    "\n",
    "Reorder the provided pseudo-code to reflect the initialization of the CBPE algorithm, the estimation of performance, and its visualization.\n",
    "\n",
    "![Answer](images/ch01-02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exercise 04\n",
    "\n",
    "\"\"\"\n",
    "Performance estimation for tip prediction\n",
    "\n",
    "In the previous exercises, you prepared a reference and analysis set for the NYC Green Taxi dataset. In this one, you will use that data to estimate the model's performance in production.\n",
    "\n",
    "First, you must initialize the DLE algorithm with the provided parameters and then plot the results.\n",
    "\n",
    "The reference and analysis set is already loaded and saved in the reference and analysis variables. Additionally, nannyml is also already imported.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    Initiate the DLE algorithm with daily chunk period, tip_amount as a y_true , and MSE metric.\n",
    "\n",
    "    Fit reference set to the DLE estimator, estimate performance for analysis set and store the output in the results variable.\n",
    "\n",
    "    Visualize the results using plot() and show() methods.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# solution\n",
    "\n",
    "estimator = nannyml.DLE(y_pred='y_pred',\n",
    "    timestamp_column_name='lpep_pickup_datetime',\n",
    "    feature_column_names=features,\n",
    "    chunk_period='d',\n",
    "    y_true='tip_amount',\n",
    "    metrics=['mse'])\n",
    "\n",
    "# Fit the reference data to the DLE algorithm\n",
    "estimator.fit(reference)\n",
    "\n",
    "# Estimate the performance on the analysis data\n",
    "results = estimator.estimate(analysis)\n",
    "\n",
    "# Plot and show the results\n",
    "results.plot().show()\n",
    "\n",
    "#----------------------------------#\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\"\"\"\n",
    "Great job! Now you have the knowledge and tools to estimate your model's performance in production. In the next chapter, we'll dive into comparing the estimated performance with the actual performance, when the ground truth becomes available!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
